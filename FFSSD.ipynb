{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mF7fWlA2Auiv"
      ],
      "mount_file_id": "1u8VMtqWk82aaJ1x3jn0sECaOnC-Ajo3d",
      "authorship_tag": "ABX9TyPIYkYG3rhdN4ojfJmhvPTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dxxi3XCEgiGN",
        "outputId": "ffe7fd61-b1d5-43aa-862d-81a573730245"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch_intermediate_layer_getter\n",
        "from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tf"
      ],
      "metadata": {
        "id": "mF7fWlA2Auiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # vgg16 = tf.keras.applications.vgg16.VGG16(weights='imagenet')\n",
        "# # vgg16.summary()\n",
        "# # for i in range(len(vgg16.layers)):\n",
        "# #   vgg16.layers[i].trainable=False\n",
        "# #   print(vgg16.layers[i].name)\n",
        "# def ftb(current_layer, former_layer=None):\n",
        "#   x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, activation='relu', padding='same')(current_layer)\n",
        "#   x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same')(x)\n",
        "\n",
        "#   if former_layer is None:\n",
        "#     return x\n",
        "  \n",
        "#   elif x.shape == former_layer.shape:\n",
        "#     return tf.keras.layers.Add()([x, former_layer])\n",
        "\n",
        "#   else:\n",
        "#     d = tf.keras.layers.Conv2DTranspose(filters=256, kernel_size=4, strides=2, padding='same')(former_layer)\n",
        "#     return tf.keras.layers.Add()([x, d])\n",
        "\n",
        "\n",
        "# def rfem(x):\n",
        "#   channel = x.shape[-1]\n",
        "#   #branch1\n",
        "  \n",
        "#   x1 = tf.keras.layers.MaxPool2D(pool_size=2, padding='same', strides=1)(x)\n",
        "#   x1 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 1), padding='same', activation='relu')(x1)\n",
        "#   x1 = tf.keras.layers.BatchNormalization()(x1)\n",
        "\n",
        "#   #branch2\n",
        "#   x2 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 1), padding='same', activation='relu')(x)\n",
        "#   x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "#   x2 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 3), padding='same', activation='relu')(x2)\n",
        "#   x2 = tf.keras.layers.BatchNormalization()(x2)\n",
        "\n",
        "#   #branch3\n",
        "#   x3 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 1), padding='same', activation='relu')(x)\n",
        "#   x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "#   x3 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(3, 1), padding='same', activation='relu')(x3)\n",
        "#   x3 = tf.keras.layers.BatchNormalization()(x3)\n",
        "\n",
        "#   #branch4\n",
        "#   x4 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 1), padding='same', activation='relu')(x)\n",
        "#   x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "#   x4 = tf.keras.layers.Conv2D(filters=channel//3, kernel_size=(3, 1), padding='same', activation='relu')(x4)\n",
        "#   x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "#   x4 = tf.keras.layers.Conv2D(filters=channel//3, kernel_size=(1, 3), padding='same', activation='relu')(x4)\n",
        "#   x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "#   x4 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(3, 1), padding='same', activation='relu')(x4)\n",
        "#   x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "#   x4 = tf.keras.layers.Conv2D(filters=channel//4, kernel_size=(1, 3), padding='same', activation='relu')(x4)\n",
        "#   x4 = tf.keras.layers.BatchNormalization()(x4)\n",
        "\n",
        "#   combined_x = tf.keras.layers.Concatenate(axis=-1)([x1, x2, x3, x4])\n",
        "\n",
        "#   return tf.keras.layers.Add()([combined_x, x/0.5])"
      ],
      "metadata": {
        "id": "FzeSzuayTWLJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = tf.keras.models.Model(inputs=input_x, outputs=[X])\n",
        "# vgg16_short = tf.keras.models.Model(vgg16.input, vgg16.layers[-5].output)\n",
        "\n",
        "# model_1 = tf.keras.Sequential([\n",
        "#     vgg16_short, \n",
        "#     tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3), padding='same', activation='relu', name='block6_conv1'),\n",
        "#     tf.keras.layers.MaxPool2D(pool_size=2, name='block6_pool'),\n",
        "#     tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), strides=2, padding='same', activation='relu', name='block7_conv1'),\n",
        "#     tf.keras.layers.MaxPool2D(pool_size=2, name='block7_pool'),\n",
        "#     tf.keras.layers.Flatten(),\n",
        "#     tf.keras.layers.Dense(units=4096, activation='relu', name='dense_layer'),\n",
        "#     tf.keras.layers.Dense(units=7, activation='softmax', name='final_layer')\n",
        "# ])\n",
        "# model_1.summary()\n",
        "\n",
        "# model_2 = tf.keras.models.Model(inputs=vgg16.input, outputs=[vgg16_short.get_layer('block4_conv2').output,\n",
        "#                                                             vgg16_short.get_layer('block4_conv3').output,\n",
        "#                                                             vgg16_short.get_layer('block5_conv3').output,\n",
        "#                                                             model_1.get_layer('block6_conv1').output,\n",
        "#                                                             model_1.get_layer('block7_conv1').output])"
      ],
      "metadata": {
        "id": "6I5wn93QZZmx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pytorch"
      ],
      "metadata": {
        "id": "VNqaRfigAq5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "    return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "    self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "    device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "  def forward(self, images):\n",
        "    return self.backbone(images)"
      ],
      "metadata": {
        "id": "tTweBAlVbjjS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(5,3,320,320)\n",
        "# y = model_with_multuple_layer(x)    \n",
        "\n",
        "encoder = Encoder()\n",
        "y = encoder(x)"
      ],
      "metadata": {
        "id": "gz1caoDAzppq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48acd403-859c-4af5-e030-235e908f5730"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 139MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0].keys(), y[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un-boiGn2rvo",
        "outputId": "8e275cf2-72c6-4319-9a1f-a7992f931ebf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(odict_keys(['out_conv4_2', 'out_conv4_3', 'out_conv7_2', 'out_conv5_3', 'out_conv6_2']),\n",
              " torch.Size([5, 512, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv4_2 = y[0]['out_conv4_2']\n",
        "conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "conv4_3 = y[0]['out_conv4_3']\n",
        "conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "conv5_3 = y[0]['out_conv5_3'][0]\n",
        "conv6_2 = y[0]['out_conv6_2'][0]\n",
        "conv7_2 = y[0]['out_conv7_2'][0]\n",
        "\n",
        "print(conv4_2.shape, conv4_3.shape, conv5_3.shape, conv6_2.shape, conv7_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvp7anur_4Vc",
        "outputId": "66e6dfc5-77e7-4956-f33c-27124e580e12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 512, 20, 20]) torch.Size([5, 512, 20, 20]) torch.Size([5, 512, 20, 20]) torch.Size([5, 512, 20, 20]) torch.Size([5, 512, 20, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ftb(current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  x = torch.nn.Conv2d(256, 256, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 256, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    return torch.add(x, d)"
      ],
      "metadata": {
        "id": "Ef4PrW4Rh1YO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rfem(x):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  # x1 = torch.nn.MaxPool2d(kernel_size=2, stride=1, padding=1)(x)\n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "  return torch.add(combined_x, x/0.5)"
      ],
      "metadata": {
        "id": "9bI20xu2tznb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s=ftb(conv4_3, conv5_3)\n",
        "a = rfem(conv5_3)\n",
        "a.shape, s.shape"
      ],
      "metadata": {
        "id": "ottYC5vczvb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "5c8bef63-dfbf-4098-baa9-901caf854fb1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d8a9f86a240f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mftb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv5_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv5_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5d270ac765ad>\u001b[0m in \u001b[0;36mftb\u001b[0;34m(current_layer, former_layer)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformer_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformer_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (40) at non-singleton dimension 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training(img):\n",
        "  backbone = Encoder()\n",
        "  feature_maps = backbone(img)\n",
        "\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "\n",
        "  f1 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  f2 = torch.add(rfem(f1), conv5_3)\n",
        "  f3 = torch.add(rfem(f2), conv6_2)\n",
        "  f4 = torch.add(rfem(f3), conv7_2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "My9tQLKLbG4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # x = torch.rand(5, 3, 320, 320)\n",
        "\n",
        "# backbone = Encoder()\n",
        "# feature_maps = backbone(x)\n",
        "\n",
        "# conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "# conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "# conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "# conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "# conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "# conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "# conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "# conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "# conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "\n",
        "# rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "# rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "# rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "# rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "\n",
        "ftb_4 = ftb(rfem14)\n",
        "ftb_3 = ftb(rfem13)\n",
        "ftb_2 = ftb(rfem12)\n",
        "ftb_1 = ftb(rfem11)\n",
        "\n",
        "# rfem21 = rfem(ftb_1)\n",
        "# rfem22 = rfem(ftb_2)\n",
        "# rfem23 = rfem(ftb_3)\n",
        "# rfem24 = rfem(ftb_4)\n",
        "\n",
        "print(ftb_1.shape, rfem21.shape, rfem11.shape)\n",
        "rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "det_1 = torch.cat((rfem21, rfem11))\n",
        "det_2 = torch.cat((rfem22, rfem12))\n",
        "det_3 = torch.cat((rfem23, rfem13))\n",
        "det_4 = torch.cat((rfem24, rfem14))\n",
        "\n",
        "\n",
        "# print(f1.shape, f2.shape, f3.shape, f4.shape)"
      ],
      "metadata": {
        "id": "hzo1EPm7Mr8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9gaL9xboMtIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}