{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "e509a15d-7457-49f9-b308-aa4721273566"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch_intermediate_layer_getter\n",
        "from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik328v3BJgoF"
      },
      "source": [
        "#Test Part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qez3yY2nGbmr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def train(model, x_train, y_train, criterion, optimizer, epochs):\n",
        "    for i in range(epochs):\n",
        "        y_pred = model(x_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        if i % 100 == 0:\n",
        "            print('Loss:', loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "class Conv2dBlock(torch.nn.Module):\n",
        "    def __init__(self, in_nums, out_nums):\n",
        "        super(Conv2dBlock, self).__init__()\n",
        "        self.Conv2d = torch.nn.Conv2d(in_nums, out_nums, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.nn.Softmax(self.Conv2d(x))\n",
        "\n",
        "class FullyConnectedNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, neurons):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        self.blocks = torch.nn.ModuleList()\n",
        "        previous = input_size\n",
        "        for i in range(len(neurons)):\n",
        "            self.blocks.append(Conv2dBlock(previous, neurons[i]))\n",
        "            previous = neurons[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Pass the input through each block\"\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "fcnet02 = FullyConnectedNet(4, [8, 10, 10, 8, 1])\n",
        "optimizer = torch.optim.SGD(fcnet02.parameters(), lr=0.01)\n",
        "# train(fcnet02, x_var, y_var, criterion, optimizer, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fGOVAbO9J1DV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "b5b84f28-4c95-4f25-f042-7885edbb9d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:03<00:00, 141MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2e28f7dd8e10>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'IntermediateLayerGetter' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "# s=FeatureExtractor()\n",
        "# s(x)\n",
        "\n",
        "model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "for param in model.named_parameters():\n",
        "  param[1].requres_grad = True\n",
        "\n",
        "model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "feature_maps = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "conv4_2=feature_maps[0]['out_conv4_2']\n",
        "conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "conv4_3=feature_maps[0]['out_conv4_3']\n",
        "conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "conv5_3=feature_maps[0]['out_conv5_3'][0]\n",
        "conv6_2=feature_maps[0]['out_conv6_2'][0]\n",
        "conv7_2=feature_maps[0]['out_conv7_2'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHJmWmK0HVnV"
      },
      "outputs": [],
      "source": [
        "# for param in model.parameters():\n",
        "#   param[1].requres_grad = True\n",
        "#   blocks.append(param[1])\n",
        "\n",
        "# model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "# print(model.features)\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "# training(backbone(images))\n",
        "\n",
        "\n",
        "\n",
        "class FullyConnectedNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, neurons):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        previous = input_size\n",
        "        for i in range(len(neurons)):\n",
        "            self.blocks.append(Conv2dBlock(previous, neurons[i]))\n",
        "            previous = neurons[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Pass the input through each block\"\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "\n",
        "previous = input_size\n",
        "for i in range(len(neurons)):\n",
        "    blocks.append(LinearBlock(previous, neurons[i]))\n",
        "    previous = neurons[i]\n",
        "\n",
        "def forward(self, x):\n",
        "  for block in blocks:\n",
        "      x = block(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  block.append(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out"
      ],
      "metadata": {
        "id": "4Hlmd4HsQ9s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "for param in model.parameters():\n",
        "  param[1].requres_grad = True\n",
        "\n",
        "model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "\n",
        "for i in model.children():\n",
        "  block = torch.nn.ModuleList(i)\n",
        "  break\n",
        "\n",
        "return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "feature_maps = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "print(feature_maps[0]['out_conv4_2'])\n",
        "conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(feature_maps[0]['out_conv4_2'])\n",
        "conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "block.append(conv4_2)\n",
        "\n",
        "conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(feature_maps[0]['out_conv4_3'])\n",
        "conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "block.append(conv4_3)\n",
        "\n",
        "conv5_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv5_3'][0])\n",
        "conv5_3=torch.nn.ReLU()(conv5_3)\n",
        "block.append(conv5_3)\n",
        "\n",
        "conv6_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv6_2'][0])\n",
        "conv6_2=torch.nn.ReLU()(conv6_2)\n",
        "block.append(conv6_2)\n",
        "\n",
        "\n",
        "conv7_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv7_2'][0])\n",
        "conv7_2=torch.nn.ReLU()(conv7_2)\n",
        "block.append(conv7_2)\n",
        "\n",
        "block, rfem11_temp = rfem(block, conv4_2)\n",
        "block, rfem12_temp = rfem(block, rfem11_temp)\n",
        "block, rfem13_temp = rfem(block, rfem12_temp)\n",
        "block, rfem14_temp = rfem(block, rfem13_temp)\n",
        "\n",
        "rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "\n",
        "block, ftb_4 = ftb(block, rfem14)\n",
        "block, ftb_3 = ftb(block, rfem13)\n",
        "block, ftb_2 = ftb(block, rfem12)\n",
        "block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "block, rfem21_temp = rfem(block, ftb_1)\n",
        "block, rfem22_temp = rfem(block, ftb_2)\n",
        "block, rfem23_temp = rfem(block, ftb_3)\n",
        "block, rfem24_temp = rfem(block, ftb_4)\n",
        "\n",
        "rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "block.append(det_1)\n",
        "det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "block.append(det_2)\n",
        "det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "block.append(det_3)\n",
        "det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "block.append(det_4)\n",
        "\n",
        "D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "block.append(out)\n"
      ],
      "metadata": {
        "id": "grWFJwj6m7bu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "2474dbe0-9c75-4a32-e9a5-2cb304ee261c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c7ea00d6f51f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreturn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'22'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'29'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv5_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'34'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv7_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'36'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv6_2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfeature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntermediateLayerGetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'IntermediateLayerGetter' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rfem(x):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "  return torch.add(combined_x, x/0.5)\n",
        "\n",
        "def ftb(current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    return torch.add(x, d)\n",
        "\n",
        "\n",
        "def training(feature_maps):\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "  rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "  rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "  rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "  ftb_4 = ftb(rfem14)\n",
        "  ftb_3 = ftb(rfem13)\n",
        "  ftb_2 = ftb(rfem12)\n",
        "  ftb_1 = ftb(rfem11)\n",
        "\n",
        "  rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "  rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "  rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "  rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "  det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "  det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "  det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "  det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "  D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "  pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "  torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "  return D\n",
        "\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    for param in model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "    # self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "    return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "    self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.backbone(images))"
      ],
      "metadata": {
        "id": "X-NIWv5ugI5l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((5, 3, 320, 320))\n",
        "s = FeatureExtractor()\n",
        "# s(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeCgrHBZgKd2",
        "outputId": "0ec8ae5a-198b-4e2f-93c0-8329fae5fd02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[7.8176e-06, 6.6645e-04, 2.2708e-04,  ..., 1.4087e-04,\n",
              "           8.7158e-05, 9.0211e-05],\n",
              "          [1.1051e-03, 1.9570e-04, 4.7800e-05,  ..., 2.7391e-04,\n",
              "           5.3019e-06, 9.6722e-05],\n",
              "          [1.7936e-04, 3.4381e-04, 1.8036e-04,  ..., 2.3597e-04,\n",
              "           2.0543e-04, 2.3218e-04],\n",
              "          ...,\n",
              "          [8.9622e-04, 2.2122e-04, 2.1478e-04,  ..., 5.2985e-04,\n",
              "           3.8868e-05, 2.3890e-04],\n",
              "          [3.5794e-04, 4.9891e-04, 2.4570e-04,  ..., 4.6587e-04,\n",
              "           1.5991e-04, 2.5332e-04],\n",
              "          [4.5381e-04, 1.3064e-04, 1.2457e-04,  ..., 2.3308e-04,\n",
              "           2.7899e-04, 1.1927e-02]],\n",
              "\n",
              "         [[6.8154e-06, 7.9429e-05, 1.8770e-04,  ..., 9.3511e-05,\n",
              "           3.8935e-04, 7.1605e-04],\n",
              "          [1.7433e-04, 1.4652e-03, 3.7408e-04,  ..., 3.4547e-03,\n",
              "           1.3083e-03, 2.8971e-03],\n",
              "          [8.8588e-05, 1.6379e-04, 3.3512e-05,  ..., 9.3611e-05,\n",
              "           2.7926e-02, 6.0792e-03],\n",
              "          ...,\n",
              "          [3.7027e-04, 2.8249e-04, 1.4651e-04,  ..., 3.3599e-04,\n",
              "           2.3323e-04, 5.7388e-02],\n",
              "          [2.3108e-04, 1.5310e-04, 6.2396e-03,  ..., 1.9355e-04,\n",
              "           1.0882e-03, 3.4666e-03],\n",
              "          [8.3442e-05, 7.1016e-04, 5.8136e-03,  ..., 1.5227e-03,\n",
              "           7.7907e-04, 1.0968e-03]],\n",
              "\n",
              "         [[1.3227e-05, 1.3890e-03, 3.9042e-05,  ..., 6.0816e-05,\n",
              "           7.8697e-05, 2.7092e-04],\n",
              "          [5.1399e-04, 1.1800e-04, 6.0409e-04,  ..., 7.3438e-05,\n",
              "           4.3422e-05, 6.2131e-04],\n",
              "          [1.2449e-04, 3.7096e-04, 7.4125e-05,  ..., 2.4406e-04,\n",
              "           2.6532e-04, 1.1347e-02],\n",
              "          ...,\n",
              "          [1.9889e-04, 2.3042e-04, 6.3308e-04,  ..., 5.4280e-04,\n",
              "           6.4350e-05, 1.1433e-04],\n",
              "          [4.3825e-05, 3.5093e-05, 7.1310e-04,  ..., 1.0020e-04,\n",
              "           6.1425e-05, 7.6764e-04],\n",
              "          [6.9230e-05, 2.6998e-03, 2.5828e-03,  ..., 9.4248e-04,\n",
              "           1.9874e-03, 2.4664e-04]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[2.3835e-03, 5.5109e-07, 1.0454e-06,  ..., 9.9335e-04,\n",
              "           1.4683e-09, 5.7339e-09],\n",
              "          [2.7310e-07, 1.6280e-10, 1.0089e-10,  ..., 1.0369e-06,\n",
              "           5.0430e-11, 3.4253e-11],\n",
              "          [2.5956e-06, 1.5547e-07, 3.4375e-09,  ..., 3.9229e-09,\n",
              "           8.5058e-11, 3.6298e-10],\n",
              "          ...,\n",
              "          [8.1816e-06, 7.7756e-09, 1.4384e-09,  ..., 2.2789e-07,\n",
              "           1.7372e-07, 4.7921e-09],\n",
              "          [3.1493e-06, 3.1066e-07, 5.7525e-09,  ..., 4.3784e-08,\n",
              "           6.4449e-09, 6.7728e-08],\n",
              "          [2.4965e-06, 1.2466e-07, 3.8200e-06,  ..., 1.0562e-05,\n",
              "           8.1027e-09, 9.8062e-05]],\n",
              "\n",
              "         [[9.0812e-11, 6.7607e-09, 8.6499e-07,  ..., 1.1197e-04,\n",
              "           1.2848e-09, 5.7441e-07],\n",
              "          [1.3526e-12, 5.3400e-10, 2.3766e-16,  ..., 2.2723e-06,\n",
              "           1.1824e-09, 1.0335e-12],\n",
              "          [3.8487e-12, 7.7169e-10, 1.4124e-11,  ..., 1.1847e-07,\n",
              "           2.8412e-11, 6.0108e-10],\n",
              "          ...,\n",
              "          [2.1004e-09, 3.9141e-13, 6.3900e-14,  ..., 2.1245e-07,\n",
              "           2.0463e-13, 1.7979e-09],\n",
              "          [4.8483e-14, 5.9687e-10, 1.0806e-12,  ..., 2.7129e-05,\n",
              "           1.0388e-12, 6.9487e-12],\n",
              "          [6.0999e-12, 3.6697e-09, 1.1012e-09,  ..., 3.9483e-06,\n",
              "           2.5750e-10, 1.0869e-05]],\n",
              "\n",
              "         [[9.9583e-11, 3.5769e-13, 7.8920e-09,  ..., 2.5908e-12,\n",
              "           2.6382e-13, 1.1274e-08],\n",
              "          [1.7476e-13, 7.8433e-13, 7.2705e-14,  ..., 3.9177e-15,\n",
              "           1.0367e-14, 8.7890e-12],\n",
              "          [4.0938e-16, 6.6531e-15, 1.1772e-15,  ..., 9.7135e-18,\n",
              "           5.0239e-15, 6.0610e-11],\n",
              "          ...,\n",
              "          [4.5402e-16, 1.2964e-13, 6.8525e-14,  ..., 4.3601e-11,\n",
              "           1.0082e-14, 9.5003e-11],\n",
              "          [5.2535e-14, 1.4155e-15, 2.0788e-15,  ..., 1.1516e-17,\n",
              "           3.0161e-17, 6.5407e-10],\n",
              "          [5.9224e-12, 8.8689e-12, 2.2561e-08,  ..., 3.2110e-12,\n",
              "           2.0952e-12, 5.2266e-08]]],\n",
              "\n",
              "\n",
              "        [[[6.0241e-05, 1.6255e-04, 6.8114e-04,  ..., 7.0331e-04,\n",
              "           5.3578e-05, 6.6257e-05],\n",
              "          [8.1961e-05, 1.4803e-04, 4.2444e-05,  ..., 9.0909e-05,\n",
              "           4.0096e-05, 1.4145e-04],\n",
              "          [5.6525e-04, 6.6766e-05, 1.9535e-04,  ..., 2.3349e-04,\n",
              "           4.9292e-04, 4.4431e-04],\n",
              "          ...,\n",
              "          [8.4906e-04, 4.3940e-04, 2.6049e-04,  ..., 3.6085e-04,\n",
              "           3.3701e-04, 3.7074e-04],\n",
              "          [1.2550e-04, 2.9338e-04, 9.4702e-05,  ..., 4.0443e-04,\n",
              "           1.4071e-04, 1.5472e-04],\n",
              "          [1.2026e-04, 1.1158e-04, 2.0195e-05,  ..., 5.1358e-04,\n",
              "           2.1054e-04, 2.0424e-04]],\n",
              "\n",
              "         [[2.8701e-04, 2.5055e-04, 2.0024e-03,  ..., 4.0497e-03,\n",
              "           5.5813e-04, 6.0136e-04],\n",
              "          [8.6609e-05, 1.8442e-04, 4.6108e-05,  ..., 1.6695e-04,\n",
              "           6.9164e-05, 1.1872e-03],\n",
              "          [3.0841e-04, 7.0198e-04, 1.5005e-04,  ..., 2.1726e-04,\n",
              "           6.2689e-04, 4.2297e-03],\n",
              "          ...,\n",
              "          [9.5421e-05, 1.2348e-03, 7.6242e-05,  ..., 2.7132e-04,\n",
              "           1.9665e-03, 7.9583e-04],\n",
              "          [9.2126e-05, 1.1227e-04, 9.2785e-05,  ..., 9.4590e-05,\n",
              "           1.0573e-03, 1.6010e-03],\n",
              "          [2.0105e-04, 8.7438e-04, 2.0860e-05,  ..., 2.1870e-03,\n",
              "           2.7256e-04, 7.6303e-03]],\n",
              "\n",
              "         [[6.2755e-04, 2.0780e-04, 5.6110e-04,  ..., 9.1351e-05,\n",
              "           1.0758e-04, 8.2097e-05],\n",
              "          [1.2570e-04, 8.6558e-04, 4.2750e-05,  ..., 7.2861e-05,\n",
              "           1.1186e-05, 1.3896e-03],\n",
              "          [9.2418e-05, 7.9035e-04, 3.7632e-04,  ..., 1.0334e-04,\n",
              "           2.6537e-04, 1.8550e-03],\n",
              "          ...,\n",
              "          [1.4959e-04, 1.2259e-04, 6.4473e-04,  ..., 1.2718e-04,\n",
              "           7.7644e-04, 1.0155e-04],\n",
              "          [8.2291e-05, 2.2019e-05, 7.1219e-05,  ..., 1.1536e-04,\n",
              "           7.1248e-04, 6.9385e-04],\n",
              "          [6.5807e-05, 3.4039e-04, 1.6802e-03,  ..., 3.8391e-04,\n",
              "           3.6026e-04, 8.8175e-04]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[7.5583e-08, 4.2181e-10, 2.5622e-07,  ..., 4.8859e-05,\n",
              "           2.0641e-11, 5.3819e-06],\n",
              "          [1.7873e-06, 3.4070e-11, 6.9951e-11,  ..., 5.1022e-07,\n",
              "           5.0358e-12, 6.2317e-05],\n",
              "          [1.7663e-05, 9.1313e-09, 5.2238e-10,  ..., 2.2711e-14,\n",
              "           1.0112e-09, 1.8613e-06],\n",
              "          ...,\n",
              "          [6.3591e-07, 1.3770e-09, 1.8883e-09,  ..., 1.7651e-07,\n",
              "           7.2892e-12, 1.4844e-06],\n",
              "          [1.1241e-07, 2.5574e-15, 4.0579e-09,  ..., 1.2959e-07,\n",
              "           3.0716e-07, 5.1017e-05],\n",
              "          [1.1613e-11, 6.5934e-07, 3.3645e-07,  ..., 1.1306e-04,\n",
              "           3.1431e-05, 9.9602e-05]],\n",
              "\n",
              "         [[2.0929e-14, 4.3420e-08, 1.6368e-06,  ..., 1.8320e-06,\n",
              "           6.3426e-07, 2.0844e-07],\n",
              "          [4.3841e-12, 2.6476e-10, 9.5686e-13,  ..., 3.2423e-05,\n",
              "           2.7809e-10, 2.8870e-09],\n",
              "          [3.0967e-13, 4.7715e-09, 4.3097e-11,  ..., 2.7243e-06,\n",
              "           3.2525e-12, 6.1586e-11],\n",
              "          ...,\n",
              "          [2.4264e-12, 2.6113e-09, 3.7534e-11,  ..., 2.9789e-07,\n",
              "           3.2108e-15, 5.0475e-07],\n",
              "          [2.3742e-15, 2.6576e-11, 1.8094e-12,  ..., 3.1147e-08,\n",
              "           2.9899e-12, 3.3472e-13],\n",
              "          [6.6326e-15, 5.6939e-13, 4.6865e-07,  ..., 6.8859e-06,\n",
              "           1.3523e-09, 4.3262e-08]],\n",
              "\n",
              "         [[3.1075e-13, 2.3330e-13, 1.9282e-12,  ..., 1.4554e-10,\n",
              "           6.5672e-13, 1.2726e-09],\n",
              "          [2.1095e-09, 3.4778e-14, 1.8322e-13,  ..., 6.8491e-15,\n",
              "           1.3423e-16, 1.3846e-07],\n",
              "          [2.9174e-14, 2.5637e-12, 9.6463e-13,  ..., 6.5369e-15,\n",
              "           4.2952e-18, 2.9342e-09],\n",
              "          ...,\n",
              "          [3.3864e-15, 2.1545e-15, 9.5103e-16,  ..., 3.1440e-15,\n",
              "           1.1394e-17, 1.2997e-12],\n",
              "          [1.4401e-16, 3.6001e-18, 5.4775e-14,  ..., 1.4034e-13,\n",
              "           1.0214e-13, 7.4001e-10],\n",
              "          [8.7223e-18, 4.3496e-14, 8.3815e-14,  ..., 6.7169e-14,\n",
              "           1.5472e-09, 7.4616e-11]]],\n",
              "\n",
              "\n",
              "        [[[1.6854e-05, 5.4231e-04, 4.2118e-04,  ..., 4.5016e-04,\n",
              "           6.7428e-05, 1.4025e-04],\n",
              "          [1.0755e-04, 2.3048e-04, 3.9226e-04,  ..., 2.4153e-04,\n",
              "           1.3644e-04, 1.7222e-04],\n",
              "          [8.9267e-04, 1.4943e-04, 2.3339e-04,  ..., 5.7865e-04,\n",
              "           1.5560e-04, 1.3891e-04],\n",
              "          ...,\n",
              "          [1.4722e-04, 7.6717e-05, 6.8248e-05,  ..., 5.4755e-04,\n",
              "           1.8049e-04, 2.3674e-04],\n",
              "          [4.3820e-05, 1.0426e-04, 1.2032e-05,  ..., 6.9085e-04,\n",
              "           1.4359e-04, 1.8263e-04],\n",
              "          [1.8561e-04, 6.1369e-05, 3.8920e-06,  ..., 6.2821e-04,\n",
              "           7.7214e-05, 1.2895e-04]],\n",
              "\n",
              "         [[8.7685e-06, 1.4700e-04, 6.3061e-04,  ..., 5.7570e-05,\n",
              "           2.5988e-04, 5.6387e-03],\n",
              "          [4.8511e-05, 1.1519e-03, 1.7424e-03,  ..., 4.2144e-04,\n",
              "           6.1333e-04, 8.2265e-03],\n",
              "          [2.0260e-03, 4.3611e-05, 1.4329e-04,  ..., 1.8114e-04,\n",
              "           2.2491e-04, 4.2234e-04],\n",
              "          ...,\n",
              "          [4.2538e-05, 5.0305e-05, 2.9853e-05,  ..., 1.0277e-04,\n",
              "           6.0871e-04, 2.3063e-03],\n",
              "          [4.4294e-06, 3.6133e-05, 1.1981e-05,  ..., 8.2095e-05,\n",
              "           1.7171e-03, 1.0389e-03],\n",
              "          [5.7924e-04, 7.4237e-05, 1.9726e-05,  ..., 8.2569e-04,\n",
              "           4.3102e-04, 2.6096e-03]],\n",
              "\n",
              "         [[6.4197e-06, 1.4473e-03, 3.8441e-04,  ..., 2.8939e-04,\n",
              "           2.1446e-04, 5.7852e-04],\n",
              "          [3.0350e-05, 3.4950e-04, 6.1891e-05,  ..., 1.8666e-04,\n",
              "           3.0082e-04, 3.9223e-03],\n",
              "          [1.1870e-04, 3.7171e-03, 1.7943e-05,  ..., 3.6436e-04,\n",
              "           8.7656e-05, 7.7114e-04],\n",
              "          ...,\n",
              "          [6.4268e-05, 3.2090e-04, 1.6610e-03,  ..., 2.4900e-04,\n",
              "           5.1803e-05, 2.8113e-03],\n",
              "          [2.0006e-05, 2.1820e-04, 1.5105e-04,  ..., 1.0003e-04,\n",
              "           2.8354e-03, 8.1714e-04],\n",
              "          [9.1268e-04, 1.1858e-03, 3.7654e-05,  ..., 3.8526e-03,\n",
              "           9.9316e-05, 2.1352e-03]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[1.7860e-11, 1.9704e-08, 2.3449e-08,  ..., 1.7031e-04,\n",
              "           1.0829e-05, 3.2198e-10],\n",
              "          [2.4915e-06, 5.0400e-14, 4.9210e-14,  ..., 1.1593e-09,\n",
              "           8.5154e-04, 5.6486e-09],\n",
              "          [1.2278e-13, 1.4537e-09, 7.1222e-11,  ..., 3.9619e-11,\n",
              "           2.4363e-10, 1.9913e-03],\n",
              "          ...,\n",
              "          [6.3811e-05, 2.8460e-09, 5.0383e-14,  ..., 2.9495e-09,\n",
              "           5.8504e-09, 3.0646e-09],\n",
              "          [1.0371e-06, 8.9651e-11, 7.6514e-12,  ..., 6.6074e-07,\n",
              "           2.4878e-07, 2.2229e-09],\n",
              "          [2.6801e-04, 3.6839e-07, 1.1969e-08,  ..., 1.9616e-06,\n",
              "           2.3961e-07, 2.5179e-05]],\n",
              "\n",
              "         [[2.0022e-15, 9.1741e-08, 7.7903e-08,  ..., 5.6280e-06,\n",
              "           3.7305e-07, 3.0320e-07],\n",
              "          [3.3644e-13, 1.9270e-12, 3.7922e-17,  ..., 7.4673e-10,\n",
              "           1.1943e-10, 1.1516e-11],\n",
              "          [4.0299e-17, 1.9113e-10, 2.4192e-12,  ..., 9.1091e-10,\n",
              "           4.0673e-08, 9.5494e-10],\n",
              "          ...,\n",
              "          [7.7374e-13, 2.0960e-17, 6.4089e-16,  ..., 3.8521e-06,\n",
              "           8.4201e-14, 2.0530e-08],\n",
              "          [7.4319e-14, 9.8980e-15, 4.1928e-18,  ..., 2.4249e-08,\n",
              "           1.9766e-15, 1.4916e-11],\n",
              "          [1.9199e-10, 6.6726e-15, 1.1493e-12,  ..., 1.9239e-06,\n",
              "           8.2140e-12, 8.3622e-11]],\n",
              "\n",
              "         [[3.1437e-16, 2.3588e-16, 1.0519e-13,  ..., 7.6261e-10,\n",
              "           1.4455e-15, 4.7622e-08],\n",
              "          [1.1852e-13, 2.7447e-16, 1.3542e-18,  ..., 1.5354e-17,\n",
              "           9.0551e-13, 2.0437e-09],\n",
              "          [3.2472e-20, 1.7063e-16, 2.7624e-14,  ..., 8.3621e-15,\n",
              "           2.8779e-14, 5.6133e-09],\n",
              "          ...,\n",
              "          [5.7941e-15, 4.1003e-24, 4.1208e-14,  ..., 9.3063e-20,\n",
              "           1.0817e-15, 2.5989e-13],\n",
              "          [5.3589e-14, 1.5430e-17, 1.0141e-16,  ..., 8.9286e-17,\n",
              "           1.2713e-17, 1.3639e-10],\n",
              "          [2.7548e-14, 2.0798e-16, 3.2446e-13,  ..., 8.4755e-11,\n",
              "           3.7746e-12, 1.0353e-09]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[3.3938e-08, 9.0600e-06, 1.3228e-05,  ..., 1.6797e-04,\n",
              "           2.4640e-06, 2.3420e-09],\n",
              "          [1.0575e-05, 9.9533e-05, 8.4004e-05,  ..., 2.7644e-04,\n",
              "           1.3988e-04, 1.1319e-05],\n",
              "          [7.5769e-06, 1.9181e-04, 3.0890e-04,  ..., 2.1377e-04,\n",
              "           2.6332e-04, 5.8988e-05],\n",
              "          ...,\n",
              "          [3.3094e-03, 2.7639e-04, 7.9587e-04,  ..., 4.7735e-04,\n",
              "           8.5429e-05, 5.2807e-05],\n",
              "          [2.3919e-03, 2.3562e-05, 2.3897e-04,  ..., 1.5657e-05,\n",
              "           1.3773e-04, 4.9038e-06],\n",
              "          [1.4830e-06, 2.0126e-05, 6.8868e-05,  ..., 2.2704e-06,\n",
              "           5.6467e-06, 1.7310e-06]],\n",
              "\n",
              "         [[2.5353e-08, 4.6734e-06, 7.8948e-06,  ..., 5.2210e-05,\n",
              "           1.5538e-06, 1.3978e-09],\n",
              "          [6.3084e-05, 4.6765e-03, 1.0264e-04,  ..., 7.3898e-04,\n",
              "           8.6086e-05, 4.1214e-04],\n",
              "          [9.5555e-05, 3.7617e-04, 3.4066e-04,  ..., 2.3788e-04,\n",
              "           1.3770e-04, 3.3987e-04],\n",
              "          ...,\n",
              "          [1.8945e-05, 7.9789e-05, 1.2390e-04,  ..., 3.4584e-04,\n",
              "           3.1003e-04, 5.9123e-05],\n",
              "          [1.6271e-06, 1.7248e-05, 1.0059e-04,  ..., 3.0333e-05,\n",
              "           1.2740e-04, 5.6417e-06],\n",
              "          [7.2872e-06, 2.0779e-04, 1.0866e-04,  ..., 6.1858e-07,\n",
              "           3.1855e-06, 1.3524e-06]],\n",
              "\n",
              "         [[7.8774e-06, 1.2112e-05, 2.3563e-05,  ..., 2.2208e-03,\n",
              "           2.1468e-04, 9.6414e-01],\n",
              "          [5.7708e-06, 9.0034e-05, 4.2417e-05,  ..., 2.6151e-04,\n",
              "           2.1939e-04, 2.0104e-03],\n",
              "          [4.7181e-06, 1.5664e-04, 6.2966e-04,  ..., 3.1878e-04,\n",
              "           4.6627e-04, 3.9492e-03],\n",
              "          ...,\n",
              "          [5.9661e-05, 2.8943e-04, 2.7389e-04,  ..., 4.5897e-04,\n",
              "           2.4096e-04, 3.0971e-02],\n",
              "          [4.0241e-06, 4.2352e-05, 2.6189e-04,  ..., 1.4359e-05,\n",
              "           6.8692e-04, 3.4397e-03],\n",
              "          [9.6019e-07, 3.3422e-05, 9.2632e-05,  ..., 4.5438e-06,\n",
              "           1.5041e-05, 7.3533e-06]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 4.6281e-37, 4.0638e-43,  ..., 1.3965e-30,\n",
              "           1.2628e-39, 0.0000e+00],\n",
              "          [4.0491e-33, 1.5233e-26, 1.2050e-35,  ..., 2.7083e-19,\n",
              "           1.8205e-33, 1.1210e-44],\n",
              "          [7.0314e-38, 1.8855e-18, 4.0596e-26,  ..., 6.2585e-26,\n",
              "           1.4475e-25, 3.7407e-31],\n",
              "          ...,\n",
              "          [9.8507e-31, 5.8587e-26, 9.2952e-24,  ..., 9.9627e-01,\n",
              "           5.4588e-26, 1.8081e-24],\n",
              "          [1.5386e-42, 1.7087e-36, 7.2216e-05,  ..., 3.1480e-35,\n",
              "           6.2347e-18, 3.8474e-40],\n",
              "          [3.6746e-41, 2.0252e-28, 2.6866e-34,  ..., 1.5029e-39,\n",
              "           1.4124e-36, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 7.9847e-40, 2.5878e-34,  ..., 5.4303e-24,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [4.2757e-39, 7.5891e-17, 9.8706e-18,  ..., 2.7633e-13,\n",
              "           3.2215e-21, 1.2549e-22],\n",
              "          [1.4786e-34, 2.7587e-23, 1.6633e-28,  ..., 8.9261e-08,\n",
              "           1.0034e-21, 8.1709e-16],\n",
              "          ...,\n",
              "          [1.1512e-15, 2.0176e-23, 1.0748e-24,  ..., 3.8114e-16,\n",
              "           3.0025e-27, 2.6821e-31],\n",
              "          [1.2909e-33, 1.6987e-41, 8.0386e-10,  ..., 2.5718e-04,\n",
              "           8.2791e-27, 7.6658e-36],\n",
              "          [0.0000e+00, 1.4045e-27, 5.5677e-31,  ..., 2.5739e-40,\n",
              "           6.4445e-40, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 2.5100e-28, 9.3385e-41,  ..., 2.8112e-30,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [8.1588e-40, 3.0502e-29, 3.5662e-38,  ..., 1.0847e-22,\n",
              "           1.7365e-34, 0.0000e+00],\n",
              "          [3.0986e-36, 2.7668e-30, 1.9912e-28,  ..., 6.6570e-20,\n",
              "           3.9026e-27, 9.2503e-25],\n",
              "          ...,\n",
              "          [7.6326e-28, 1.6942e-27, 1.8271e-34,  ..., 1.3394e-16,\n",
              "           5.7727e-31, 2.4801e-27],\n",
              "          [1.2277e-40, 8.4078e-44, 3.1923e-16,  ..., 1.7049e-22,\n",
              "           1.6995e-26, 5.7689e-35],\n",
              "          [0.0000e+00, 9.0169e-37, 5.9564e-12,  ..., 1.1477e-42,\n",
              "           4.0638e-44, 0.0000e+00]]],\n",
              "\n",
              "\n",
              "        [[[8.4102e-07, 5.9282e-06, 2.5958e-06,  ..., 2.4502e-05,\n",
              "           1.3725e-05, 6.6421e-07],\n",
              "          [6.8251e-06, 1.1321e-04, 6.2983e-04,  ..., 8.9706e-05,\n",
              "           2.9260e-05, 9.7504e-06],\n",
              "          [8.2629e-03, 1.1721e-04, 1.0666e-03,  ..., 2.7896e-04,\n",
              "           6.5877e-05, 3.4071e-06],\n",
              "          ...,\n",
              "          [2.7516e-05, 3.2234e-04, 5.2810e-04,  ..., 2.1626e-05,\n",
              "           3.4720e-04, 7.5929e-05],\n",
              "          [7.5170e-04, 1.0454e-04, 3.7419e-04,  ..., 3.7185e-04,\n",
              "           3.9117e-05, 1.0729e-06],\n",
              "          [3.9002e-08, 3.1508e-08, 7.1347e-05,  ..., 5.7749e-05,\n",
              "           3.6533e-05, 1.9825e-14]],\n",
              "\n",
              "         [[8.2235e-07, 3.5381e-06, 1.3358e-06,  ..., 1.4623e-05,\n",
              "           4.1466e-06, 3.9642e-07],\n",
              "          [9.5464e-05, 4.3845e-04, 6.2970e-04,  ..., 6.1809e-04,\n",
              "           4.0474e-05, 6.8439e-05],\n",
              "          [9.5742e-05, 7.7029e-05, 1.0084e-03,  ..., 1.6607e-03,\n",
              "           3.9318e-05, 5.1632e-06],\n",
              "          ...,\n",
              "          [4.5952e-05, 3.3498e-03, 3.1612e-03,  ..., 1.3557e-04,\n",
              "           1.5067e-04, 7.6032e-05],\n",
              "          [8.5682e-06, 5.1685e-05, 2.4675e-04,  ..., 2.2273e-04,\n",
              "           6.8439e-05, 1.3981e-05],\n",
              "          [1.0643e-10, 3.3427e-08, 4.2582e-05,  ..., 1.6001e-04,\n",
              "           6.3625e-05, 2.5353e-14]],\n",
              "\n",
              "         [[3.1770e-03, 2.4905e-05, 3.2969e-04,  ..., 3.9487e-04,\n",
              "           5.1895e-03, 8.3650e-01],\n",
              "          [9.5983e-06, 9.6693e-05, 2.2454e-04,  ..., 3.5036e-04,\n",
              "           6.7811e-05, 1.5268e-03],\n",
              "          [9.3472e-05, 1.1490e-04, 6.6705e-04,  ..., 5.8580e-04,\n",
              "           8.7938e-05, 1.4533e-04],\n",
              "          ...,\n",
              "          [8.4577e-05, 4.7563e-04, 1.5713e-03,  ..., 2.5591e-05,\n",
              "           9.0255e-04, 8.1085e-04],\n",
              "          [2.1935e-06, 1.4879e-04, 4.5199e-04,  ..., 4.7229e-04,\n",
              "           1.1890e-04, 3.5910e-05],\n",
              "          [3.4130e-12, 7.9531e-08, 9.8915e-05,  ..., 6.5127e-05,\n",
              "           6.4313e-05, 3.7254e-14]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 4.0638e-44, 0.0000e+00,  ..., 2.9772e-21,\n",
              "           2.5682e-41, 0.0000e+00],\n",
              "          [1.6748e-38, 4.1850e-31, 1.1828e-17,  ..., 9.2985e-34,\n",
              "           7.5986e-39, 0.0000e+00],\n",
              "          [8.4502e-33, 6.6001e-37, 7.3178e-23,  ..., 4.1093e-21,\n",
              "           6.7269e-32, 0.0000e+00],\n",
              "          ...,\n",
              "          [8.7244e-32, 1.4636e-19, 1.7912e-24,  ..., 1.7562e-12,\n",
              "           8.5492e-23, 1.2191e-31],\n",
              "          [4.6243e-44, 2.2185e-20, 5.9864e-17,  ..., 1.2229e-13,\n",
              "           4.9567e-28, 0.0000e+00],\n",
              "          [0.0000e+00, 2.2421e-44, 1.5815e-30,  ..., 2.8888e-25,\n",
              "           1.0553e-33, 0.0000e+00]],\n",
              "\n",
              "         [[1.9284e-38, 0.0000e+00, 0.0000e+00,  ..., 2.2486e-19,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [8.1934e-42, 1.2109e-30, 1.8418e-23,  ..., 7.9632e-18,\n",
              "           2.9087e-27, 1.7224e-21],\n",
              "          [8.9545e-40, 2.7468e-38, 1.6688e-22,  ..., 7.1835e-17,\n",
              "           2.1553e-24, 1.8387e-40],\n",
              "          ...,\n",
              "          [8.0835e-35, 6.6491e-12, 1.0264e-20,  ..., 4.1530e-26,\n",
              "           9.4854e-27, 7.3218e-29],\n",
              "          [0.0000e+00, 2.3293e-21, 2.3468e-19,  ..., 1.5648e-24,\n",
              "           1.1080e-38, 7.9077e-37],\n",
              "          [0.0000e+00, 0.0000e+00, 1.2909e-33,  ..., 2.3798e-29,\n",
              "           2.2817e-37, 0.0000e+00]],\n",
              "\n",
              "         [[2.9831e-40, 1.7159e-40, 0.0000e+00,  ..., 7.2394e-26,\n",
              "           5.3249e-43, 4.2039e-44],\n",
              "          [2.4988e-23, 1.6283e-20, 3.0005e-09,  ..., 8.0523e-35,\n",
              "           3.6388e-39, 5.4740e-39],\n",
              "          [4.6319e-33, 1.6032e-36, 8.5543e-11,  ..., 2.0133e-24,\n",
              "           4.8986e-30, 0.0000e+00],\n",
              "          ...,\n",
              "          [3.7426e-31, 3.4850e-21, 9.1503e-22,  ..., 3.3956e-38,\n",
              "           1.3289e-26, 3.4141e-31],\n",
              "          [5.4649e-33, 1.3247e-27, 6.0143e-23,  ..., 1.5828e-21,\n",
              "           1.1289e-34, 1.4013e-45],\n",
              "          [0.0000e+00, 0.0000e+00, 6.7814e-26,  ..., 1.3317e-24,\n",
              "           5.9585e-31, 0.0000e+00]]],\n",
              "\n",
              "\n",
              "        [[[1.2982e-07, 5.6420e-10, 2.7960e-05,  ..., 1.2497e-05,\n",
              "           2.5224e-06, 1.2355e-07],\n",
              "          [3.0285e-06, 2.6526e-04, 7.3921e-04,  ..., 1.5364e-04,\n",
              "           6.6771e-05, 1.1151e-05],\n",
              "          [6.8764e-04, 4.1472e-04, 2.7176e-04,  ..., 4.2753e-04,\n",
              "           3.6907e-04, 4.6041e-05],\n",
              "          ...,\n",
              "          [1.1264e-03, 1.6989e-04, 6.2074e-04,  ..., 6.8041e-04,\n",
              "           1.8190e-04, 5.6136e-05],\n",
              "          [1.0494e-04, 1.6159e-04, 2.8054e-04,  ..., 3.2103e-04,\n",
              "           2.6278e-04, 5.4759e-05],\n",
              "          [2.7656e-08, 2.3636e-05, 1.0620e-04,  ..., 2.0568e-04,\n",
              "           2.0231e-05, 1.5140e-06]],\n",
              "\n",
              "         [[4.4202e-07, 3.3674e-10, 9.3357e-06,  ..., 8.7917e-06,\n",
              "           1.3038e-06, 7.8229e-08],\n",
              "          [1.1173e-04, 6.2757e-03, 1.3729e-03,  ..., 5.9937e-03,\n",
              "           6.4349e-04, 1.2739e-04],\n",
              "          [6.9918e-04, 3.1543e-04, 1.6159e-03,  ..., 8.8066e-04,\n",
              "           2.3686e-04, 5.8680e-04],\n",
              "          ...,\n",
              "          [6.6480e-04, 4.6993e-04, 5.2590e-03,  ..., 2.6898e-03,\n",
              "           2.1738e-04, 2.1374e-03],\n",
              "          [1.4751e-05, 2.6482e-04, 2.1642e-04,  ..., 3.8296e-04,\n",
              "           1.0668e-04, 4.2867e-05],\n",
              "          [1.3161e-07, 3.8505e-06, 3.1374e-05,  ..., 1.0710e-03,\n",
              "           1.6649e-05, 9.0359e-07]],\n",
              "\n",
              "         [[7.2197e-05, 3.8860e-09, 4.2839e-05,  ..., 8.6710e-05,\n",
              "           9.6227e-05, 6.1857e-02],\n",
              "          [7.4706e-06, 1.2195e-04, 7.5990e-04,  ..., 1.7105e-04,\n",
              "           1.1652e-04, 4.8580e-04],\n",
              "          [4.9694e-05, 2.7128e-04, 3.8873e-04,  ..., 4.6323e-04,\n",
              "           7.7719e-04, 7.1412e-03],\n",
              "          ...,\n",
              "          [8.4061e-06, 2.2753e-04, 9.8217e-04,  ..., 1.3497e-03,\n",
              "           1.2377e-03, 1.7813e-04],\n",
              "          [7.3255e-06, 1.5850e-04, 7.0715e-04,  ..., 4.7905e-04,\n",
              "           5.3550e-04, 2.0602e-04],\n",
              "          [3.7357e-08, 9.3127e-06, 1.4865e-04,  ..., 3.0974e-04,\n",
              "           2.2523e-05, 1.6924e-05]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[1.5554e-43, 0.0000e+00, 1.0584e-40,  ..., 9.4741e-36,\n",
              "           4.6243e-44, 0.0000e+00],\n",
              "          [1.0228e-34, 6.2282e-35, 3.6234e-31,  ..., 5.0526e-16,\n",
              "           4.3081e-34, 1.4013e-45],\n",
              "          [9.3367e-35, 5.9652e-29, 5.1820e-37,  ..., 2.0218e-23,\n",
              "           3.2284e-22, 1.5911e-35],\n",
              "          ...,\n",
              "          [1.4414e-33, 3.1748e-25, 2.7705e-16,  ..., 2.7029e-03,\n",
              "           5.8204e-18, 2.9585e-13],\n",
              "          [9.0517e-40, 9.3270e-09, 3.0663e-21,  ..., 4.6315e-12,\n",
              "           3.0571e-08, 1.9134e-25],\n",
              "          [0.0000e+00, 7.6483e-29, 1.9038e-35,  ..., 2.4355e-21,\n",
              "           7.8052e-32, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 1.4013e-44,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [6.6012e-36, 3.1476e-17, 6.0733e-10,  ..., 7.3368e-07,\n",
              "           3.0678e-18, 1.6116e-27],\n",
              "          [3.9509e-26, 6.3855e-22, 5.0277e-23,  ..., 1.3624e-14,\n",
              "           2.4272e-23, 1.2977e-34],\n",
              "          ...,\n",
              "          [4.7693e-40, 1.5615e-20, 7.3337e-16,  ..., 8.8859e-07,\n",
              "           4.8398e-18, 2.1402e-28],\n",
              "          [9.0524e-43, 4.1899e-25, 1.8663e-14,  ..., 9.9619e-01,\n",
              "           3.9631e-28, 4.4941e-22],\n",
              "          [0.0000e+00, 2.4836e-34, 4.4704e-29,  ..., 1.2190e-26,\n",
              "           1.4941e-40, 1.7796e-43]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 1.9369e-39,  ..., 1.7022e-34,\n",
              "           1.4013e-45, 0.0000e+00],\n",
              "          [3.0917e-31, 4.9365e-32, 1.4645e-22,  ..., 1.0582e-25,\n",
              "           1.0271e-34, 5.6052e-45],\n",
              "          [6.5914e-29, 1.6607e-06, 3.1333e-38,  ..., 5.2096e-10,\n",
              "           4.9026e-25, 1.7560e-32],\n",
              "          ...,\n",
              "          [3.9100e-31, 8.8960e-26, 3.0248e-04,  ..., 1.0220e-16,\n",
              "           2.5025e-24, 1.9406e-23],\n",
              "          [7.9165e-24, 2.3837e-25, 4.5201e-18,  ..., 1.0584e-16,\n",
              "           1.4943e-23, 5.5592e-21],\n",
              "          [0.0000e+00, 6.3685e-41, 4.2475e-16,  ..., 9.0364e-28,\n",
              "           2.0096e-39, 4.3440e-44]]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOZ_Sq5dgKbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAuX27tHUPX"
      },
      "source": [
        "#dadada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6Zvz3aj_L9",
        "outputId": "001dd363-1ce2-474a-ddcd-4f73833ac027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "# def rfem(x):\n",
        "#   channel = x.shape[1]\n",
        "#   #branch1\n",
        "  \n",
        "#   x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "#   x1 = torch.nn.ReLU()(x1)\n",
        "#   x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "#   #branch2\n",
        "#   x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "#   x2 = torch.nn.ReLU()(x2)\n",
        "#   x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "#   x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "#   x2 = torch.nn.ReLU()(x2)\n",
        "#   x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "#   #branch3\n",
        "#   x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "#   x2 = torch.nn.ReLU()(x2)\n",
        "#   x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "#   x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "#   x3 = torch.nn.ReLU()(x3)\n",
        "#   x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "#   #branch4\n",
        "#   x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "#   x4 = torch.nn.ReLU()(x4)\n",
        "#   x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "#   x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "#   x4 = torch.nn.ReLU()(x4)\n",
        "#   x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "#   x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "#   x4 = torch.nn.ReLU()(x4)\n",
        "#   x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "#   x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "#   x4 = torch.nn.ReLU()(x4)\n",
        "#   x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "#   x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "#   x4 = torch.nn.ReLU()(x4)\n",
        "#   x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "#   combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "#   return torch.add(combined_x, x/0.5)\n",
        "\n",
        "# def ftb(current_layer, former_layer=None):\n",
        "#   x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "#   x = torch.nn.ReLU(inplace=True)(x)\n",
        "#   x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "#   if former_layer is None:\n",
        "#     return x\n",
        "#   elif x.shape == former_layer.shape:\n",
        "#     return torch.add(x, former_layer)\n",
        "\n",
        "#   else:\n",
        "#     d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "#     return torch.add(x, d)\n",
        "\n",
        "\n",
        "# def training(feature_maps):\n",
        "#   conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "#   conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "#   conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "#   conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "#   conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "#   conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "#   conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "#   conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "#   conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "#   rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "#   rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "#   rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "#   rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "#   ftb_4 = ftb(rfem14)\n",
        "#   ftb_3 = ftb(rfem13)\n",
        "#   ftb_2 = ftb(rfem12)\n",
        "#   ftb_1 = ftb(rfem11)\n",
        "\n",
        "#   rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "#   rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "#   rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "#   rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "#   det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "#   det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "#   det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "#   det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "#   D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "#   pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "#   torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "#   return D\n",
        "\n",
        "# class FeatureExtractor(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(FeatureExtractor, self).__init__()\n",
        "#     model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "#     device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     model = model.to(device)\n",
        "#     for param in model.named_parameters():\n",
        "#       param[1].requres_grad = True\n",
        "#     # self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#     model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "#     return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "#     self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "#   def forward(self, images):\n",
        "#     return training(self.backbone(images))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    print(out.shape)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "2aefaa1b-290a-42d8-e8f2-6fabcc80cc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnKE3neESZ5",
        "outputId": "aac4e7e1-10e2-467c-915f-0f79864c086e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 512, 40, 30])\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRW4iC1uXf4"
      },
      "outputs": [],
      "source": [
        "for name, param in detector.named_parameters():\n",
        "    print(name, param.data)\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2nsbro-Tse"
      },
      "outputs": [],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dOW6SREUn2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jVAuX27tHUPX",
        "FF2pFCXWgHC4",
        "2VF-O12KLyIk"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}