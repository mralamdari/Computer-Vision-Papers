{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "887e331a-a853-4da2-d154-6b32b3982bf4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch_intermediate_layer_getter\n",
        "from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik328v3BJgoF"
      },
      "source": [
        "#Test Part"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "xs = np.array([[0., 0., 0., 0.], [0., 0., 0., 1.], [0., 0., 1., 0.], [0., 0., 1., 1.],\n",
        "              [0., 1., 0., 0.], [0., 1., 0., 1.], [0., 1., 1., 0.], [0., 1., 1., 1.],\n",
        "              [1., 0., 0., 0.], [1., 0., 0., 1.], [1., 0., 1., 0.], [1., 0., 1., 1.],\n",
        "              [1., 1., 0., 0.], [1., 1., 0., 1.], [1., 1., 1., 0.], [1., 1., 1., 1.]], dtype=float)\n",
        "\n",
        "ys = np.array([[0.], [1.], [1.], [0.], [1.], [0.], [0.], [1.], [1.], [0.], [0.], [1.],\n",
        "               [0.], [1.], [1.], [0.]], dtype=float)\n",
        "\n",
        "x_var = Variable(Tensor(xs), requires_grad=False)\n",
        "y_var = Variable(Tensor(ys), requires_grad=False)\n",
        "\n",
        "EPOCHS = 1000\n",
        "\n",
        "\n",
        "# Helper function to train the network\n",
        "def train(model, x_train, y_train, criterion, optimizer, epochs):\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # Make predictions (forward propagation)\n",
        "        y_pred = model(x_train)\n",
        "\n",
        "        # Compute and print the loss every hundred epochs\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        if i % 100 == 0:\n",
        "            print('Loss:', loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "class LinearBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_nums, out_nums):\n",
        "        super(LinearBlock, self).__init__()\n",
        "        self.linear = nn.Linear(in_nums, out_nums)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "\n",
        "class FullyConnectedNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, neurons):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        previous = input_size\n",
        "        for i in range(len(neurons)):\n",
        "            self.blocks.append(LinearBlock(previous, neurons[i]))\n",
        "            previous = neurons[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Crete a network with 2 hidden layers and 1 output layer, with sigmoid activations\n",
        "fcnet01 = FullyConnectedNet(4, # We have a four dimensional input\n",
        "                            [4, 4, 1, 5, 7, 8, 20, 2, 1], # We two hidden layers with 4 neurons each, and an output layer[nn.Sigmoid(), nn.Sigmoid(), nn.Sigmoid()] # Using sigmoid for activation\n",
        "                            )\n",
        "\n",
        "print(fcnet01.blocks)\n",
        "# Since it's a 0-1 problem, we will use Binary Cross Entropy as our loss function\n",
        "# criterion = nn.BCELoss()\n",
        "# optimizer = torch.optim.SGD(fcnet01.parameters(), lr=0.01)\n",
        "\n",
        "# Then, our usual training loop\n",
        "# train(fcnet01, x_var, y_var, criterion, optimizer, EPOCHS)\n",
        "print()\n",
        "\n",
        "# for i in fcnet01.parameters():\n",
        "  # print(i)"
      ],
      "metadata": {
        "id": "gMt1EonTUCqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qez3yY2nGbmr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Helper function to train the network\n",
        "def train(model, x_train, y_train, criterion, optimizer, epochs):\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # Make predictions (forward propagation)\n",
        "        y_pred = model(x_train)\n",
        "\n",
        "        # Compute and print the loss every hundred epochs\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        if i % 100 == 0:\n",
        "            print('Loss:', loss)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Create a reusable module\n",
        "# PyTorch makes writing modular OO code extremely easy\n",
        "class Conv2dBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_nums, out_nums, activation='relu'):\n",
        "        super(Conv2dBlock, self).__init__()\n",
        "        self.Conv2d = torch.nn.Conv2d(in_nums, out_nums, kernel_size=1)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.activation == 'softmax':\n",
        "          return torch.nn.Softmax(self.Conv2d(x))\n",
        "        else:\n",
        "          return torch.nn.ReLU(self.Conv2d(x))\n",
        "\n",
        "\n",
        "\n",
        "class FullyConnectedNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, neurons):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "\n",
        "        # For now, we will have a linear layer followed by an activation function\n",
        "        # assert len(neurons) == len(activations), 'Number of activations must be equal to the number of activations'\n",
        "\n",
        "        # We will need a list of blocks cascaded one after the other, so we keep them in a ModuleList instead of a Python list\n",
        "        self.blocks = torch.nn.ModuleList()\n",
        "\n",
        "        previous = input_size\n",
        "        for i in range(len(neurons)):\n",
        "            self.blocks.append(Conv2dBlock(previous, neurons[i]))\n",
        "            previous = neurons[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Pass the input through each block\"\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Crete a network with 2 hidden layers and 1 output layer, with sigmoid activations\n",
        "fcnet01 = FullyConnectedNet(4, # We have a four dimensional input\n",
        "                            [4, 4, 1] # We two hidden layers with 4 neurons each, and an output layer\n",
        "                                       # with 1 neuron\n",
        "                             # Using sigmoid for activation\n",
        "                            )\n",
        "\n",
        "# Since it's a 0-1 problem, we will use Binary Cross Entropy as our loss function\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(fcnet01.parameters(), lr=0.01)\n",
        "\n",
        "# Then, our usual training loop\n",
        "# train(fcnet01, x_var, y_var, criterion, optimizer, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QEi6QcJoGfaC"
      },
      "outputs": [],
      "source": [
        "fcnet02 = FullyConnectedNet(4, [8, 10, 10, 8, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qEiHwfh0Jic3"
      },
      "outputs": [],
      "source": [
        "def rfem(x):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "  return torch.add(combined_x, x/0.5)\n",
        "\n",
        "def ftb(current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    return torch.add(x, d)\n",
        "\n",
        "\n",
        "def training(feature_maps):\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "  rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "  rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "  rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "  ftb_4 = ftb(rfem14)\n",
        "  ftb_3 = ftb(rfem13)\n",
        "  ftb_2 = ftb(rfem12)\n",
        "  ftb_1 = ftb(rfem11)\n",
        "\n",
        "  rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "  rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "  rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "  rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "  det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "  det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "  det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "  det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "  D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "  pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "  torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "  return D\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    for param in model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "    return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "    self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.backbone(images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fGOVAbO9J1DV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d52354-0fe2-4237-a5d9-8d039b5ea987"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[9.8962e-05, 4.4256e-02, 1.6262e-03,  ..., 3.6744e-03,\n",
              "           7.5087e-05, 7.7983e-03],\n",
              "          [2.1250e-04, 2.0445e-04, 1.0175e-04,  ..., 5.9498e-05,\n",
              "           6.5472e-05, 1.0263e-03],\n",
              "          [3.1021e-04, 1.7861e-03, 3.6780e-03,  ..., 1.3946e-03,\n",
              "           2.4746e-04, 2.5430e-05],\n",
              "          ...,\n",
              "          [3.2847e-04, 3.2236e-04, 5.4410e-04,  ..., 2.8115e-04,\n",
              "           2.4814e-04, 2.4909e-03],\n",
              "          [1.1798e-04, 6.4517e-04, 1.1276e-04,  ..., 1.4343e-03,\n",
              "           7.7211e-04, 3.0207e-03],\n",
              "          [4.2949e-05, 1.2503e-04, 4.2920e-05,  ..., 3.3875e-04,\n",
              "           1.5858e-04, 3.0216e-04]],\n",
              "\n",
              "         [[2.3630e-05, 6.5888e-05, 6.7214e-04,  ..., 3.0885e-04,\n",
              "           3.3284e-05, 2.9799e-04],\n",
              "          [1.3044e-04, 6.0573e-05, 2.7108e-05,  ..., 5.4305e-05,\n",
              "           1.4392e-05, 2.6853e-05],\n",
              "          [1.1299e-04, 1.0988e-05, 1.1283e-04,  ..., 5.8271e-05,\n",
              "           5.7088e-05, 4.9526e-05],\n",
              "          ...,\n",
              "          [1.0033e-04, 8.5709e-04, 4.4999e-04,  ..., 2.2577e-04,\n",
              "           6.5010e-04, 1.4839e-03],\n",
              "          [1.8987e-05, 3.4344e-05, 5.3188e-04,  ..., 1.8975e-03,\n",
              "           5.9411e-05, 1.1885e-03],\n",
              "          [3.7807e-05, 3.2153e-05, 4.8128e-05,  ..., 3.7752e-04,\n",
              "           1.7214e-04, 1.1076e-03]],\n",
              "\n",
              "         [[2.7218e-05, 2.3255e-04, 3.0637e-04,  ..., 7.7424e-04,\n",
              "           1.5998e-04, 1.2529e-04],\n",
              "          [2.2251e-05, 5.8879e-05, 3.3251e-05,  ..., 4.1911e-04,\n",
              "           8.7526e-04, 8.9113e-05],\n",
              "          [5.2036e-05, 2.4642e-04, 5.2816e-04,  ..., 1.8335e-03,\n",
              "           1.0214e-04, 1.5216e-04],\n",
              "          ...,\n",
              "          [1.2530e-04, 8.8947e-03, 1.3494e-03,  ..., 8.2751e-05,\n",
              "           8.7903e-04, 5.6991e-04],\n",
              "          [1.5796e-05, 9.0050e-05, 1.1852e-03,  ..., 1.0843e-03,\n",
              "           4.6715e-04, 2.5896e-04],\n",
              "          [2.4885e-05, 1.1945e-05, 2.3452e-05,  ..., 8.3864e-05,\n",
              "           3.8849e-05, 1.9499e-04]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[6.6404e-17, 4.1938e-19, 5.8879e-14,  ..., 5.3040e-16,\n",
              "           1.7181e-13, 1.1031e-07],\n",
              "          [9.1672e-21, 1.0607e-14, 1.4136e-10,  ..., 1.8717e-14,\n",
              "           1.9831e-14, 2.3054e-12],\n",
              "          [3.6195e-16, 3.0820e-18, 5.3257e-11,  ..., 3.3813e-14,\n",
              "           9.3135e-18, 4.7550e-16],\n",
              "          ...,\n",
              "          [8.7336e-11, 1.4305e-13, 1.0188e-11,  ..., 9.9083e-13,\n",
              "           9.0439e-14, 3.9178e-13],\n",
              "          [6.5255e-15, 2.8226e-17, 5.5959e-11,  ..., 7.2840e-14,\n",
              "           5.1467e-16, 8.3238e-12],\n",
              "          [7.0755e-08, 6.3713e-10, 4.7529e-12,  ..., 1.3128e-07,\n",
              "           1.2441e-11, 3.5710e-10]],\n",
              "\n",
              "         [[3.6813e-10, 3.6171e-09, 1.9775e-10,  ..., 1.3991e-06,\n",
              "           3.0149e-11, 3.2454e-08],\n",
              "          [3.4132e-10, 8.7690e-12, 8.1221e-13,  ..., 8.6466e-10,\n",
              "           1.3357e-10, 6.6395e-09],\n",
              "          [7.3939e-07, 1.6622e-14, 2.3463e-13,  ..., 1.8885e-08,\n",
              "           9.5713e-13, 9.9604e-13],\n",
              "          ...,\n",
              "          [1.3528e-04, 5.2899e-10, 1.1620e-08,  ..., 2.7353e-10,\n",
              "           5.0663e-12, 2.1386e-10],\n",
              "          [7.2140e-03, 1.0623e-12, 3.0239e-10,  ..., 9.7700e-08,\n",
              "           1.4488e-13, 6.4990e-09],\n",
              "          [2.2313e-04, 3.8515e-08, 1.4230e-10,  ..., 3.0999e-05,\n",
              "           1.1335e-06, 4.1939e-07]],\n",
              "\n",
              "         [[6.5973e-09, 8.5734e-12, 1.3682e-12,  ..., 2.0133e-09,\n",
              "           2.3003e-15, 3.3466e-05],\n",
              "          [5.9370e-16, 2.5651e-20, 7.9962e-14,  ..., 4.4040e-14,\n",
              "           3.8289e-18, 8.6487e-11],\n",
              "          [3.5229e-12, 5.7107e-14, 1.7663e-08,  ..., 3.5699e-09,\n",
              "           1.4791e-13, 4.1597e-12],\n",
              "          ...,\n",
              "          [6.5724e-07, 1.2639e-10, 1.1975e-06,  ..., 3.2970e-09,\n",
              "           7.9972e-16, 3.0931e-10],\n",
              "          [4.0271e-11, 1.9402e-14, 6.6202e-14,  ..., 4.1619e-12,\n",
              "           4.6823e-13, 6.8540e-10],\n",
              "          [1.7064e-09, 1.5469e-13, 4.2820e-10,  ..., 5.9217e-09,\n",
              "           6.9004e-11, 9.3143e-11]]],\n",
              "\n",
              "\n",
              "        [[[5.0763e-05, 6.8261e-03, 1.8030e-02,  ..., 2.9439e-04,\n",
              "           1.0413e-03, 1.6112e-03],\n",
              "          [8.9140e-05, 8.6832e-05, 7.0093e-05,  ..., 7.6013e-04,\n",
              "           1.0254e-06, 7.7822e-04],\n",
              "          [5.6237e-04, 4.3803e-03, 2.9358e-02,  ..., 2.4675e-04,\n",
              "           2.4301e-04, 5.9227e-05],\n",
              "          ...,\n",
              "          [3.9462e-04, 3.6674e-04, 1.9550e-04,  ..., 4.5891e-04,\n",
              "           3.7892e-04, 1.5705e-02],\n",
              "          [6.0113e-04, 1.1172e-04, 2.6438e-03,  ..., 1.1637e-04,\n",
              "           1.9388e-03, 4.6865e-03],\n",
              "          [1.6930e-04, 3.2984e-04, 1.8017e-04,  ..., 1.1113e-07,\n",
              "           1.7702e-04, 3.6424e-04]],\n",
              "\n",
              "         [[3.0475e-05, 1.0811e-04, 7.4110e-05,  ..., 2.7079e-05,\n",
              "           1.1078e-05, 4.4264e-05],\n",
              "          [7.0793e-05, 4.0497e-05, 1.1559e-05,  ..., 1.5273e-05,\n",
              "           9.2892e-08, 9.3809e-06],\n",
              "          [7.1753e-05, 3.7410e-05, 3.1756e-05,  ..., 7.7131e-06,\n",
              "           1.0060e-05, 1.8389e-05],\n",
              "          ...,\n",
              "          [1.6861e-04, 6.8817e-05, 1.7346e-03,  ..., 3.3304e-05,\n",
              "           1.0027e-05, 4.4295e-04],\n",
              "          [1.0515e-04, 3.9733e-05, 1.4913e-04,  ..., 3.7245e-04,\n",
              "           7.5147e-05, 3.0979e-04],\n",
              "          [9.9247e-05, 1.0839e-04, 1.2358e-04,  ..., 1.7002e-07,\n",
              "           9.8191e-05, 1.2611e-03]],\n",
              "\n",
              "         [[7.0351e-06, 5.0211e-04, 1.2701e-04,  ..., 5.5348e-05,\n",
              "           7.6469e-06, 3.1822e-04],\n",
              "          [5.4150e-05, 3.5178e-05, 8.2491e-06,  ..., 2.0193e-04,\n",
              "           1.9511e-08, 5.5718e-04],\n",
              "          [3.0747e-05, 3.6491e-05, 1.9453e-03,  ..., 2.2043e-04,\n",
              "           2.5808e-04, 1.2154e-04],\n",
              "          ...,\n",
              "          [1.2806e-04, 2.6723e-04, 2.1097e-04,  ..., 1.1997e-04,\n",
              "           2.4240e-04, 4.6870e-04],\n",
              "          [3.8626e-05, 3.6669e-04, 8.7127e-04,  ..., 1.2664e-04,\n",
              "           8.5192e-04, 7.0105e-04],\n",
              "          [1.9342e-05, 8.2994e-05, 6.8990e-05,  ..., 3.7878e-08,\n",
              "           8.3045e-05, 4.5587e-05]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[5.8320e-14, 1.1819e-19, 1.0273e-12,  ..., 2.5179e-16,\n",
              "           7.8612e-17, 1.1227e-12],\n",
              "          [1.5424e-15, 3.1376e-17, 4.2967e-12,  ..., 2.7161e-15,\n",
              "           2.9758e-16, 1.8990e-10],\n",
              "          [3.8110e-18, 1.0202e-18, 5.0265e-14,  ..., 9.1997e-21,\n",
              "           9.2493e-17, 4.8342e-11],\n",
              "          ...,\n",
              "          [1.4890e-16, 1.9211e-21, 8.8706e-12,  ..., 6.9932e-14,\n",
              "           6.2579e-15, 8.5998e-11],\n",
              "          [1.0896e-12, 3.8385e-19, 7.4331e-10,  ..., 1.0918e-14,\n",
              "           4.8118e-13, 1.0168e-12],\n",
              "          [7.5986e-12, 1.3894e-11, 5.5860e-12,  ..., 1.9148e-09,\n",
              "           6.8001e-11, 1.1173e-10]],\n",
              "\n",
              "         [[4.1200e-04, 1.0878e-10, 1.2044e-08,  ..., 3.8507e-07,\n",
              "           1.0068e-11, 1.0912e-08],\n",
              "          [1.6320e-07, 1.1849e-14, 3.4331e-12,  ..., 6.0349e-11,\n",
              "           5.3617e-07, 5.5293e-08],\n",
              "          [5.1368e-06, 4.4439e-11, 9.8948e-13,  ..., 2.2394e-15,\n",
              "           1.3160e-09, 1.7314e-09],\n",
              "          ...,\n",
              "          [9.0130e-07, 6.8959e-10, 4.1248e-08,  ..., 4.1216e-08,\n",
              "           1.2501e-08, 1.0151e-05],\n",
              "          [4.4544e-06, 3.0090e-15, 3.0943e-14,  ..., 4.8292e-08,\n",
              "           1.9153e-09, 9.1047e-11],\n",
              "          [4.7259e-06, 1.8971e-06, 5.0552e-12,  ..., 1.9261e-05,\n",
              "           3.3767e-10, 1.2027e-07]],\n",
              "\n",
              "         [[6.4167e-08, 1.4791e-14, 3.2695e-13,  ..., 6.1625e-10,\n",
              "           2.2880e-14, 8.5788e-09],\n",
              "          [4.0199e-16, 2.7413e-21, 3.4644e-15,  ..., 1.7336e-11,\n",
              "           5.2727e-24, 7.7915e-11],\n",
              "          [3.5422e-12, 3.2748e-11, 9.3104e-09,  ..., 1.1766e-10,\n",
              "           5.3023e-14, 4.1142e-08],\n",
              "          ...,\n",
              "          [8.5288e-18, 1.5319e-12, 1.1984e-11,  ..., 6.9955e-10,\n",
              "           9.0215e-12, 1.5749e-10],\n",
              "          [6.9144e-16, 1.0569e-21, 1.8341e-15,  ..., 5.2081e-14,\n",
              "           3.0517e-12, 7.6078e-13],\n",
              "          [1.9941e-14, 9.2611e-15, 1.8114e-10,  ..., 6.3504e-09,\n",
              "           1.7272e-10, 3.2646e-08]]],\n",
              "\n",
              "\n",
              "        [[[2.1881e-04, 1.4735e-02, 2.0270e-03,  ..., 6.8003e-03,\n",
              "           1.9219e-04, 3.3956e-07],\n",
              "          [1.1476e-04, 3.5963e-04, 2.6317e-04,  ..., 3.7823e-04,\n",
              "           1.9802e-05, 2.2118e-04],\n",
              "          [5.4982e-04, 1.9620e-04, 5.8744e-03,  ..., 1.3113e-03,\n",
              "           8.2314e-05, 7.2723e-04],\n",
              "          ...,\n",
              "          [3.7875e-04, 9.4316e-05, 4.5312e-04,  ..., 9.1179e-05,\n",
              "           8.9782e-04, 3.3538e-04],\n",
              "          [1.4331e-04, 2.9797e-03, 4.0724e-03,  ..., 1.8419e-04,\n",
              "           2.3062e-04, 1.3850e-04],\n",
              "          [2.0938e-04, 1.9297e-04, 1.5764e-04,  ..., 1.6664e-04,\n",
              "           1.6956e-04, 1.1890e-03]],\n",
              "\n",
              "         [[5.5920e-05, 1.1434e-04, 2.5692e-04,  ..., 9.7264e-05,\n",
              "           6.0226e-06, 2.8456e-07],\n",
              "          [6.9817e-05, 2.4342e-05, 1.1052e-05,  ..., 6.4490e-05,\n",
              "           5.9105e-06, 5.7633e-06],\n",
              "          [3.0740e-04, 4.8872e-07, 4.9496e-05,  ..., 5.1313e-05,\n",
              "           1.5517e-05, 7.7900e-04],\n",
              "          ...,\n",
              "          [1.2375e-04, 1.8799e-05, 4.5669e-05,  ..., 4.2318e-05,\n",
              "           1.8875e-05, 3.7111e-03],\n",
              "          [2.7329e-05, 4.0934e-05, 6.6248e-05,  ..., 8.9753e-05,\n",
              "           6.9004e-05, 1.0157e-02],\n",
              "          [1.3723e-04, 6.1109e-05, 1.7628e-04,  ..., 2.5182e-04,\n",
              "           1.3766e-04, 1.3699e-03]],\n",
              "\n",
              "         [[5.2228e-05, 4.3825e-04, 1.0175e-04,  ..., 1.1539e-04,\n",
              "           7.0707e-04, 2.3873e-08],\n",
              "          [6.5873e-05, 4.0111e-04, 4.0061e-05,  ..., 6.2968e-05,\n",
              "           1.5857e-04, 1.3382e-05],\n",
              "          [6.2338e-05, 2.8647e-06, 7.8200e-04,  ..., 1.0969e-03,\n",
              "           5.0749e-04, 8.4516e-05],\n",
              "          ...,\n",
              "          [7.3266e-05, 1.1563e-03, 7.9665e-04,  ..., 1.2734e-04,\n",
              "           7.0435e-04, 7.5161e-04],\n",
              "          [1.1689e-04, 4.0481e-04, 5.2576e-04,  ..., 4.9498e-04,\n",
              "           1.7671e-04, 8.3218e-05],\n",
              "          [1.5899e-04, 1.6640e-04, 6.6778e-04,  ..., 1.4507e-04,\n",
              "           1.0218e-04, 2.7587e-04]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[5.0239e-11, 2.2739e-13, 2.7406e-12,  ..., 2.2950e-16,\n",
              "           2.6810e-18, 8.3230e-10],\n",
              "          [3.9487e-14, 7.2667e-14, 6.2569e-13,  ..., 6.1593e-12,\n",
              "           8.4963e-17, 8.9499e-15],\n",
              "          [2.0159e-14, 4.2659e-16, 6.8451e-17,  ..., 5.1083e-15,\n",
              "           2.7703e-17, 4.0540e-14],\n",
              "          ...,\n",
              "          [1.1493e-15, 1.8286e-19, 1.7380e-13,  ..., 2.8678e-13,\n",
              "           2.5242e-15, 6.8038e-11],\n",
              "          [1.0892e-15, 1.5766e-18, 2.8671e-10,  ..., 5.5523e-13,\n",
              "           6.8760e-13, 5.2319e-10],\n",
              "          [1.9016e-13, 1.6874e-09, 1.2917e-12,  ..., 3.8514e-11,\n",
              "           3.4913e-10, 4.6428e-11]],\n",
              "\n",
              "         [[1.8381e-06, 5.0151e-09, 1.5603e-07,  ..., 6.9217e-12,\n",
              "           6.7982e-11, 6.1605e-06],\n",
              "          [2.7215e-07, 1.4210e-07, 8.2165e-11,  ..., 1.5564e-10,\n",
              "           1.7366e-14, 3.7392e-12],\n",
              "          [1.2520e-08, 2.7688e-06, 7.5662e-10,  ..., 6.7833e-11,\n",
              "           3.3233e-12, 5.7430e-09],\n",
              "          ...,\n",
              "          [1.2274e-04, 1.6100e-12, 3.8100e-10,  ..., 3.5079e-06,\n",
              "           2.0100e-08, 1.0296e-05],\n",
              "          [3.4277e-05, 2.1607e-11, 7.3366e-12,  ..., 1.3952e-07,\n",
              "           1.6385e-11, 2.5144e-07],\n",
              "          [2.5553e-09, 5.7818e-06, 1.1398e-11,  ..., 9.5526e-09,\n",
              "           1.2114e-07, 4.8142e-07]],\n",
              "\n",
              "         [[5.6955e-08, 5.3445e-12, 1.3584e-12,  ..., 4.1395e-12,\n",
              "           6.5965e-17, 1.1424e-08],\n",
              "          [8.7263e-14, 2.1598e-17, 6.5561e-14,  ..., 5.5578e-17,\n",
              "           4.5192e-20, 1.1158e-14],\n",
              "          [1.3958e-10, 1.6601e-09, 3.6839e-09,  ..., 6.5282e-12,\n",
              "           1.9716e-08, 3.1075e-13],\n",
              "          ...,\n",
              "          [1.5940e-13, 1.1668e-13, 6.2990e-14,  ..., 3.7711e-10,\n",
              "           1.7629e-11, 2.0017e-10],\n",
              "          [5.4109e-11, 3.8384e-18, 8.1112e-17,  ..., 2.9225e-14,\n",
              "           3.5111e-13, 1.3866e-12],\n",
              "          [3.3485e-14, 1.4059e-10, 4.9345e-11,  ..., 1.4614e-09,\n",
              "           1.3291e-10, 9.6273e-12]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[1.2467e-04, 1.3363e-04, 1.1448e-04,  ..., 1.1550e-03,\n",
              "           3.8345e-05, 1.6573e-11],\n",
              "          [5.1688e-03, 9.7686e-04, 2.6455e-03,  ..., 3.8489e-04,\n",
              "           1.1346e-03, 9.1254e-05],\n",
              "          [6.1667e-03, 2.7915e-03, 7.3220e-04,  ..., 3.8644e-04,\n",
              "           3.4877e-04, 1.3414e-06],\n",
              "          ...,\n",
              "          [3.0353e-04, 1.7922e-04, 1.3672e-03,  ..., 1.1307e-03,\n",
              "           4.3126e-04, 9.9486e-04],\n",
              "          [1.6074e-04, 3.3273e-04, 2.6811e-03,  ..., 8.2272e-04,\n",
              "           1.4727e-04, 7.8436e-05],\n",
              "          [7.1019e-08, 1.0339e-04, 7.5638e-03,  ..., 5.3718e-06,\n",
              "           3.5667e-05, 1.5003e-06]],\n",
              "\n",
              "         [[2.6743e-07, 2.1208e-05, 9.3370e-05,  ..., 2.7491e-05,\n",
              "           3.2577e-06, 2.6670e-12],\n",
              "          [3.7921e-05, 1.3792e-04, 4.5723e-04,  ..., 7.9104e-05,\n",
              "           3.2479e-05, 8.6252e-06],\n",
              "          [5.0064e-05, 3.1267e-04, 7.6715e-04,  ..., 6.5509e-04,\n",
              "           3.1307e-04, 4.5717e-07],\n",
              "          ...,\n",
              "          [7.1520e-05, 1.1302e-04, 5.2917e-04,  ..., 6.0398e-04,\n",
              "           2.7273e-04, 4.3442e-05],\n",
              "          [8.8259e-06, 8.4115e-05, 2.7830e-04,  ..., 9.0124e-03,\n",
              "           1.6429e-04, 2.9205e-05],\n",
              "          [3.3490e-08, 1.4800e-05, 1.6390e-04,  ..., 8.6051e-06,\n",
              "           5.6162e-04, 2.8651e-05]],\n",
              "\n",
              "         [[1.9433e-05, 4.7841e-05, 7.6669e-05,  ..., 5.8941e-05,\n",
              "           4.5749e-06, 5.2128e-08],\n",
              "          [1.3436e-04, 3.5038e-04, 1.2900e-03,  ..., 4.3955e-05,\n",
              "           1.8609e-05, 8.3570e-05],\n",
              "          [3.1454e-05, 1.4424e-04, 5.7283e-04,  ..., 2.3818e-04,\n",
              "           4.6819e-04, 1.0866e-06],\n",
              "          ...,\n",
              "          [2.8150e-04, 1.7743e-04, 4.6362e-04,  ..., 2.3898e-04,\n",
              "           2.1423e-04, 1.0724e-05],\n",
              "          [1.0785e-05, 1.1528e-04, 3.1289e-04,  ..., 3.3446e-04,\n",
              "           3.9055e-04, 3.1491e-05],\n",
              "          [4.2627e-07, 3.5009e-06, 1.9884e-05,  ..., 2.1758e-06,\n",
              "           2.1446e-04, 9.0295e-07]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 3.2319e-32, 6.9713e-31,  ..., 4.4972e-31,\n",
              "           4.2122e-37, 0.0000e+00],\n",
              "          [0.0000e+00, 3.5945e-27, 7.9216e-23,  ..., 2.6697e-34,\n",
              "           2.5311e-11, 0.0000e+00],\n",
              "          [1.3951e-39, 3.9779e-22, 6.5264e-14,  ..., 1.0142e-25,\n",
              "           4.4885e-16, 0.0000e+00],\n",
              "          ...,\n",
              "          [5.3417e-20, 1.3775e-30, 6.1466e-24,  ..., 3.5645e-23,\n",
              "           4.1506e-27, 3.2066e-32],\n",
              "          [1.4013e-45, 8.4789e-32, 2.9086e-19,  ..., 1.5485e-33,\n",
              "           2.1275e-36, 2.8389e-35],\n",
              "          [0.0000e+00, 4.8717e-31, 2.9122e-22,  ..., 0.0000e+00,\n",
              "           2.6290e-32, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 2.0248e-32, 3.5247e-29,  ..., 5.5891e-34,\n",
              "           1.3718e-38, 0.0000e+00],\n",
              "          [1.4013e-45, 8.4742e-15, 7.5257e-25,  ..., 2.1116e-19,\n",
              "           1.7329e-28, 6.0672e-41],\n",
              "          [1.0401e-37, 3.2704e-13, 7.1804e-18,  ..., 8.3893e-17,\n",
              "           3.4364e-15, 0.0000e+00],\n",
              "          ...,\n",
              "          [9.2957e-33, 3.3285e-18, 3.7225e-21,  ..., 6.6716e-23,\n",
              "           3.7621e-22, 2.8116e-31],\n",
              "          [1.6675e-43, 1.0011e-17, 4.0348e-25,  ..., 1.4783e-29,\n",
              "           1.1350e-37, 2.6079e-29],\n",
              "          [0.0000e+00, 2.6856e-22, 4.3722e-31,  ..., 0.0000e+00,\n",
              "           1.0647e-28, 0.0000e+00]],\n",
              "\n",
              "         [[2.8124e-26, 1.2491e-20, 1.7336e-25,  ..., 1.4675e-28,\n",
              "           2.1074e-20, 0.0000e+00],\n",
              "          [5.5285e-40, 1.0057e-28, 7.3584e-17,  ..., 5.6940e-32,\n",
              "           1.2119e-28, 0.0000e+00],\n",
              "          [1.2698e-35, 2.0299e-15, 5.2846e-20,  ..., 9.3432e-21,\n",
              "           3.7183e-14, 0.0000e+00],\n",
              "          ...,\n",
              "          [1.6811e-36, 9.9783e-32, 6.5761e-20,  ..., 1.4051e-23,\n",
              "           5.7954e-23, 3.8295e-27],\n",
              "          [3.9050e-37, 9.7561e-24, 1.0165e-15,  ..., 1.8087e-24,\n",
              "           4.2011e-34, 3.5562e-38],\n",
              "          [0.0000e+00, 6.8961e-39, 1.1185e-28,  ..., 6.0555e-35,\n",
              "           1.3517e-30, 8.8452e-31]]],\n",
              "\n",
              "\n",
              "        [[[1.4056e-08, 2.1381e-04, 4.5173e-04,  ..., 2.1845e-05,\n",
              "           1.9114e-04, 3.8676e-06],\n",
              "          [1.2101e-03, 6.7523e-03, 1.1830e-03,  ..., 8.8193e-04,\n",
              "           7.8522e-04, 7.0231e-09],\n",
              "          [5.9769e-05, 1.2158e-03, 6.3060e-04,  ..., 2.0531e-03,\n",
              "           2.4014e-04, 7.2166e-03],\n",
              "          ...,\n",
              "          [1.1132e-02, 1.3624e-04, 8.9552e-04,  ..., 1.7269e-04,\n",
              "           3.5943e-04, 3.4578e-04],\n",
              "          [1.1917e-02, 3.0035e-04, 8.3885e-04,  ..., 1.4630e-04,\n",
              "           3.0467e-04, 7.1629e-05],\n",
              "          [2.3914e-04, 2.3482e-04, 1.6565e-04,  ..., 1.5784e-04,\n",
              "           2.0225e-05, 3.1464e-06]],\n",
              "\n",
              "         [[1.7781e-12, 2.0059e-06, 1.3391e-05,  ..., 3.1157e-05,\n",
              "           1.1857e-03, 1.5971e-06],\n",
              "          [7.8576e-06, 6.5308e-05, 7.4778e-05,  ..., 3.4655e-04,\n",
              "           1.2151e-04, 2.8516e-09],\n",
              "          [1.8910e-06, 2.0892e-04, 6.8935e-04,  ..., 5.4612e-04,\n",
              "           2.3904e-04, 1.8380e-04],\n",
              "          ...,\n",
              "          [3.3093e-05, 2.5204e-04, 6.0336e-04,  ..., 4.1242e-04,\n",
              "           3.6561e-04, 1.6991e-04],\n",
              "          [1.1278e-05, 6.5533e-05, 3.2646e-04,  ..., 1.8274e-04,\n",
              "           2.0370e-04, 8.0288e-07],\n",
              "          [1.0045e-07, 5.1113e-05, 3.6297e-04,  ..., 4.8319e-05,\n",
              "           8.3909e-04, 1.8810e-04]],\n",
              "\n",
              "         [[2.2651e-11, 2.1825e-06, 1.5042e-05,  ..., 6.5840e-04,\n",
              "           4.8227e-05, 4.7382e-05],\n",
              "          [4.1205e-05, 3.3029e-04, 2.6742e-04,  ..., 9.3747e-04,\n",
              "           1.9675e-04, 3.9272e-08],\n",
              "          [6.0877e-07, 2.1546e-04, 7.4297e-04,  ..., 4.2383e-04,\n",
              "           1.3859e-04, 5.0243e-04],\n",
              "          ...,\n",
              "          [9.0421e-06, 4.6342e-04, 7.8486e-04,  ..., 1.9823e-04,\n",
              "           2.7170e-04, 1.2477e-03],\n",
              "          [2.7719e-05, 6.8287e-05, 5.3435e-04,  ..., 1.7688e-04,\n",
              "           3.1447e-04, 5.4679e-07],\n",
              "          [3.9852e-06, 3.2477e-06, 1.5345e-04,  ..., 1.7814e-05,\n",
              "           3.9752e-04, 2.0321e-06]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 2.7884e-31,  ..., 9.5614e-35,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 4.8262e-33, 6.2319e-31,  ..., 1.7784e-23,\n",
              "           7.4389e-31, 0.0000e+00],\n",
              "          [1.4013e-45, 3.8467e-31, 1.1195e-18,  ..., 2.4161e-14,\n",
              "           9.0106e-18, 5.2482e-28],\n",
              "          ...,\n",
              "          [9.1391e-28, 1.7048e-30, 1.2628e-22,  ..., 4.2605e-19,\n",
              "           5.0050e-25, 4.9371e-26],\n",
              "          [1.2094e-36, 1.1245e-32, 1.2274e-29,  ..., 1.2585e-33,\n",
              "           4.7768e-26, 0.0000e+00],\n",
              "          [0.0000e+00, 1.0995e-35, 1.3261e-34,  ..., 3.6516e-39,\n",
              "           3.3833e-32, 4.4842e-44]],\n",
              "\n",
              "         [[0.0000e+00, 7.0065e-45, 1.5090e-30,  ..., 2.3966e-33,\n",
              "           0.0000e+00, 2.3542e-43],\n",
              "          [1.4013e-45, 3.3533e-21, 1.6531e-34,  ..., 2.4191e-25,\n",
              "           3.9527e-27, 0.0000e+00],\n",
              "          [0.0000e+00, 4.1746e-24, 2.7839e-21,  ..., 3.7272e-18,\n",
              "           4.7708e-19, 6.4229e-21],\n",
              "          ...,\n",
              "          [1.7982e-29, 1.5823e-25, 7.6078e-18,  ..., 9.7893e-27,\n",
              "           2.3315e-10, 1.3670e-20],\n",
              "          [1.3391e-39, 3.1137e-21, 3.9088e-32,  ..., 8.0982e-33,\n",
              "           1.7481e-12, 0.0000e+00],\n",
              "          [0.0000e+00, 7.9560e-25, 1.0499e-36,  ..., 2.6699e-25,\n",
              "           5.1013e-25, 4.4315e-30]],\n",
              "\n",
              "         [[0.0000e+00, 8.8422e-43, 3.7132e-31,  ..., 3.1390e-30,\n",
              "           1.4013e-45, 1.6571e-25],\n",
              "          [0.0000e+00, 1.6791e-30, 1.0987e-35,  ..., 3.0133e-23,\n",
              "           1.2401e-19, 0.0000e+00],\n",
              "          [0.0000e+00, 3.7600e-27, 1.3530e-24,  ..., 5.7350e-10,\n",
              "           2.5234e-05, 2.2254e-30],\n",
              "          ...,\n",
              "          [4.9825e-37, 5.8240e-19, 3.8584e-17,  ..., 9.6534e-28,\n",
              "           1.5140e-24, 1.0439e-18],\n",
              "          [1.4684e-32, 2.0248e-25, 9.9518e-25,  ..., 4.5146e-31,\n",
              "           6.5380e-12, 0.0000e+00],\n",
              "          [0.0000e+00, 1.0157e-35, 2.9865e-28,  ..., 6.9517e-35,\n",
              "           2.8940e-26, 2.0877e-32]]],\n",
              "\n",
              "\n",
              "        [[[3.6282e-05, 2.4565e-05, 1.0236e-03,  ..., 3.4592e-04,\n",
              "           6.4833e-12, 9.5969e-07],\n",
              "          [6.4651e-04, 2.3138e-04, 3.9134e-03,  ..., 4.9461e-04,\n",
              "           1.4671e-05, 1.7343e-05],\n",
              "          [8.5000e-05, 1.4155e-04, 7.4628e-04,  ..., 1.5087e-03,\n",
              "           3.4208e-05, 3.6838e-05],\n",
              "          ...,\n",
              "          [1.4464e-03, 6.6385e-04, 1.4265e-03,  ..., 3.5708e-03,\n",
              "           3.4779e-04, 3.1565e-03],\n",
              "          [4.7753e-03, 3.1553e-04, 7.2135e-04,  ..., 2.1052e-03,\n",
              "           1.2846e-04, 2.7905e-04],\n",
              "          [2.1353e-05, 1.5629e-03, 7.6835e-04,  ..., 1.0367e-04,\n",
              "           2.1019e-04, 3.4034e-05]],\n",
              "\n",
              "         [[2.8493e-08, 1.1145e-06, 4.1788e-05,  ..., 2.3233e-05,\n",
              "           7.1530e-12, 6.5819e-08],\n",
              "          [2.0841e-06, 7.8831e-05, 1.9841e-04,  ..., 2.1827e-04,\n",
              "           3.4798e-05, 5.9495e-06],\n",
              "          [4.1310e-06, 2.1713e-04, 1.1463e-03,  ..., 1.0247e-03,\n",
              "           1.2036e-04, 1.0719e-05],\n",
              "          ...,\n",
              "          [5.7643e-05, 4.2199e-04, 7.6810e-04,  ..., 1.3942e-03,\n",
              "           3.3612e-04, 1.9965e-04],\n",
              "          [2.5821e-05, 1.1140e-04, 8.0378e-03,  ..., 8.1381e-04,\n",
              "           2.8236e-04, 4.1694e-05],\n",
              "          [2.3933e-06, 1.0855e-05, 6.3879e-05,  ..., 6.9545e-04,\n",
              "           1.2439e-04, 1.2414e-05]],\n",
              "\n",
              "         [[2.1764e-05, 5.9279e-07, 1.0272e-04,  ..., 5.1049e-05,\n",
              "           3.5171e-11, 2.0218e-05],\n",
              "          [5.7446e-05, 4.6943e-04, 9.8402e-04,  ..., 1.2442e-03,\n",
              "           6.5949e-05, 7.5899e-05],\n",
              "          [2.1878e-06, 1.5936e-04, 6.1352e-04,  ..., 5.9830e-04,\n",
              "           3.0596e-05, 1.7370e-05],\n",
              "          ...,\n",
              "          [5.2523e-05, 3.6403e-04, 3.8060e-04,  ..., 4.6002e-04,\n",
              "           3.1953e-04, 6.3442e-04],\n",
              "          [1.3285e-04, 4.0512e-04, 2.8635e-04,  ..., 1.4836e-03,\n",
              "           5.2769e-04, 5.9744e-06],\n",
              "          [1.3082e-05, 4.0120e-06, 3.6421e-05,  ..., 4.7035e-05,\n",
              "           3.1177e-05, 3.2820e-07]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 1.4296e-20,  ..., 1.1633e-32,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 2.7171e-32, 1.2619e-21,  ..., 3.5366e-18,\n",
              "           6.5673e-37, 1.5487e-35],\n",
              "          [1.8805e-42, 2.0787e-23, 1.2367e-08,  ..., 1.6238e-21,\n",
              "           1.8738e-39, 0.0000e+00],\n",
              "          ...,\n",
              "          [1.3051e-24, 1.1787e-21, 4.9670e-14,  ..., 7.3459e-13,\n",
              "           5.9630e-27, 1.1385e-26],\n",
              "          [1.5138e-40, 6.7100e-34, 9.4966e-34,  ..., 1.6625e-24,\n",
              "           1.1870e-26, 1.6180e-28],\n",
              "          [0.0000e+00, 0.0000e+00, 4.8601e-27,  ..., 2.3234e-32,\n",
              "           2.8607e-31, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 0.0000e+00, 1.4803e-25,  ..., 1.9361e-34,\n",
              "           0.0000e+00, 2.1019e-44],\n",
              "          [0.0000e+00, 1.1103e-25, 8.5060e-31,  ..., 1.3527e-13,\n",
              "           5.0765e-34, 8.5758e-10],\n",
              "          [2.8026e-45, 2.2238e-24, 8.1100e-20,  ..., 3.5206e-10,\n",
              "           1.1088e-28, 2.3000e-38],\n",
              "          ...,\n",
              "          [4.4902e-37, 2.2062e-18, 1.2830e-18,  ..., 8.1761e-17,\n",
              "           1.6015e-29, 6.4541e-22],\n",
              "          [1.0432e-33, 6.8231e-31, 1.0280e-21,  ..., 6.2120e-24,\n",
              "           1.6784e-23, 3.6336e-36],\n",
              "          [3.2230e-44, 3.1809e-43, 1.5421e-37,  ..., 5.8253e-26,\n",
              "           1.1731e-27, 0.0000e+00]],\n",
              "\n",
              "         [[2.9427e-44, 4.4842e-44, 4.9051e-26,  ..., 8.5954e-17,\n",
              "           0.0000e+00, 5.0307e-43],\n",
              "          [1.1491e-43, 2.7775e-34, 3.7149e-30,  ..., 3.5832e-08,\n",
              "           6.2978e-32, 4.6539e-35],\n",
              "          [0.0000e+00, 2.0385e-29, 3.0157e-18,  ..., 4.7658e-18,\n",
              "           5.8020e-35, 3.4612e-43],\n",
              "          ...,\n",
              "          [6.9680e-35, 3.8294e-20, 2.8277e-25,  ..., 5.8636e-21,\n",
              "           3.3616e-24, 3.5124e-07],\n",
              "          [1.5953e-38, 1.6426e-29, 5.6937e-27,  ..., 5.0225e-24,\n",
              "           2.7598e-16, 1.4391e-42],\n",
              "          [0.0000e+00, 3.1668e-39, 3.3782e-37,  ..., 4.2622e-21,\n",
              "           1.2555e-27, 0.0000e+00]]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "s=FeatureExtractor()\n",
        "# s(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rHJmWmK0HVnV"
      },
      "outputs": [],
      "source": [
        "# for param in model.parameters():\n",
        "#   param[1].requres_grad = True\n",
        "#   blocks.append(param[1])\n",
        "\n",
        "# model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "# print(model.features)\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "# training(backbone(images))\n",
        "\n",
        "\n",
        "\n",
        "class FullyConnectedNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, neurons):\n",
        "        super(FullyConnectedNet, self).__init__()\n",
        "        previous = input_size\n",
        "        for i in range(len(neurons)):\n",
        "            self.blocks.append(Conv2dBlock(previous, neurons[i]))\n",
        "            previous = neurons[i]\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"Pass the input through each block\"\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "\n",
        "previous = input_size\n",
        "for i in range(len(neurons)):\n",
        "    blocks.append(LinearBlock(previous, neurons[i]))\n",
        "    previous = neurons[i]\n",
        "\n",
        "def forward(self, x):\n",
        "  for block in blocks:\n",
        "      x = block(x)\n",
        "  return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  block.append(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out"
      ],
      "metadata": {
        "id": "4Hlmd4HsQ9s4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "for param in model.parameters():\n",
        "  param[1].requres_grad = True\n",
        "\n",
        "model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "\n",
        "for i in model.children():\n",
        "  block = torch.nn.ModuleList(i)\n",
        "  break\n",
        "\n",
        "return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "feature_maps = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "print(feature_maps[0]['out_conv4_2'])\n",
        "\n",
        "conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "conv7_2 = feature_maps[0]['out_conv7_2'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "mthSkrRBi8LF",
        "outputId": "157ad082-b98b-48b9-f91f-fbf069f80f08"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-2db57a6192bd>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreturn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'22'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'29'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv5_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'34'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv7_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'36'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv6_2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfeature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntermediateLayerGetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mconv4_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'IntermediateLayerGetter' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "for param in model.parameters():\n",
        "  param[1].requres_grad = True\n",
        "\n",
        "model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "\n",
        "for i in model.children():\n",
        "  block = torch.nn.ModuleList(i)\n",
        "  break\n",
        "\n",
        "return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "feature_maps = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "print(feature_maps[0]['out_conv4_2'])\n",
        "conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(feature_maps[0]['out_conv4_2'])\n",
        "conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "block.append(conv4_2)\n",
        "\n",
        "conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(feature_maps[0]['out_conv4_3'])\n",
        "conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "block.append(conv4_3)\n",
        "\n",
        "conv5_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv5_3'][0])\n",
        "conv5_3=torch.nn.ReLU()(conv5_3)\n",
        "block.append(conv5_3)\n",
        "\n",
        "conv6_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv6_2'][0])\n",
        "conv6_2=torch.nn.ReLU()(conv6_2)\n",
        "block.append(conv6_2)\n",
        "\n",
        "\n",
        "conv7_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1)(feature_maps[0]['out_conv7_2'][0])\n",
        "conv7_2=torch.nn.ReLU()(conv7_2)\n",
        "block.append(conv7_2)\n",
        "\n",
        "block, rfem11_temp = rfem(block, conv4_2)\n",
        "block, rfem12_temp = rfem(block, rfem11_temp)\n",
        "block, rfem13_temp = rfem(block, rfem12_temp)\n",
        "block, rfem14_temp = rfem(block, rfem13_temp)\n",
        "\n",
        "rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "\n",
        "block, ftb_4 = ftb(block, rfem14)\n",
        "block, ftb_3 = ftb(block, rfem13)\n",
        "block, ftb_2 = ftb(block, rfem12)\n",
        "block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "block, rfem21_temp = rfem(block, ftb_1)\n",
        "block, rfem22_temp = rfem(block, ftb_2)\n",
        "block, rfem23_temp = rfem(block, ftb_3)\n",
        "block, rfem24_temp = rfem(block, ftb_4)\n",
        "\n",
        "rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "block.append(det_1)\n",
        "det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "block.append(det_2)\n",
        "det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "block.append(det_3)\n",
        "det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "block.append(det_4)\n",
        "\n",
        "D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "block.append(out)\n"
      ],
      "metadata": {
        "id": "grWFJwj6m7bu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "66309aa9-2aa2-4b6e-f825-86795d6793c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c7ea00d6f51f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mreturn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'20'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'22'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv4_3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'29'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv5_3'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'34'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv7_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'36'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'out_conv6_2'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfeature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntermediateLayerGetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv4_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mconv4_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'IntermediateLayerGetter' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training(feature_maps):\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "  rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "  rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "  rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "  ftb_4 = ftb(rfem14)\n",
        "  ftb_3 = ftb(rfem13)\n",
        "  ftb_2 = ftb(rfem12)\n",
        "  ftb_1 = ftb(rfem11)\n",
        "\n",
        "  rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "  rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "  rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "  rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "  det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "  det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "  det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "  det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "  D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "  pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "  torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "  return D\n"
      ],
      "metadata": {
        "id": "bz3otbUmMtnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVAuX27tHUPX"
      },
      "source": [
        "#dadada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6Zvz3aj_L9",
        "outputId": "001dd363-1ce2-474a-ddcd-4f73833ac027"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "def rfem(x):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "  return torch.add(combined_x, x/0.5)\n",
        "\n",
        "def ftb(current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    return torch.add(x, d)\n",
        "\n",
        "\n",
        "def training(feature_maps):\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "  rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "  rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "  rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "  ftb_4 = ftb(rfem14)\n",
        "  ftb_3 = ftb(rfem13)\n",
        "  ftb_2 = ftb(rfem12)\n",
        "  ftb_1 = ftb(rfem11)\n",
        "\n",
        "  rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "  rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "  rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "  rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "  det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "  det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "  det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "  det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "  D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "  pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "  torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "  return D\n",
        "\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    for param in model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "    #######################################################\n",
        "    self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "    return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "    self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.backbone(images))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s=FeatureExtractor()\n",
        "s(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "cruqtZEMgSqT",
        "outputId": "25665b2a-8f6b-4920-b9d7-a8b3323fad01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b928c7692e82>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-53573b0151d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-53573b0151d9>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(feature_maps)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mconv7_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out_conv7_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mrfem11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0mrfem12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv5_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mrfem13\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv7_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: rfem() missing 1 required positional argument: 'block'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGeLQ7LwhVwc"
      },
      "outputs": [],
      "source": [
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "# device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# for param in model.named_parameters():\n",
        "#   param[1].requres_grad = True\n",
        "# model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "# # backbone = torch.nn.Sequential(backbone)\n",
        "# dd = backbone(x)\n",
        "# y = training(dd)\n",
        "\n",
        "\n",
        "# i = torch.nn.Conv2d(y.shape[1], y.shape[1], kernel_size=1)(y)\n",
        "# m = torch.nn.Sequential([i])\n",
        "# optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
        "\n",
        "# y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UX7-ezAPiM3B",
        "outputId": "015d1ac1-917b-4180-f719-d2bc4eec4d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 74.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "backbone = torch.nn.Sequential(*req_layers)\n",
        "for param in backbone.named_parameters():\n",
        "  param[1].requres_grad = True\n",
        "z = backbone(x)\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    print(out.shape)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "2aefaa1b-290a-42d8-e8f2-6fabcc80cc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnKE3neESZ5",
        "outputId": "aac4e7e1-10e2-467c-915f-0f79864c086e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 512, 40, 30])\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRW4iC1uXf4"
      },
      "outputs": [],
      "source": [
        "for name, param in detector.named_parameters():\n",
        "    print(name, param.data)\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2nsbro-Tse"
      },
      "outputs": [],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dOW6SREUn2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}