{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "40a86fcf-da72-49c1-8845-69439cd6fc75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "!pip install torch_intermediate_layer_getter\n",
        "from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001dd363-1ce2-474a-ddcd-4f73833ac027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "def rfem(x):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4)(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4)(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4)(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same')(x)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same')(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4)(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "\n",
        "  return torch.add(combined_x, x/0.5)\n",
        "\n",
        "def ftb(current_layer, former_layer=None):\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1)(current_layer)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1)(x)\n",
        "  \n",
        "  if former_layer is None:\n",
        "    return x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1)(former_layer)\n",
        "    return torch.add(x, d)\n",
        "\n",
        "\n",
        "def training(feature_maps):\n",
        "  conv4_2 = feature_maps[0]['out_conv4_2']\n",
        "  conv4_2=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_2)\n",
        "  conv4_2=torch.nn.ReLU()(conv4_2)\n",
        "  conv4_3 = feature_maps[0]['out_conv4_3']\n",
        "  conv4_3=torch.nn.Conv2d(in_channels=512, out_channels=512, kernel_size=2, stride=2)(conv4_3)\n",
        "  conv4_3=torch.nn.ReLU()(conv4_3)\n",
        "  conv5_3 = feature_maps[0]['out_conv5_3'][0]\n",
        "  conv6_2 = feature_maps[0]['out_conv6_2'][0]\n",
        "  conv7_2 = feature_maps[0]['out_conv7_2'][0]\n",
        "\n",
        "  rfem11 = torch.add(rfem(conv4_2), conv4_3)\n",
        "  rfem12 = torch.add(rfem(rfem11), conv5_3)\n",
        "  rfem13 = torch.add(rfem(rfem12), conv7_2)\n",
        "  rfem14 = torch.add(rfem(rfem13), conv6_2)\n",
        "\n",
        "  ftb_4 = ftb(rfem14)\n",
        "  ftb_3 = ftb(rfem13)\n",
        "  ftb_2 = ftb(rfem12)\n",
        "  ftb_1 = ftb(rfem11)\n",
        "\n",
        "  rfem21 = torch.add(rfem(ftb_1), conv4_3)\n",
        "  rfem22 = torch.add(rfem(ftb_2), conv5_3)\n",
        "  rfem23 = torch.add(rfem(ftb_3), conv7_2)\n",
        "  rfem24 = torch.add(rfem(ftb_4), conv6_2)\n",
        "\n",
        "  det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "  det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "  det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "  det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "\n",
        "  D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "\n",
        "  pp=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1)(D)\n",
        "  torch.nn.Sequential()\n",
        "  \n",
        "  \n",
        "  return D\n",
        "\n",
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "    device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    for param in model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "    model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "    return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "    self.backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.backbone(images))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# model = torchvision.models.vgg16(weights='IMAGENET1K_V1')\n",
        "# device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = model.to(device)\n",
        "# for param in model.named_parameters():\n",
        "#   param[1].requres_grad = True\n",
        "# model.features = torch.nn.Sequential(*([model.features[i] for i in range(30)] + [model.features[i] for i in range(23, 30)]))\n",
        "# return_layers = {'20': 'out_conv4_2', '22': 'out_conv4_3', '29': 'out_conv5_3','34': 'out_conv7_2', '36': 'out_conv6_2'}\n",
        "# backbone = IntermediateLayerGetter(model.features, return_layers=return_layers)\n",
        "# # backbone = torch.nn.Sequential(backbone)\n",
        "# dd = backbone(x)\n",
        "# y = training(dd)\n",
        "\n",
        "\n",
        "# i = torch.nn.Conv2d(y.shape[1], y.shape[1], kernel_size=1)(y)\n",
        "# m = torch.nn.Sequential([i])\n",
        "# optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "xGeLQ7LwhVwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7bbcb07-f4b9-418c-e201-a177fcf7f9a5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.5623e-04, 6.3551e-05, 6.8429e-05,  ..., 9.7480e-05,\n",
              "           5.0096e-05, 1.1709e-04],\n",
              "          [2.9337e-05, 5.1427e-04, 2.1523e-04,  ..., 1.4988e-04,\n",
              "           2.7488e-06, 1.3711e-04],\n",
              "          [6.0908e-04, 7.2120e-05, 4.0614e-04,  ..., 1.3078e-04,\n",
              "           1.4358e-04, 1.7766e-05],\n",
              "          ...,\n",
              "          [8.9665e-06, 6.1010e-05, 1.9174e-05,  ..., 2.4165e-04,\n",
              "           3.7494e-04, 1.3055e-05],\n",
              "          [3.7621e-04, 3.4387e-05, 3.8275e-05,  ..., 1.4155e-04,\n",
              "           8.3442e-05, 4.6387e-05],\n",
              "          [1.2785e-04, 8.2776e-05, 4.8940e-04,  ..., 2.1911e-04,\n",
              "           9.3782e-05, 2.6039e-07]],\n",
              "\n",
              "         [[7.3664e-05, 4.7621e-04, 3.9670e-04,  ..., 6.0521e-05,\n",
              "           7.8811e-04, 1.1912e-04],\n",
              "          [2.0994e-04, 4.4477e-05, 1.0836e-04,  ..., 7.6435e-05,\n",
              "           3.2754e-06, 1.6020e-04],\n",
              "          [6.3438e-04, 3.4258e-04, 4.2619e-05,  ..., 1.1011e-04,\n",
              "           8.2397e-05, 5.2262e-05],\n",
              "          ...,\n",
              "          [4.4546e-04, 3.8482e-05, 1.8825e-04,  ..., 9.7835e-04,\n",
              "           1.4704e-04, 3.9941e-05],\n",
              "          [7.2286e-04, 3.0338e-05, 8.7442e-05,  ..., 1.0981e-03,\n",
              "           3.6059e-04, 3.6023e-04],\n",
              "          [1.0814e-04, 1.3046e-05, 3.9442e-05,  ..., 2.1251e-04,\n",
              "           7.1843e-05, 1.1666e-07]],\n",
              "\n",
              "         [[3.8766e-05, 2.7345e-04, 1.0353e-04,  ..., 3.8685e-04,\n",
              "           8.0255e-05, 1.1645e-04],\n",
              "          [1.2486e-04, 1.4305e-04, 1.3828e-04,  ..., 2.2497e-04,\n",
              "           5.4302e-06, 8.1429e-04],\n",
              "          [1.4271e-04, 1.0718e-04, 1.2123e-04,  ..., 4.6248e-04,\n",
              "           3.0828e-04, 1.4120e-05],\n",
              "          ...,\n",
              "          [1.3847e-05, 4.4409e-05, 3.0808e-05,  ..., 3.1949e-04,\n",
              "           6.9637e-05, 2.0154e-02],\n",
              "          [1.1057e-04, 5.5863e-05, 5.5290e-05,  ..., 7.9115e-05,\n",
              "           6.6587e-05, 2.0068e-04],\n",
              "          [7.9019e-05, 1.6508e-04, 1.3833e-01,  ..., 2.2874e-04,\n",
              "           1.0148e-04, 2.0962e-07]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[4.5745e-12, 1.2412e-11, 6.8531e-10,  ..., 5.2257e-11,\n",
              "           2.6880e-14, 1.0726e-09],\n",
              "          [8.0718e-15, 2.1935e-17, 6.9555e-20,  ..., 2.7827e-16,\n",
              "           2.4762e-15, 4.6762e-11],\n",
              "          [7.5162e-13, 1.0750e-12, 2.3360e-16,  ..., 7.8775e-14,\n",
              "           1.6786e-12, 1.2832e-11],\n",
              "          ...,\n",
              "          [1.4336e-13, 3.5628e-17, 3.8857e-14,  ..., 4.6635e-09,\n",
              "           1.8145e-11, 3.5553e-10],\n",
              "          [6.3948e-11, 5.4174e-16, 1.6215e-17,  ..., 3.9244e-12,\n",
              "           4.6080e-13, 5.4752e-12],\n",
              "          [2.7599e-11, 9.1400e-12, 1.3704e-11,  ..., 1.1763e-10,\n",
              "           2.6240e-14, 3.9331e-12]],\n",
              "\n",
              "         [[4.0768e-11, 8.1178e-08, 1.2218e-10,  ..., 2.3631e-08,\n",
              "           4.2355e-11, 5.9015e-14],\n",
              "          [2.5368e-12, 1.0296e-17, 2.7506e-15,  ..., 1.2956e-15,\n",
              "           2.9954e-11, 1.7752e-13],\n",
              "          [5.5455e-09, 1.5391e-14, 1.1971e-11,  ..., 4.4135e-11,\n",
              "           2.0443e-11, 7.9165e-14],\n",
              "          ...,\n",
              "          [1.2448e-09, 4.5226e-12, 1.0054e-11,  ..., 9.5408e-11,\n",
              "           4.4512e-11, 1.6353e-09],\n",
              "          [2.5636e-11, 2.9275e-16, 1.0834e-15,  ..., 3.4139e-12,\n",
              "           4.0579e-14, 4.7949e-14],\n",
              "          [7.3780e-09, 2.8362e-16, 2.9855e-13,  ..., 7.3219e-11,\n",
              "           5.2029e-10, 5.5437e-12]],\n",
              "\n",
              "         [[3.0457e-08, 1.0271e-07, 2.5039e-08,  ..., 3.8655e-10,\n",
              "           1.2332e-13, 3.4382e-08],\n",
              "          [2.5586e-12, 9.0230e-16, 4.0638e-16,  ..., 2.4643e-13,\n",
              "           2.7094e-13, 6.0646e-09],\n",
              "          [1.0305e-07, 1.7973e-08, 2.1878e-13,  ..., 4.0799e-10,\n",
              "           7.1527e-15, 1.0421e-10],\n",
              "          ...,\n",
              "          [2.4062e-13, 4.6719e-08, 6.6718e-11,  ..., 6.9060e-12,\n",
              "           1.4915e-12, 7.0552e-11],\n",
              "          [1.1476e-10, 2.7904e-15, 3.6610e-12,  ..., 1.7312e-08,\n",
              "           5.6084e-14, 2.0815e-12],\n",
              "          [1.4133e-10, 6.0984e-13, 1.8021e-07,  ..., 1.1418e-09,\n",
              "           2.0809e-08, 1.4035e-08]]],\n",
              "\n",
              "\n",
              "        [[[3.0610e-05, 5.2917e-05, 1.0525e-04,  ..., 1.5346e-04,\n",
              "           5.6016e-05, 6.8507e-07],\n",
              "          [1.7340e-04, 2.4950e-04, 3.0343e-04,  ..., 1.2345e-04,\n",
              "           5.6741e-05, 1.5018e-04],\n",
              "          [5.6166e-04, 1.4602e-04, 1.3452e-04,  ..., 5.2226e-04,\n",
              "           1.1294e-04, 5.0652e-04],\n",
              "          ...,\n",
              "          [4.3812e-05, 3.5775e-05, 1.4112e-04,  ..., 1.1348e-04,\n",
              "           9.0418e-05, 1.5913e-04],\n",
              "          [5.6038e-05, 1.3419e-05, 3.4516e-05,  ..., 7.8581e-05,\n",
              "           9.3201e-05, 4.5894e-05],\n",
              "          [1.5429e-04, 4.0298e-04, 1.0208e-04,  ..., 3.1569e-04,\n",
              "           1.3804e-04, 3.6546e-04]],\n",
              "\n",
              "         [[2.2724e-05, 1.6275e-04, 1.4931e-04,  ..., 1.9274e-04,\n",
              "           1.0512e-04, 9.2464e-07],\n",
              "          [1.2302e-04, 2.8872e-05, 1.3492e-04,  ..., 2.2888e-04,\n",
              "           3.3921e-05, 5.4512e-04],\n",
              "          [6.2196e-03, 2.4620e-04, 5.7767e-04,  ..., 1.0051e-03,\n",
              "           4.0138e-04, 1.3265e-04],\n",
              "          ...,\n",
              "          [7.1939e-04, 1.2897e-04, 2.0822e-04,  ..., 1.2995e-03,\n",
              "           2.4107e-04, 3.7259e-04],\n",
              "          [2.2675e-03, 7.1608e-06, 7.5978e-06,  ..., 1.4831e-03,\n",
              "           2.1821e-04, 8.7290e-04],\n",
              "          [1.1348e-03, 2.8630e-05, 2.7208e-05,  ..., 5.8406e-04,\n",
              "           9.7775e-05, 4.3470e-04]],\n",
              "\n",
              "         [[1.2271e-04, 1.2239e-04, 1.7129e-04,  ..., 1.3019e-04,\n",
              "           1.3065e-04, 1.8177e-06],\n",
              "          [9.4807e-05, 1.0402e-04, 1.1248e-04,  ..., 1.1997e-04,\n",
              "           6.6724e-05, 4.4906e-05],\n",
              "          [1.9051e-04, 9.5531e-05, 1.3123e-04,  ..., 1.6641e-04,\n",
              "           1.2175e-04, 2.6168e-03],\n",
              "          ...,\n",
              "          [3.5900e-05, 7.4736e-05, 1.2253e-04,  ..., 1.7545e-04,\n",
              "           1.4137e-04, 5.8873e-04],\n",
              "          [7.3045e-05, 1.9042e-05, 3.1985e-05,  ..., 5.3690e-05,\n",
              "           1.2863e-04, 5.0411e-03],\n",
              "          [1.4240e-04, 1.5923e-04, 1.7295e-04,  ..., 5.1744e-04,\n",
              "           2.5785e-04, 6.8299e-04]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[4.0336e-10, 2.1646e-11, 5.1940e-12,  ..., 2.3003e-10,\n",
              "           4.4239e-15, 1.3041e-11],\n",
              "          [7.0328e-11, 1.5371e-16, 3.4603e-21,  ..., 6.9322e-16,\n",
              "           5.4986e-17, 2.4059e-11],\n",
              "          [3.5806e-11, 2.2194e-17, 3.9340e-15,  ..., 1.0828e-15,\n",
              "           2.0604e-17, 2.6475e-12],\n",
              "          ...,\n",
              "          [2.2802e-11, 1.0537e-15, 2.4162e-12,  ..., 3.6527e-11,\n",
              "           1.8122e-11, 4.6320e-10],\n",
              "          [4.8186e-17, 1.6083e-16, 3.9062e-13,  ..., 1.7592e-09,\n",
              "           2.4653e-15, 8.5382e-08],\n",
              "          [2.0578e-10, 3.5956e-12, 2.4177e-09,  ..., 1.0531e-07,\n",
              "           8.5967e-11, 3.7434e-09]],\n",
              "\n",
              "         [[5.4793e-10, 2.5360e-09, 1.5020e-10,  ..., 4.1320e-08,\n",
              "           1.3826e-14, 1.4588e-14],\n",
              "          [4.3025e-10, 1.3948e-16, 7.6370e-14,  ..., 4.6751e-12,\n",
              "           5.8569e-13, 5.2399e-14],\n",
              "          [4.2601e-10, 9.7036e-19, 9.9899e-13,  ..., 2.7501e-14,\n",
              "           2.5563e-16, 2.3131e-14],\n",
              "          ...,\n",
              "          [3.5879e-10, 9.6061e-14, 3.2368e-13,  ..., 5.1659e-12,\n",
              "           2.1433e-08, 2.3257e-16],\n",
              "          [1.6311e-16, 1.7243e-12, 1.7778e-14,  ..., 1.6934e-09,\n",
              "           6.1603e-11, 8.2597e-15],\n",
              "          [6.4184e-09, 1.3482e-17, 7.8784e-10,  ..., 1.4404e-07,\n",
              "           8.0269e-11, 2.0353e-10]],\n",
              "\n",
              "         [[1.0373e-10, 5.9882e-08, 5.3049e-08,  ..., 3.1696e-11,\n",
              "           6.4358e-14, 1.8194e-10],\n",
              "          [1.3780e-10, 2.9324e-13, 2.8607e-13,  ..., 4.4658e-17,\n",
              "           2.3991e-13, 5.9792e-12],\n",
              "          [2.6131e-06, 2.3592e-08, 2.3498e-11,  ..., 3.8306e-14,\n",
              "           2.8514e-15, 3.4677e-11],\n",
              "          ...,\n",
              "          [1.0133e-09, 1.1309e-09, 1.9821e-08,  ..., 1.3494e-13,\n",
              "           9.4634e-11, 2.2217e-10],\n",
              "          [1.4701e-13, 1.6093e-12, 3.7735e-10,  ..., 3.3904e-12,\n",
              "           7.4547e-16, 9.1667e-11],\n",
              "          [7.3895e-08, 7.1649e-15, 1.3073e-06,  ..., 1.3237e-09,\n",
              "           7.3917e-10, 7.3473e-08]]],\n",
              "\n",
              "\n",
              "        [[[1.0187e-04, 1.0651e-05, 4.2417e-05,  ..., 2.8954e-05,\n",
              "           7.3227e-06, 1.1273e-05],\n",
              "          [2.8311e-05, 9.2805e-06, 4.9873e-05,  ..., 1.9503e-05,\n",
              "           9.4732e-06, 7.0327e-04],\n",
              "          [8.9624e-05, 2.0256e-05, 9.7817e-05,  ..., 6.1310e-05,\n",
              "           7.4970e-05, 1.5188e-04],\n",
              "          ...,\n",
              "          [1.8225e-05, 2.8078e-05, 4.8518e-05,  ..., 1.8819e-04,\n",
              "           7.0281e-05, 9.7740e-05],\n",
              "          [3.7337e-05, 5.2186e-05, 4.7616e-05,  ..., 2.8263e-04,\n",
              "           1.2640e-04, 4.9446e-05],\n",
              "          [4.6119e-04, 3.2125e-04, 1.2425e-04,  ..., 2.9891e-05,\n",
              "           1.5537e-04, 4.9118e-05]],\n",
              "\n",
              "         [[8.6320e-05, 5.0053e-05, 1.0042e-04,  ..., 4.9147e-05,\n",
              "           3.0680e-05, 5.4362e-05],\n",
              "          [1.0290e-03, 1.9239e-05, 3.2278e-05,  ..., 5.0653e-05,\n",
              "           1.8304e-05, 5.6575e-05],\n",
              "          [7.3386e-03, 2.3642e-04, 4.9182e-05,  ..., 4.7290e-05,\n",
              "           2.5716e-03, 1.1078e-03],\n",
              "          ...,\n",
              "          [3.6914e-04, 1.3185e-05, 3.1527e-04,  ..., 8.1709e-04,\n",
              "           1.6603e-04, 2.6774e-04],\n",
              "          [6.6472e-04, 9.6659e-05, 4.8257e-04,  ..., 4.5307e-05,\n",
              "           5.5905e-05, 2.7421e-04],\n",
              "          [3.9384e-04, 2.1287e-04, 1.2654e-04,  ..., 1.9858e-05,\n",
              "           4.7391e-04, 2.0544e-05]],\n",
              "\n",
              "         [[2.5015e-04, 1.3856e-04, 7.5612e-05,  ..., 6.4631e-05,\n",
              "           1.4875e-05, 7.1438e-04],\n",
              "          [7.6251e-05, 2.7805e-05, 4.2198e-05,  ..., 5.3808e-05,\n",
              "           3.5066e-05, 8.9259e-05],\n",
              "          [1.7480e-04, 6.3048e-05, 1.6833e-04,  ..., 7.5191e-05,\n",
              "           6.8127e-05, 3.6341e-04],\n",
              "          ...,\n",
              "          [1.8653e-05, 3.5845e-05, 2.5809e-05,  ..., 1.6565e-04,\n",
              "           9.5958e-05, 7.5991e-05],\n",
              "          [2.2040e-04, 8.6792e-05, 2.6808e-05,  ..., 2.4705e-04,\n",
              "           1.3853e-04, 1.0709e-04],\n",
              "          [3.8816e-04, 1.2208e-04, 4.1022e-02,  ..., 6.9020e-04,\n",
              "           1.0631e-04, 6.5066e-03]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[2.1182e-10, 8.9594e-16, 1.0068e-13,  ..., 2.2816e-14,\n",
              "           5.1968e-14, 6.8903e-11],\n",
              "          [1.1724e-11, 8.9035e-18, 8.1415e-22,  ..., 2.3421e-19,\n",
              "           2.4052e-21, 5.8393e-13],\n",
              "          [4.7957e-15, 3.0502e-16, 1.9951e-15,  ..., 7.5160e-15,\n",
              "           4.4048e-20, 5.1465e-16],\n",
              "          ...,\n",
              "          [2.0764e-12, 3.3720e-13, 3.6410e-13,  ..., 1.3735e-11,\n",
              "           9.9517e-12, 1.6731e-08],\n",
              "          [1.6307e-12, 2.4374e-15, 4.5174e-15,  ..., 1.7737e-13,\n",
              "           2.5543e-12, 1.3870e-09],\n",
              "          [4.6032e-10, 1.6721e-13, 1.0196e-08,  ..., 1.5625e-11,\n",
              "           1.4948e-10, 8.1198e-11]],\n",
              "\n",
              "         [[2.4391e-12, 1.8222e-15, 2.1562e-09,  ..., 1.1348e-13,\n",
              "           7.1075e-13, 1.2613e-17],\n",
              "          [1.3638e-10, 5.1130e-18, 3.3523e-13,  ..., 1.5883e-18,\n",
              "           5.6892e-14, 3.1498e-18],\n",
              "          [7.9380e-11, 3.9914e-13, 7.5858e-07,  ..., 2.2264e-12,\n",
              "           1.0141e-14, 2.0514e-20],\n",
              "          ...,\n",
              "          [9.8227e-15, 4.7237e-11, 1.7295e-11,  ..., 2.5302e-07,\n",
              "           8.5664e-09, 1.2781e-10],\n",
              "          [3.1797e-10, 4.6116e-18, 1.1672e-09,  ..., 1.2891e-14,\n",
              "           1.1683e-09, 6.0386e-14],\n",
              "          [3.1800e-06, 3.2892e-15, 7.1992e-11,  ..., 1.1766e-10,\n",
              "           8.6381e-13, 3.9957e-12]],\n",
              "\n",
              "         [[9.7977e-11, 6.4940e-13, 1.0015e-08,  ..., 4.7727e-13,\n",
              "           1.4466e-10, 1.7189e-10],\n",
              "          [2.3603e-13, 3.5445e-11, 3.8252e-16,  ..., 1.4901e-16,\n",
              "           8.5698e-14, 1.6656e-10],\n",
              "          [6.2776e-08, 4.9126e-07, 1.7883e-13,  ..., 3.7706e-11,\n",
              "           2.1131e-17, 1.3758e-16],\n",
              "          ...,\n",
              "          [1.6528e-12, 1.9493e-07, 3.8374e-08,  ..., 7.5500e-10,\n",
              "           3.0987e-11, 2.6441e-10],\n",
              "          [4.9683e-15, 1.2224e-11, 1.5504e-10,  ..., 3.4432e-09,\n",
              "           4.4795e-09, 5.0766e-10],\n",
              "          [2.0645e-07, 2.4524e-12, 3.6305e-06,  ..., 4.0712e-10,\n",
              "           1.0133e-10, 1.2814e-08]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[5.1908e-05, 4.5931e-06, 6.5118e-06,  ..., 3.2180e-04,\n",
              "           1.7798e-06, 1.3543e-09],\n",
              "          [9.4432e-05, 2.6255e-04, 2.3021e-04,  ..., 1.3142e-04,\n",
              "           1.6827e-05, 3.0965e-06],\n",
              "          [1.1597e-04, 2.3223e-04, 3.7815e-04,  ..., 1.0679e-02,\n",
              "           9.0277e-05, 2.7988e-06],\n",
              "          ...,\n",
              "          [3.0451e-05, 7.9224e-05, 1.2248e-03,  ..., 1.6097e-04,\n",
              "           6.9416e-05, 1.7150e-05],\n",
              "          [4.0779e-06, 4.4732e-05, 1.3248e-04,  ..., 9.4332e-04,\n",
              "           1.3805e-03, 3.9181e-06],\n",
              "          [1.2261e-07, 2.8470e-07, 2.9144e-05,  ..., 3.7290e-05,\n",
              "           2.0110e-05, 1.0267e-06]],\n",
              "\n",
              "         [[1.5735e-06, 6.8124e-05, 6.4054e-05,  ..., 1.0793e-03,\n",
              "           4.3244e-05, 7.8788e-07],\n",
              "          [1.4918e-05, 4.7805e-05, 2.0368e-04,  ..., 6.6906e-05,\n",
              "           4.3924e-05, 1.0198e-05],\n",
              "          [7.7690e-05, 4.5167e-04, 4.9637e-04,  ..., 4.7030e-04,\n",
              "           9.5207e-05, 3.2772e-05],\n",
              "          ...,\n",
              "          [1.1266e-04, 2.3162e-04, 5.4330e-04,  ..., 4.8611e-04,\n",
              "           2.4464e-04, 6.1897e-05],\n",
              "          [1.2923e-05, 1.3078e-04, 3.0958e-04,  ..., 2.9771e-04,\n",
              "           1.2761e-04, 1.0456e-05],\n",
              "          [2.2271e-07, 1.5232e-06, 8.6568e-05,  ..., 2.6016e-04,\n",
              "           2.8615e-04, 4.4436e-06]],\n",
              "\n",
              "         [[4.5732e-04, 2.3503e-04, 2.0178e-04,  ..., 9.1269e-04,\n",
              "           1.3306e-03, 6.1064e-06],\n",
              "          [3.3978e-05, 9.5611e-05, 5.0786e-04,  ..., 1.0998e-03,\n",
              "           1.0971e-04, 1.8750e-05],\n",
              "          [3.3609e-04, 4.6119e-04, 6.9386e-04,  ..., 6.1020e-04,\n",
              "           1.2923e-04, 5.2847e-05],\n",
              "          ...,\n",
              "          [2.8136e-04, 2.6790e-04, 8.0200e-04,  ..., 5.4862e-04,\n",
              "           1.5865e-04, 1.5326e-04],\n",
              "          [1.3854e-04, 2.7574e-04, 4.4278e-04,  ..., 5.0896e-04,\n",
              "           8.6020e-05, 7.9969e-06],\n",
              "          [7.8492e-07, 8.7842e-06, 3.1604e-04,  ..., 4.8440e-04,\n",
              "           3.9153e-05, 7.2070e-06]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 6.6301e-37, 4.8243e-36,  ..., 4.1987e-31,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 3.5328e-29, 1.2548e-08,  ..., 5.0501e-34,\n",
              "           3.2464e-41, 0.0000e+00],\n",
              "          [2.1358e-29, 9.1088e-20, 3.6578e-21,  ..., 9.1590e-18,\n",
              "           8.5538e-35, 3.2230e-44],\n",
              "          ...,\n",
              "          [4.7665e-34, 2.4095e-05, 5.4857e-10,  ..., 1.5049e-22,\n",
              "           4.2231e-28, 2.0120e-19],\n",
              "          [2.2921e-35, 2.8715e-24, 8.2814e-11,  ..., 5.2726e-18,\n",
              "           2.7113e-31, 5.8339e-39],\n",
              "          [0.0000e+00, 3.0506e-42, 1.6233e-24,  ..., 2.0900e-31,\n",
              "           7.2087e-36, 8.2677e-44]],\n",
              "\n",
              "         [[5.9851e-39, 8.9988e-33, 1.5839e-37,  ..., 2.2255e-23,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [3.6518e-37, 4.6293e-33, 2.2717e-23,  ..., 1.3524e-27,\n",
              "           8.4614e-27, 1.4976e-40],\n",
              "          [2.9767e-27, 4.3517e-18, 5.4194e-19,  ..., 2.2358e-10,\n",
              "           1.7220e-28, 1.6007e-38],\n",
              "          ...,\n",
              "          [5.3133e-30, 1.0931e-25, 4.6131e-17,  ..., 1.0264e-07,\n",
              "           1.5322e-26, 7.1610e-23],\n",
              "          [5.8094e-36, 1.4916e-16, 3.2471e-20,  ..., 5.9768e-14,\n",
              "           1.6610e-22, 1.3197e-27],\n",
              "          [2.8026e-45, 0.0000e+00, 2.1631e-23,  ..., 6.0676e-32,\n",
              "           5.9315e-30, 1.0533e-31]],\n",
              "\n",
              "         [[0.0000e+00, 1.6422e-41, 1.9714e-38,  ..., 2.3218e-24,\n",
              "           8.1567e-41, 0.0000e+00],\n",
              "          [9.5104e-35, 7.5467e-30, 6.5445e-09,  ..., 2.9312e-29,\n",
              "           1.2948e-32, 1.8367e-29],\n",
              "          [2.2158e-32, 1.7248e-22, 5.9979e-24,  ..., 1.2588e-07,\n",
              "           1.7760e-28, 0.0000e+00],\n",
              "          ...,\n",
              "          [5.7507e-35, 3.7877e-04, 1.9440e-18,  ..., 1.9688e-24,\n",
              "           9.5018e-21, 3.5053e-29],\n",
              "          [3.5501e-36, 9.4929e-25, 1.4536e-23,  ..., 3.8906e-23,\n",
              "           1.1195e-30, 2.3219e-40],\n",
              "          [0.0000e+00, 0.0000e+00, 2.6997e-34,  ..., 1.7810e-30,\n",
              "           7.2512e-35, 0.0000e+00]]],\n",
              "\n",
              "\n",
              "        [[[1.1100e-08, 1.5555e-06, 4.0958e-06,  ..., 1.2370e-06,\n",
              "           7.7584e-07, 9.7656e-09],\n",
              "          [9.3838e-07, 1.6674e-05, 4.2026e-04,  ..., 4.3354e-04,\n",
              "           1.2066e-04, 5.1055e-06],\n",
              "          [1.3127e-11, 2.9883e-05, 2.7982e-04,  ..., 1.5816e-03,\n",
              "           3.8840e-04, 1.4758e-05],\n",
              "          ...,\n",
              "          [3.3141e-07, 3.3055e-05, 2.1682e-04,  ..., 1.0233e-03,\n",
              "           2.3090e-03, 6.1784e-07],\n",
              "          [1.5389e-04, 1.2332e-04, 3.3816e-03,  ..., 1.2919e-04,\n",
              "           1.8892e-04, 1.1980e-06],\n",
              "          [6.1528e-08, 2.1585e-05, 2.1683e-06,  ..., 1.3651e-06,\n",
              "           2.0899e-08, 3.2994e-08]],\n",
              "\n",
              "         [[3.9468e-07, 8.7221e-05, 5.8623e-05,  ..., 7.0072e-07,\n",
              "           6.8250e-06, 1.0342e-06],\n",
              "          [3.0709e-06, 3.5427e-05, 1.1255e-04,  ..., 2.6241e-04,\n",
              "           1.8115e-04, 7.1444e-05],\n",
              "          [9.3773e-12, 8.7367e-05, 2.6314e-04,  ..., 5.2979e-04,\n",
              "           3.2579e-04, 1.7178e-04],\n",
              "          ...,\n",
              "          [1.0916e-06, 1.7980e-05, 4.6840e-04,  ..., 6.3698e-04,\n",
              "           4.7506e-04, 3.0370e-06],\n",
              "          [1.3824e-05, 4.6735e-05, 3.9434e-04,  ..., 3.7769e-04,\n",
              "           2.3432e-04, 4.0168e-06],\n",
              "          [3.8261e-07, 6.3107e-05, 6.3391e-06,  ..., 7.9673e-06,\n",
              "           8.3632e-08, 2.0956e-07]],\n",
              "\n",
              "         [[3.1032e-06, 1.9759e-05, 4.2807e-04,  ..., 1.8150e-06,\n",
              "           3.0833e-03, 1.4111e-05],\n",
              "          [9.9435e-06, 5.7273e-05, 3.7787e-04,  ..., 8.7754e-04,\n",
              "           6.2663e-04, 4.1461e-04],\n",
              "          [3.1049e-11, 1.4274e-04, 2.6921e-03,  ..., 4.1708e-04,\n",
              "           4.5908e-04, 1.3244e-04],\n",
              "          ...,\n",
              "          [4.3069e-06, 2.4475e-05, 3.0410e-03,  ..., 2.7505e-03,\n",
              "           1.4013e-03, 3.4385e-06],\n",
              "          [1.8338e-05, 7.9897e-05, 3.4537e-03,  ..., 1.4128e-03,\n",
              "           1.6489e-04, 7.0473e-06],\n",
              "          [2.7634e-06, 5.5915e-04, 4.3139e-05,  ..., 1.7259e-05,\n",
              "           1.2663e-07, 2.5832e-06]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 1.1612e-37, 4.1952e-32,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [0.0000e+00, 3.8191e-29, 9.8428e-14,  ..., 5.6047e-28,\n",
              "           3.2835e-32, 1.1083e-36],\n",
              "          [0.0000e+00, 1.1877e-27, 6.9875e-28,  ..., 3.0425e-25,\n",
              "           8.9446e-21, 3.5935e-29],\n",
              "          ...,\n",
              "          [0.0000e+00, 2.0489e-33, 6.7179e-15,  ..., 3.6431e-28,\n",
              "           3.6743e-23, 6.9094e-41],\n",
              "          [8.5759e-42, 7.2669e-11, 3.1328e-26,  ..., 2.7036e-23,\n",
              "           9.9882e-37, 2.1655e-34],\n",
              "          [0.0000e+00, 6.1287e-25, 6.5902e-31,  ..., 8.4078e-45,\n",
              "           0.0000e+00, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 3.1922e-42, 5.6364e-39,  ..., 0.0000e+00,\n",
              "           0.0000e+00, 0.0000e+00],\n",
              "          [2.3653e-33, 5.9768e-16, 2.9196e-19,  ..., 2.3107e-31,\n",
              "           2.2577e-31, 8.7862e-37],\n",
              "          [0.0000e+00, 4.5097e-20, 1.3439e-07,  ..., 3.2451e-15,\n",
              "           2.4182e-13, 1.4362e-18],\n",
              "          ...,\n",
              "          [0.0000e+00, 8.0757e-33, 5.0583e-21,  ..., 3.5867e-27,\n",
              "           1.9057e-18, 1.7401e-39],\n",
              "          [0.0000e+00, 1.2195e-36, 2.6989e-27,  ..., 4.6002e-24,\n",
              "           2.8343e-23, 3.4375e-33],\n",
              "          [0.0000e+00, 1.1605e-27, 2.2731e-20,  ..., 2.5160e-37,\n",
              "           0.0000e+00, 1.9374e-39]],\n",
              "\n",
              "         [[0.0000e+00, 7.8207e-27, 2.6583e-26,  ..., 0.0000e+00,\n",
              "           7.9589e-29, 0.0000e+00],\n",
              "          [2.1870e-40, 2.7365e-37, 2.1420e-31,  ..., 1.1898e-27,\n",
              "           2.6525e-27, 9.4773e-40],\n",
              "          [0.0000e+00, 6.1936e-26, 1.7186e-20,  ..., 1.9378e-18,\n",
              "           2.4195e-22, 4.4764e-24],\n",
              "          ...,\n",
              "          [0.0000e+00, 9.6795e-30, 8.4669e-27,  ..., 5.4067e-25,\n",
              "           4.8520e-22, 0.0000e+00],\n",
              "          [0.0000e+00, 9.8710e-24, 8.5664e-24,  ..., 1.5417e-20,\n",
              "           3.8588e-23, 1.4368e-40],\n",
              "          [0.0000e+00, 1.9631e-34, 1.1911e-42,  ..., 7.0065e-44,\n",
              "           0.0000e+00, 0.0000e+00]]],\n",
              "\n",
              "\n",
              "        [[[1.5790e-05, 2.5028e-05, 6.3182e-05,  ..., 3.2492e-06,\n",
              "           2.3918e-06, 1.8567e-10],\n",
              "          [2.3672e-05, 3.4427e-04, 3.1471e-03,  ..., 9.7926e-05,\n",
              "           6.8346e-05, 2.2152e-06],\n",
              "          [1.3117e-05, 6.0311e-04, 1.2480e-03,  ..., 7.3623e-04,\n",
              "           1.6800e-03, 3.2256e-05],\n",
              "          ...,\n",
              "          [2.6682e-05, 5.9363e-05, 8.5904e-04,  ..., 7.4543e-04,\n",
              "           2.6563e-03, 1.6498e-05],\n",
              "          [9.1073e-08, 6.5975e-05, 6.1439e-04,  ..., 2.5293e-05,\n",
              "           2.6776e-04, 6.6022e-06],\n",
              "          [1.5970e-08, 4.2886e-12, 1.0529e-05,  ..., 2.8362e-07,\n",
              "           5.8124e-06, 7.9998e-08]],\n",
              "\n",
              "         [[4.9708e-06, 3.2861e-04, 1.3676e-04,  ..., 1.1637e-04,\n",
              "           5.7014e-05, 2.5559e-08],\n",
              "          [3.3578e-05, 1.8036e-04, 2.7830e-04,  ..., 2.5019e-04,\n",
              "           1.3212e-04, 1.4194e-05],\n",
              "          [3.8350e-05, 4.3530e-04, 6.5870e-04,  ..., 3.0597e-04,\n",
              "           4.7114e-04, 9.4305e-05],\n",
              "          ...,\n",
              "          [4.7624e-05, 9.2183e-05, 4.0189e-04,  ..., 7.6402e-04,\n",
              "           3.6983e-04, 4.8234e-05],\n",
              "          [3.0912e-07, 1.1957e-04, 2.7092e-04,  ..., 7.4362e-05,\n",
              "           4.6440e-05, 2.2320e-05],\n",
              "          [8.4068e-08, 1.6000e-11, 3.0784e-05,  ..., 9.1426e-01,\n",
              "           1.0779e-05, 2.3388e-07]],\n",
              "\n",
              "         [[2.2456e-04, 1.8087e-04, 1.5789e-03,  ..., 4.3421e-05,\n",
              "           4.2713e-05, 2.4124e-01],\n",
              "          [2.9027e-04, 4.3614e-04, 2.1286e-04,  ..., 5.6353e-03,\n",
              "           1.6548e-04, 1.2162e-05],\n",
              "          [9.8879e-04, 1.7515e-03, 1.0398e-03,  ..., 6.3325e-04,\n",
              "           3.2589e-04, 1.0292e-04],\n",
              "          ...,\n",
              "          [6.6048e-04, 2.9773e-04, 1.0485e-03,  ..., 1.0076e-03,\n",
              "           5.6110e-04, 8.1355e-05],\n",
              "          [6.9025e-07, 1.9696e-04, 4.8365e-03,  ..., 7.1271e-05,\n",
              "           2.9232e-04, 4.2799e-05],\n",
              "          [3.5711e-08, 3.5697e-11, 2.8337e-04,  ..., 3.4312e-06,\n",
              "           3.8386e-05, 1.2129e-05]],\n",
              "\n",
              "         ...,\n",
              "\n",
              "         [[0.0000e+00, 1.6480e-38, 2.1360e-38,  ..., 2.5764e-36,\n",
              "           3.0075e-40, 0.0000e+00],\n",
              "          [6.1065e-40, 4.5882e-21, 6.0802e-41,  ..., 7.1027e-30,\n",
              "           1.6672e-29, 3.8255e-43],\n",
              "          [1.1564e-32, 2.4594e-28, 1.0241e-11,  ..., 7.6887e-33,\n",
              "           1.0326e-19, 3.9531e-34],\n",
              "          ...,\n",
              "          [8.7754e-34, 1.3322e-22, 2.4171e-13,  ..., 1.0668e-12,\n",
              "           4.2955e-27, 5.6050e-35],\n",
              "          [0.0000e+00, 2.7472e-26, 5.6897e-23,  ..., 1.0268e-25,\n",
              "           6.7614e-40, 5.7714e-31],\n",
              "          [0.0000e+00, 0.0000e+00, 2.7484e-20,  ..., 0.0000e+00,\n",
              "           6.9942e-40, 0.0000e+00]],\n",
              "\n",
              "         [[0.0000e+00, 1.8244e-39, 2.9932e-42,  ..., 2.2438e-35,\n",
              "           3.0824e-35, 0.0000e+00],\n",
              "          [2.5170e-38, 7.3940e-24, 1.0065e-40,  ..., 2.3797e-29,\n",
              "           3.7374e-12, 1.5829e-33],\n",
              "          [3.3340e-34, 8.6111e-13, 1.0169e-16,  ..., 1.4806e-26,\n",
              "           4.6021e-14, 1.4052e-25],\n",
              "          ...,\n",
              "          [2.9139e-31, 2.4531e-29, 1.3905e-26,  ..., 7.6523e-17,\n",
              "           3.1328e-08, 4.4896e-29],\n",
              "          [0.0000e+00, 2.0666e-17, 5.7392e-24,  ..., 7.5014e-33,\n",
              "           1.0066e-26, 7.8479e-25],\n",
              "          [0.0000e+00, 0.0000e+00, 7.0093e-25,  ..., 0.0000e+00,\n",
              "           7.2050e-32, 0.0000e+00]],\n",
              "\n",
              "         [[4.6748e-38, 4.1746e-37, 1.9570e-34,  ..., 1.0650e-43,\n",
              "           3.6113e-19, 0.0000e+00],\n",
              "          [4.4809e-39, 6.0057e-20, 6.5124e-38,  ..., 4.7021e-30,\n",
              "           2.3779e-23, 3.1966e-36],\n",
              "          [5.9553e-36, 2.3029e-19, 1.6143e-22,  ..., 1.5278e-23,\n",
              "           7.7198e-17, 4.5927e-33],\n",
              "          ...,\n",
              "          [5.1274e-41, 6.3937e-31, 3.4879e-20,  ..., 2.9056e-11,\n",
              "           2.6054e-16, 6.4635e-24],\n",
              "          [0.0000e+00, 1.1896e-28, 2.3844e-27,  ..., 7.0206e-29,\n",
              "           1.7183e-30, 9.9296e-42],\n",
              "          [0.0000e+00, 0.0000e+00, 4.4483e-33,  ..., 0.0000e+00,\n",
              "           1.2920e-42, 0.0000e+00]]]], grad_fn=<CatBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "backbone = torch.nn.Sequential(*req_layers)\n",
        "for param in backbone.named_parameters():\n",
        "  param[1].requres_grad = True\n",
        "z = backbone(x)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "UX7-ezAPiM3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape, y.shape, dd[1].shape\n",
        "\n",
        "\n",
        "# for name, param in y.named_parameters():\n",
        "#     print(name, param.data)\n",
        "#     if param.requires_grad:\n",
        "#         print(name, param.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnActvQcjoJZ",
        "outputId": "e5327d88-972f-4f63-b0d8-3b71b641efaf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 2048, 10, 10]),\n",
              " torch.Size([10, 2048, 20, 20]),\n",
              " torch.Size([5, 512, 10, 10]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    print(out.shape)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        print(1212121212121212121212)        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        print(999990909090)\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        print(4444444444444444444444)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        print(333333333333333333333)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ASeYFJo4s8kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a85f31c-3056-40fb-dd19-25e5ed1ee8a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1212121212121212121212\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "MFnKE3neESZ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "d2699f59-5054-475b-fc93-a7e5aa4f15d7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-db6e86c9f4aa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mod_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-ace30a43776c>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, learning_rate, train_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapturable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         differentiable=differentiable, fused=fused)\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"optimizer got an empty parameter list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mparam_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: optimizer got an empty parameter list"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 10\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in detector.named_parameters():\n",
        "    print(name, param.data)\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "id": "fFRW4iC1uXf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2nsbro-Tse"
      },
      "outputs": [],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dOW6SREUn2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}