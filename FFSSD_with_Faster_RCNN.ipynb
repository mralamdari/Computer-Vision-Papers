{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7731492-b91b-4e49-c80b-23561e8b9af0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da522efe-88ac-4ff0-9f37-0ce2fee627d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87qVg5W5OQz6",
        "outputId": "20c2cd9b-1f11-4f06-8463-c5f4204abf87"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:44<00:00,  2.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b8d8d9-8273-4d9e-bf4b-60f8739654a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.659668922424316,\n",
              " 4.641712188720703,\n",
              " 4.6340556144714355,\n",
              " 4.712459087371826,\n",
              " 4.613870143890381,\n",
              " 4.614302158355713,\n",
              " 4.608339786529541,\n",
              " 4.621920108795166,\n",
              " 4.640611171722412,\n",
              " 4.574926376342773,\n",
              " 4.66522216796875,\n",
              " 4.744327545166016,\n",
              " 4.67869234085083,\n",
              " 4.734846115112305,\n",
              " 4.590364456176758,\n",
              " 4.616464614868164,\n",
              " 4.615306377410889,\n",
              " 4.584320545196533,\n",
              " 4.716510772705078,\n",
              " 4.716322422027588]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "fbd1228d-4ad4-44de-accf-c94af4f887f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc014b69990>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAEUCAYAAAB+lmCLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFxUlEQVR4nO29e3xU5bX//5n7JJNJQia3ScwFUC4agiZIjFiFJgpIrVSORzQeFCme0qEGOD1y0ooIhx+hpdoWQcGcoPZbaCzUWi8cEJFAKRfT5FBAbSABkkBuJGFmMrnMdf/+mNl7Zidzz4TZe/K8X6/9IrP3M3ueyZA161nPZ60loCiKAoFAIPAEYbgnQCAQCIFAjBaBQOAVxGgRCAReQYwWgUDgFcRoEQgEXkGMFoFA4BXEaBEIBF5BjBaBQOAVxGgRCAReIQ73BG4VNpsNra2tUCqVEAgE4Z4OgUAYAkVR6O3tRVpaGoRCL/4UNQLKy8spAFRpaanHMQ899BAFYNjx6KOPMmOee+65Ydfnzp3Luk93dzf1zDPPUEqlkoqLi6NeeOEFqre31++5trS0uJ0HOchBDm4dLS0tXv+Wg/a0ampqsGvXLuTm5nod9+GHH8JkMjGPu7u7MX36dDz55JOscfPmzcO7777LPJbJZKzrJSUlaGtrw+HDh2E2m7F06VK8+OKL2Lt3r1/zVSqVAICWlhbExsb69RwCgXDr0Ov1yMjIYP5WPRGU0TIYDCgpKUFFRQU2bdrkdWxCQgLrcVVVFaKjo4cZLZlMhtTUVLf3+Pbbb3Hw4EHU1NRgxowZAIA333wTjz76KH71q18hLS3N55zpJWFsbCwxWgQCh/EVvgkqEK/RaLBgwQIUFxcH/NzKykosXrwYCoWCdb66uhrJycmYPHkyVqxYge7ububaqVOnEB8fzxgsACguLoZQKMSZM2fcvo7RaIRer2cdBAKB/wTsaVVVVaGurg41NTUBv9hXX32FCxcuoLKyknV+3rx5eOKJJzB+/Hg0NjbiZz/7GebPn49Tp05BJBKhvb0dycnJ7ImLxUhISEB7e7vb1yovL8eGDRsCniOBQOA2ARmtlpYWlJaW4vDhw5DL5QG/WGVlJaZNm4aZM2eyzi9evJj5edq0acjNzcXEiRNRXV2NoqKigF8HAMrKyrBmzRrmMb1eJhAI/Cag5WFtbS06OzuRl5cHsVgMsViMY8eOYdu2bRCLxbBarR6f29fXh6qqKixbtszn60yYMAGJiYloaGgAAKSmpqKzs5M1xmKxoKenx2McTCaTMfErEsciECKHgDytoqIinD9/nnVu6dKlmDJlCtauXQuRSOTxufv27YPRaMSzzz7r83WuXbuG7u5uqNVqAEBhYSG0Wi1qa2uRn58PAPjyyy9hs9lQUFAQyFsgEAg8JyCjpVQqkZOTwzqnUCigUqmY80uWLEF6ejrKy8tZ4yorK7Fw4UKoVCrWeYPBgA0bNmDRokVITU1FY2MjXn75Zdx+++2YO3cuAGDq1KmYN28eli9fjp07d8JsNmPlypVYvHixXzuHBAIhcgi5Ir65uXmYmrW+vh4nTpzA559/Pmy8SCTCuXPn8P7770Or1SItLQ2PPPII/vu//5ul1dqzZw9WrlyJoqIiCIVCLFq0CNu2bQv19AljCP2gGdo+MzJV0eGeCiEABBQ1Nhpb6PV6xMXFQafTkfgWAQDwxFt/w7lrOhx/eQ7S4qPCPZ0xj79/oyRhmjAmoSgKF1r1sNgoXLiuC/d0CAFAjBZhTNLdZ4LJYgMANPf0h3k2hEAgRoswJmnTDjI/N3UTo8UniNEijEladQPMz03E0+IVxGgRxiStWqfRau7uC+NMCIFCjBZhTNKmcy4Pr90cgMVqC+NsCIFAjBZhTOLqaVlsFMuIEbgNMVqEMclQI0WC8fyBGC3CmKTN4WklKKQAgKYeEtfiC8RoEcYcVhuFjl4jAOC+CfbKus3E0+INxGgRxhydvYOw2iiIhQLkZY4DENrlYbtuEP/v1FX0GS0huyfByZhpIUYg0LQ6hKUpsXKMT7SX/Q6lVuv1z+uxr/YaIBDg3+7LCtl9CXaIp0UYc7Q5hKVp8XJkOSo8NHf3IVS1A75ps/cj6NSTHcnRgBgtwpiDTuFRx0XhtnHREAiAPpMV3X0mH8/0jc1GofGGAQCgHzCP+H6E4RCjRRhz0Ck86ng55BIRUmPt/Q5CEde6rh3AoNkuVNUPkpjWaECMFmHMQXtaaXH2GlqZCY4lYghkDw0OLwsgntZoQYwWYcxBx7TUcXYPi45rhcLTaux0MVqDxGiNBsRoEcYcrQ41PF2tNEtl30EMhVarwcVo9ZLl4ahAjBZhTGGy2NBlsAtLaU+LXh6GQvbgarTI8nB0IEaLMKbo0A+CogCZWMik8IRqeUhRFDumRTytUWFERmvLli0QCARYtWqVxzGzZ8+GQCAYdixYsAAAYDabsXbtWkybNg0KhQJpaWlYsmQJWltbWffJzs4edo8tW7aMZPqEMQhd3UEdJ4dAIAAAZCXYl4ddBuOIVOzdfSZo+53elcFoISVvRoGgFfE1NTXYtWsXcnNzvY778MMPYTI59S/d3d2YPn06nnzySQBAf38/6urqsG7dOkyfPh03b95EaWkpvv/97+Pvf/87614bN27E8uXLmcdKpTLY6RPGKHR1B3Wcs/tOXLQEcVES6AbMaO7px1R1cN2a6KWhOk7OvI7BaEF8tHSEsya4EpTRMhgMKCkpQUVFBTZt2uR1bEJCAutxVVUVoqOjGaMVFxeHw4cPs8Zs374dM2fORHNzMzIzM5nzSqUSqampwUyZQADA1mi5kqWKxrlrOjR1B2+0aFHp5FQltP1mDJit0A8QoxVqgloeajQaLFiwAMXFxQE/t7KyEosXL4ZCofA4RqfTQSAQID4+nnV+y5YtUKlUuOeee7B161ZYLJ5deaPRCL1ezzoIhKEaLZpQaLVoT+v2pBjERtn9ASJ7CD0Be1pVVVWoq6tDTU1NwC/21Vdf4cKFC6isrPQ4ZnBwEGvXrsXTTz/Natj40ksvIS8vDwkJCTh58iTKysrQ1taGN954w+19ysvLsWHDhoDnSIhs2rx4WsDIgvGM0UqOQaxcgg69kewgjgIBGa2WlhaUlpbi8OHDkMvlvp8whMrKSkybNg0zZ850e91sNuNf//VfQVEU3n77bda1NWvWMD/n5uZCKpXi3//931FeXg6ZTDbsXmVlZazn6PV6ZGRkBDxnQmTR6sHTooPxI+mB2OhqtKIkAIinNRoEtDysra1FZ2cn8vLyIBaLIRaLcezYMWzbtg1isRhWq9Xjc/v6+lBVVYVly5a5vU4brKamJhw+fNhn6/qCggJYLBZcvXrV7XWZTIbY2FjWQSB48rQyR+hp9RktjGh1YlIMYuX08pDIHkJNQJ5WUVERzp8/zzq3dOlSTJkyBWvXroVIJPL43H379sFoNOLZZ58ddo02WJcuXcLRo0ehUql8zuXs2bMQCoVITk4O5C0QxjADJituOiQJ6qGelsNoXdcOwGy1QSIKLNxLB+FVCinGKaROT4ssD0NOQEZLqVQiJyeHdU6hUEClUjHnlyxZgvT0dJSXl7PGVVZWYuHChcMMktlsxr/8y7+grq4On376KaxWK9rb2wHYdx6lUilOnTqFM2fOYM6cOVAqlTh16hRWr16NZ599FuPGjQv4TRPGJvTOoUIqYjwhmhSlHFKxECaLDa3aASa1x1/oeNbE5BgAQKycXh4STyvUhLxyaXNzM4RC9rdUfX09Tpw4gc8//3zY+OvXr+Pjjz8GANx9992sa0ePHsXs2bMhk8lQVVWF1157DUajEePHj8fq1atZMSsCwRdMHa34KEZYSiMUCpCZEI2GTgOauvuDNlq300aL3j0knlbIGbHRqq6u9voYACZPnuyxKmR2drbPipF5eXk4ffp0sFMkEAC4aLTi3G8iZdFGK4hgvKvcAXD1tIjRCjUk95AwZvCk0aLJdCm9HCh0zqHT06JjWmR5GGqI0SKMGTztHNJkJQS3g2iy2Jjn0EZLKSfi0tGCdOMhjBmYOloePC2mrlaAy8Pmnj5YbRQUUhGz9GSWhxEY0/rtF5dw7GInxCIhJCIBJCIhxEIhpGIBxEIhJI7zYsc1iUiI5+/PZuqXjRRitAhjBrqrtKc/HmZ52NMPiqKGBes94bpzSD+HXh5GWiHAQbMVv/7iYsDP+/70NGK0CIRAYSo8eFge3jYuCgIB0G+y4obBiGSlf1kfQ4PwAJzi0gjztHSO9yMSCvDbxXfDYqVgttpgtlKw2Oz/mq02WKwuP9soJCmHZ60ECzFahDGBftAMg6NWlqfloUwsQlpcFK5rB9Dc3R+w0aI1WoDT0zKYLLDZKAiF/nltXIc2WnFREnwvNy0scyCBeMKYgN45jI+WIErqOXMjM4hg/NCdQ8AZiKcooHcEhQW5hqvRChfEaHGUP/69Bf/1p3Ow2kLT9Xis49RoeY+rMNUe/AzG22wUGjvtEglXoyUTiyCX2P+8ImmJSFdmjSVGizCUXx6sR1VNC862aMM9lYjAqdHyvuQLVKvVqhvAgNkKsUNR70okCkxpTyueGC2CK4NmK9Mx5kavMcyziQx8abRosh2yB389LTqelZ2oGJZkHYkCU7I8JLil3bHLBQA3DMRohQK6jpav5SFTwdTPmJa7nUOa2AgUmOr67f0eiNEisKA7xgBAF/G0QgLtaaX58LTomFZ3n4nZbfRGo5sgPI0yAgWmzPIwmhgtggutLp5WF/G0QoK7LjzuUMolTD/EJj/iWkOrO7jirF4aOctDLVkeEtzR5uJpkZjWyKEoivFePWm0XAlkidh4Y/jOIU0kCkxJTIvgFnp7HiCeVii42W+G0WJvmpoS51uZ7a/soafPhJ4+e4xnQtLw+luRmMpDjBbBLXTQGAC6DCYvIwn+QHtZiTEyyMSehaU0/lZ7oJeG6fFRiJYOTy6JSMlDPzFaBDe06cjyMJTQ8SxfQXiaTKbag/eYlrd4FhCZ1UudgfjwNaAlRouDtLl4WgNmK/oiKA0kHLT5qFg6FH97IPo0WhHmaVEURQLxhOHoB81MrprUIVYkca2R4a9Gi4ZeHrZqB2ByxMLc4S7n0JVIE5f2maxMWhkxWgQG2suKi5Ig1eEZEKM1MvzVaNEkKWWIkohgo+wtxTzR6NPTiixxKb00lIqFTF5lOBjRK2/ZsgUCgQCrVq3yOGb27NkQCATDjgULFjBjKIrCq6++CrVajaioKBQXF+PSpUus+/T09KCkpASxsbGIj4/HsmXLYDAYRjJ9TuLafCExxh43IHGtkdEWoKclEAhcqj24j2v1GS2MQZvoRg0PIOJ6H2pd1PD+FkgcDYI2WjU1Ndi1axdyc3O9jvvwww/R1tbGHBcuXIBIJMKTTz7JjPnlL3+Jbdu2YefOnThz5gwUCgXmzp2LwUFnbKekpARff/01Dh8+jE8//RTHjx/Hiy++GOz0OQu905UeH8UUTrtBdhBHRGuAnhbArmLqjssOfVaCQsqIUYdCl6fpNdpravEdLiRLA0EaLYPBgJKSElRUVPhslpqQkIDU1FTmOHz4MKKjoxmjRVEUfvOb3+CVV17B448/jtzcXPzud79Da2srPvroIwDAt99+i4MHD+J//ud/UFBQgAceeABvvvkmqqqq0NraGsxb4CzO3nxyJMbYjRZJ5Qkeq41icjn99bQA37KHhhu9ANznHNLQgXiKshcD5Dt6DgThgSCNlkajwYIFC1BcXBzwcysrK7F48WIoFPZt5StXrqC9vZ11r7i4OBQUFODUqVMAgFOnTiE+Ph4zZsxgxhQXF0MoFOLMmTNuX8doNEKv17MOPuBa94k2WiRpOni6DEZYbBSEAiA5gJK/vnYQ3VUrHYpcIoJUHDk1tbQc0GgBQZRbrqqqQl1dHWpqagJ+sa+++goXLlxAZWUlc669vR0AkJKSwhqbkpLCXGtvb0dycjJ74mIxEhISmDFDKS8vx4YNGwKeY7hxXR7Su4jE0woe+veZEiuHWOT/d7QvrZa7wn/uiJVL0GUwRoQqnlHDhzFZGgjQ02ppaUFpaSn27NkDudz/+ABNZWUlpk2bhpkzZwb83EApKyuDTqdjjpaWllF/zVDgTOyVI4leHhJPK2hcf5+BQC8P6c48Q/Eld6CJJIEpFzRaQIBGq7a2Fp2dncjLy4NYLIZYLMaxY8ewbds2iMViWK1Wj8/t6+tDVVUVli1bxjqfmpoKAOjo6GCd7+joYK6lpqais7OTdd1isaCnp4cZMxSZTIbY2FjWwXUoinJRb0chSenYPSRGK2hoT0sdYPuq9HFREAkFGDTb0DnE0zVbbbja5b+nBURGpQcu5B0CARqtoqIinD9/HmfPnmWOGTNmoKSkBGfPnoVI5Dmva9++fTAajXj22WdZ58ePH4/U1FQcOXKEOafX63HmzBkUFhYCAAoLC6HValFbW8uM+fLLL2Gz2VBQUBDIW+A03X0mmCw2CAT25YwzEE92D4OF+RII0NOSiITMbuPQuFZTdz8sNgrRUpHP+0aS7IEru4cBxbSUSiVycnJY5xQKBVQqFXN+yZIlSE9PR3l5OWtcZWUlFi5cCJVKxTpP67w2bdqEO+64A+PHj8e6deuQlpaGhQsXAgCmTp2KefPmYfny5di5cyfMZjNWrlyJxYsXIy0tPG2MRgPaK0iKkUEqFjJGi07lUchIx7dAafOzoYU7shIUaOkZQFN3H2aOT2DOM0H4pBifeqVIEpgyydJhjmmF/K+gubkZQiHbgauvr8eJEyfw+eefu33Oyy+/jL6+Prz44ovQarV44IEHcPDgQVbcbM+ePVi5ciWKioogFAqxaNEibNu2LdTTDytMuoljKaOQiREtFdmbh/YaidEKAvp3GohGiyZTFQ00DNdqeatWOpRISuVxelrhS5YGQmC0qqurvT4GgMmTJ7sNZtIIBAJs3LgRGzdu9DgmISEBe/fuDXaavIBJN3FZciTGyNDc048ugxHZicNrNhG840zhCcbTci978JUo7YoykjytgfC3DwNI7iGnYKpruvyB0ap4soMYOGarM4ge1PLQQzFA5/LQ95dIbATViddyoKkFQIwWp2h1sz3P5B+SVJ6A6dAPgqLs1TJUHlJtvJGZ4NBqueQf2mxUcMtDnntaVhvF6AbD2dQCIEaLU7S58bQYVTwRmAYMvXOYGieHUBh4gi+df3iz38wYnTb9IPpN9uasWSp/PC1ap8XvmFbvoBl0hId4WgQGZ9B4uNEiy8PAYTRaAcodaGJkYsbTpZtc0EvDLFX0sOas7mDqxBv57WnR8axoqciv9z2aEKPFESxWGzp7h2uKmJgW8bQCxlWoGyyZQ4LxgQThAdeYFr89La5otABitDhDR68RNgqQiASMdwWAJE2PgLYReloAmCVgkyMHMZB4FgDERUXG7iGdLB3unUOAGC3OQP+BDY2/0Kk8ZHkYOMzGRgg8raHLw8A9LbNX2Q/X4UJnaRpitDjCda175XZSjKPkcq+J1//pw4E73VugDC1Rw5RYTlL69XzaM7FR9hrrfIUrydIAMVqcwVOOXKLD0xowW3n9nz4cBFpm2R1ZLhVMb/aZ0O1ozjox2T+hr0wsZBqU8FmrxZUCgAAxWpzBndwBAKKl9lQegATjA2HQbGUMTDApPDS0VqtVN4Bv2+yFJD01Z3WHQCCICFU8F/od0hCjxRGuaz3HX4gqPnDoEstREtGIvIPEGCmipSJQFFB98QYA79VK3REJ+YdcUcMDxGhxBm/xF6LVChymbHW8fESdY1w78xz51l7zzVtdeHc4Bab897TI7iGBoc1L8wXSSixw6HhW2gjiWTR0XKvR0YHH33gWTSQITIlOi8BiwGRFjyP+ku5meejUapH8Q39p041co0UzNF0ncE8rEpaHJBBPcIH+A4uWipia4q6QmFbghEKjRUMvD2n81WjRREKdeD3RaRFccW2+4C7+QpKmA4fZjQ2Jp+U0WuOiJVC5ZCz4g7NOPH+NFtFpEVi4q6PlCgnEB87QKrAjISvBuTwM1MsC+L97aLba0O/QCBKjRQDgUt3BQ9CYLA8DpzUEaniatHg5xI7UqqCMFs91WnQQXiAAlHJitAhwCRp7EEEmuSwPSSqPbwxGC9McNRSellgkRPo4+30mBhiEB/hfCJAOwitlYoiCqEsWaojR4gCtPkqo0Kk8g2YbSeXxAzqepZSLEROiZiAF4xMgEAD3TVD5HjwEJc8LAXKlszTNiIzWli1bmBZg3tBqtdBoNFCr1ZDJZJg0aRIOHDjAXM/OzoZAIBh2aDQaZszs2bOHXf/Rj340kulzBiam5WF5SFJ5AoP5EgiBRoum/Ilc1Py8GDnpcQE/l++BeD1HuvDQBP01VFNTg127diE3N9frOJPJhIcffhjJycnYv38/0tPT0dTUhPj4eNa9XLtTX7hwAQ8//DCefPJJ1r2WL1/O6tgTHc3eiuYjFEU56z55yZFLUsrQ1E268viDP7/PQBEJ2XXOAoHvDVu1A9xJ4QGCNFoGgwElJSWoqKjApk2bvI7dvXs3enp6cPLkSUgk9jednZ3NGpOUlMR6vGXLFkycOBEPPfQQ63x0dDRSU1ODmTJn0Q9amCWfN88gMcZutIjswTetXrILwgHtafUOWkBR1IjSisIBV5q00gS1PNRoNFiwYAGKi4t9jv34449RWFgIjUaDlJQU5OTkYPPmzSzPyhWTyYTf//73eOGFF4Z9uHv27EFiYiJycnJQVlaG/v5+t/cAAKPRCL1ezzq4CL00HBctQZRjCegOOpWH7CD6JpQarVBAi0stNgoDZv7FJLmk0QKC8LSqqqpQV1eHmpoav8ZfvnwZX375JUpKSnDgwAE0NDTgxz/+McxmM9avXz9s/EcffQStVovnn3+edf6ZZ55BVlYW0tLScO7cOaxduxb19fX48MMP3b5ueXk5NmzYEOjbu+X427adlj2QVB7ftIVQDR8KoiQiiIUCWGwU9AMWv8vacAUdn41WS0sLSktLcfjwYVbLem/YbDYkJyfjnXfegUgkQn5+Pq5fv46tW7e6NVqVlZWYP38+0tLSWOdffPFF5udp06ZBrVajqKgIjY2NmDhx4rD7lJWVYc2aNcxjvV6PjIwMf9/qLcPftu1EFe8/odRohQKBQIDYKAl6+kzQD5qRypF5+QuXkqWBAI1WbW0tOjs7kZeXx5yzWq04fvw4tm/fDqPRCJGIvcRRq9WQSCSs81OnTkV7eztMJhOkUueORFNTE7744guP3pMrBQUFAICGhga3Rksmk0EmCy5weivxt207UcX7h31jY+RdeEJNrFxsN1o8DMbrOJQsDQRotIqKinD+/HnWuaVLl2LKlClYu3btMIMFALNmzcLevXths9kgFNpDaBcvXoRarWYZLAB49913kZycjAULFvicy9mzZwHYjSKfafWzJDBRxfuHbsDMxI245NHwWWDK6+WhUqlETk4O65xCoYBKpWLOL1myBOnp6SgvLwcArFixAtu3b0dpaSl+8pOf4NKlS9i8eTNeeukl1n1sNhveffddPPfccxCL2dNqbGzE3r178eijj0KlUuHcuXNYvXo1HnzwQZ+SC67jzDsky8NQQH8JqBRSyCWeNzZuNXwWmGo5Ji4NeUSwubmZ8agAICMjA4cOHcLq1auRm5uL9PR0lJaWYu3ataznffHFF2hubsYLL7ww7J5SqRRffPEFfvOb36Cvrw8ZGRlYtGgRXnnllVBP/5bjb0PRJJflIR+3zW8VvlKiwgWfBaa89rTcUV1d7fUxABQWFuL06dNe7/PII494zKvLyMjAsWPHgp0iZ7HZKL+L1Q1N5QlVekqkwTWNFo1r/0M+QVEUp5paACT3MKx09RlhtlIQCICUWO9GK1oqhsKh4yJLRM9wTaNFQ2u16ERuvjBotsFksQHgjqdFjFYYoXe5UpRySES+P4pEEoz3Cdc0WjR8XR7SXpZIKGC+NMMNMVphpDXAHDlG9kA8LY8wv1POeVr8LAToqtHiShyVGK0wEmg1giSi1fKJvxsbtxqmTjzPPC0u9TukIUYrjLQF6BXQwXgS03IPRVFMk1bOeVo8DcRzqd8hDTFaYaTVTzU8DWkl5p2b/WaYrPagcbKSY0aLEZfya3mo5VAXHhpitMKIv3mHNEQV750OvVNYKhVz6782Xz0tPcc0WgAxWmHF3woPNEQV751Ox++FNu5cQunS3IJPdf65liwNEKMVNsxWG/NHFujykHha7qE9rWQfmrdwQC8PzVYKg2ZbmGfjP1zqLE1DjFaYaNcNgqIAqUgIlcI/pfHQVB4CG9oDTeGgp6WQikA3suHTDiIJxPMAiqJw5NsO/PXSjVF9HXprPjVODqGfbZlcU3kMRn4FdG8FTk+Le0aLrqkFAL08NFpcSeEBiNEaxp4zzVj2/t+x8ZNvYLWNnjfjrKPl/1LGNZWni+wgDqNT7/C0OLg8BJzBeB2PBKZcK7UMEKM1jO/fnYZYuRiXOg04cL5t1F7nuo+2YZ4gqTye6ex1eFocXB4C/BSYkt1DHhArl+CH35kAANh25NKoeVt03mGgJVSSSCqPRzocnhYXA/EAP2UPtCKe6LQ4zvOzskfd2wpU7kDjFJgSo+UKRVFMIJ6znpacXwJTiqKYuRJPi+PcCm/rusPTSg8wR44OxhNPi43WRQ3PRZ0W4LI85ImnZTBamP/7xGjxgNH2toKtsJkUYx9PUnnY0Jq3cdESyMTcKKEyFL6Vp6E1WjKxkFOlq4nR8sBoelsDJivzHyLg5SFJmnYLI3fgWM6hK0o5v8rTcK3MMg0xWl4YLW+LTpSOkYkRKw+sbDJRxbuH9rS4qNGi4dvuoY6DydIAMVpeGS1vy7VQXaCF1YjRcg8fPC16eciXkssR6Wlt2bIFAoEAq1at8jpOq9VCo9FArVZDJpNh0qRJOHDgAHP9tddeg0AgYB1Tpkxh3WNwcBAajQYqlQoxMTFYtGgROjo6RjJ9vxgNb8spdwi8UB29M3ajl6TyuMKk8HDa0+KX5CHijFZNTQ127drls++gyWTCww8/jKtXr2L//v2or69HRUUF0tPTWePuuusutLW1MceJEydY11evXo1PPvkE+/btw7Fjx9Da2oonnngi2On7zWh4W/TyMD2INle0p2W0kFQeV5yeFoeNlpxfy0NnsjR3UniAIFuIGQwGlJSUoKKiAps2bfI6dvfu3ejp6cHJkychkdgtdnZ29vCJiMVITU11ew+dTofKykrs3bsX3/3udwHYu1FPnToVp0+fxn333RfM2/Cb52dl43/+epnxth6bnjai+zmXh4F7WlFSERRSEfpMVnQZTExwd6zT2cvtFB6Af3XiI8rT0mg0WLBgAYqLi32O/fjjj1FYWAiNRoOUlBTk5ORg8+bNsFqtrHGXLl1CWloaJkyYgJKSEjQ3NzPXamtrYTabWa83ZcoUZGZm4tSpU25f12g0Qq/Xs45giZVLsOyB0HlbbSMsCUyKAQ6HSeHhw/KQJ55WxATiq6qqUFdXx7S998Xly5exf/9+WK1WHDhwAOvWrcPrr7/O8tAKCgrw3nvv4eDBg3j77bdx5coVfOc730Fvby8AoL29HVKpFPHx8ax7p6SkoL293e3rlpeXIy4ujjkyMjICfassQhnboj2tQIWlNKQYIBuKopwpPJwOxNsXNiaLDYNmq4/R4Uc3wL2mFkCARqulpQWlpaXYs2cP5HL//nPYbDYkJyfjnXfeQX5+Pp566in8/Oc/x86dO5kx8+fPx5NPPonc3FzMnTsXBw4cgFarxR//+MfA3o0LZWVl0Ol0zNHS0hL0vQD7B+fqbdmC9LYoimLKLAfbm4/sILLRD1iYhqJcVcMDgEIq5lVNrYhYHtbW1qKzsxN5eXkQi8UQi8U4duwYtm3bBrFYPGzJBwBqtRqTJk2CSORU1E6dOhXt7e0wmdyruuPj4zFp0iQ0NDQAAFJTU2EymaDValnjOjo6PMbBZDIZYmNjWcdIYXlbF4LztnQDZgw4vmWDXR6SVB42HY6lYVyUhFPK7aEIhQJeCUyZQDyfl4dFRUU4f/48zp49yxwzZsxASUkJzp49yzJMNLNmzUJDQwNsNmeJ2YsXL0KtVkMqdb8rYTAY0NjYCLVaDQDIz8+HRCLBkSNHmDH19fVobm5GYWFhIG9hRLh6W7/9Ijhvi/ayEhTSoP/AnKk8xGgBrnW0uOtl0Sh5tIMYEZ6WUqlETk4O61AoFFCpVMjJyQEALFmyBGVlZcxzVqxYgZ6eHpSWluLixYv47LPPsHnzZmg0GmbMT3/6Uxw7dgxXr17FyZMn8YMf/AAikQhPP/00ACAuLg7Lli3DmjVrcPToUdTW1mLp0qUoLCwc9Z3DoYzU2wqm+N9QnKk8JP8Q4IewlIZP5Wm4arSCkjx4o7m5GUKh0xZmZGTg0KFDWL16NXJzc5Geno7S0lKsXbuWGXPt2jU8/fTT6O7uRlJSEh544AGcPn0aSUlJzJhf//rXEAqFWLRoEYxGI+bOnYu33nor1NP3Ce1t/fqLi/jtF5fwaI7a73LJwMjkDjQkpsWGDyk8NHQqD9dV8VYbxcyRS514gBAYrerqaq+PAaCwsBCnT5/2eI+qqiqfryOXy7Fjxw7s2LEj0CmGnOdnZaPyxGXG2/perv+6rVa6bfsIOiAnKcnuoSu89LQ4vjx09QS51NQCILmHQTGS2FYbXWY5yJ1DgHTlGQofUnho+CIwpZeGMTIxJCJumQluzYZHBBvbGqncASCpPENx1oYnnlao4GJDCxpitILE1dv6xcF/oqWn36/n0XmHI1keRklFiJHZV/ZkieisDc8PT4sf1Uu52O+QhhitEbD0gWykxcnR0jOAH7z1N/xf802v4602iom/jGR5CACJMQ6tFg8rmA6arfjFwX/iaH3niO9FURRPPS1ue8hMQwtitCKLWLkEf/rx/ZiqjkWXwYTF75zGZ+c8LxW7DEaYrRSEgpFXI+DzDuLvTzfh7epG/Pcn34z4XvpBC9Nmnh+7h/yQPHCxdRgNMVojRB0Xhf0/KkTRlGQYLTZo9tZhx9EGtwFyWu6QEiuHeITBTb4arUGzFe8cvwwAaO7ph8Vq8/EM79xweFmxcjGn1fA0fBGXclWjBRCjFRIUMjHeWTIDS2dlAwC2HqrHy/vPMflwNCOt7uAKX2UP+2uvMboqi41ififBwvVeh0Phi7iUTuHhWoUHgBitkCESCrD+sbuw8fG7IBQA+2qvYcnuM0xsAHB6WiONZwH89LTMVht2HmtknWvq9m8DwxN0PIsPQXiAP+JSEogfQywpzEbl8/dCIRXh9OUePPHWSVzt6gPglDuExGjxMJXnk3+04trNAagUUtw/UQUAaOrpG9E9+VCSxhW+SR6IpzVGmDM5GftX3I+0ODkud/XhB2/9DTVXe1y6SodgecizTtM2G4W3qu1e1rLvjMekFCUAoHmknpaePyk8gNNzGTTbYLRwt6YWiWmNQaaqY/HRylmYflscbvabUVJxBqcvdwMIlaflWB7yJKZ16Ot2NHQaECsX49/uy0KWKhpA6JaHfPG0lDIx6AZMXF4ikt3DMUqyUo6qFwsxPycVJqsNNx3BzbQRJEvT8CmVh6IobD9qr432/P3ZUMolTqPlpyjXE3wqSwPYa2rRwmAuB+OZQDzHmloAxGiNOlFSEXY8k4cfPTQRgD1gn5EQukC80WJDL8dTeaov3sDXrXpES0VYOms8ACAzQQEAaO7uG5HR5ZunBfBDYMrl5WHIS9MQhiMUCvBf86egYHwCzFYb4qNH/u1Fp/IYjBZ09RqZPwSuQVEUdnxp97JKCjIxTmF/7xkJURAIgD6TFd19JsYIB3pvPqXw0MRGSXBdO8BZT8tosTLVdblotIindQuZMyUZj9zlvjx0MPAhlefMlR78vekmpCIh0z8SAGRiEdQObVWwcS2D0cL8cfHL0+K2wJT2sgQCpxiWSxCjxWP4oNXa4YhlPTnjtmE9CTMdca3mIGUPtJellIkRJeW+Gp6G63XiaQ8wVi4JqMDlrYIYLR7DdVX8P1q0+OulLoiEAiam50qWI64VrKfFh16H7mAqPXDU0+KyGh4gRovXcN3Tor2sx+9OQ0ZC9LDrjKcVrNHimbCUho4/9nLUaHE5CA8Qo8VruGy06tt78fk3HRAIgB/Pvt3tmJHKHviWwkPD9eqlxGgRRg0uLw/fqrZ7WfNzUnF7cozbMSNeHvIsWZqG64F4pt9hJBqtLVu2QCAQYNWqVV7HabVaaDQaqNVqyGQyTJo0CQcOHGCul5eX495774VSqURycjIWLlyI+vp61j1mz54NgUDAOn70ox+NZPq8h949vMGx3cOrXX345B+tADx7WYBzedhlMKIvCK1ZB92Fh8Ndpd3B9ZpaXPe0gt7PrKmpwa5du5Cbm+t1nMlkwsMPP4zk5GTs378f6enpaGpqQnx8PDPm2LFj0Gg0uPfee2GxWPCzn/0MjzzyCL755hsoFApm3PLly7Fx40bmcXT08DjJWIKrqTw7jzXCRgFzJichJz3O47i4KAnioyXQ9pvR3NOPqerAuoB30l14eOdpcVtcGpFGy2AwoKSkBBUVFdi0aZPXsbt370ZPTw9OnjwJicT+S8jOzmaNOXjwIOvxe++9h+TkZNTW1uLBBx9kzkdHRyM1NXQ6J77jmjRNURQEgvBvT7dqB/CnumsAgJXf9exl0WQlREPbr0NTdxBGi+7CwztPi9tpPDoOV3gAglweajQaLFiwAMXFxT7HfvzxxygsLIRGo0FKSgpycnKwefNmWK2eM9x1Oh0AICEhgXV+z549SExMRE5ODsrKytDf7zkWYjQaodfrWUekQce0TBxK5Xnn+GWYrRQKxicgPyvB5/hMlSOdJwitFv89LW4brYjxtKqqqlBXV4eamhq/xl++fBlffvklSkpKcODAATQ0NODHP/4xzGYz1q9fP2y8zWbDqlWrMGvWLOTk5DDnn3nmGWRlZSEtLQ3nzp3D2rVrUV9fjw8//NDt65aXl2PDhg2Bvj1eIZdwK5Wny2BEVU0zAP+8LMDuaQGBB+MNRgv6TLQanl+eVhzHdw/pwpVxHEyWBgI0Wi0tLSgtLcXhw4chl/v37Waz2ZCcnIx33nkHIpEI+fn5uH79OrZu3erWaGk0Gly4cAEnTpxgnX/xxReZn6dNmwa1Wo2ioiI0NjZi4sThwsWysjKsWbOGeazX65GRkeHvW+UNiTFSGIwW3Og1YkKS+126W0XliSsYNNsw/bY4PHB7ol/PcariAzNatJcVIxNDIeNeqok36NSYAbMVJosNUjG3NvEjytOqra1FZ2cn8vLymHNWqxXHjx/H9u3bYTQaIRKx0ynUajUkEgnr/NSpU9He3g6TyQSp1GnNV65ciU8//RTHjx/Hbbfd5nUuBQUFAICGhga3Rksmk0Em49c3cDAkKWW42t0f9vxDXb8Z/+9UEwBAM+d2v+NrwXpazoql/PuMY1yMbO+gGaogksVHE53DA4wIo1VUVITz58+zzi1duhRTpkzB2rVrhxksAJg1axb27t0Lm80GodD+jXLx4kWo1WrGYFEUhZ/85Cf485//jOrqaowfP97nXM6ePQvAbhTHMlwRmL5xuB4GowWTU5Qonpri9/OyHDGt69oBmK02v1uw8zWFBwDEIiGzrO8dtHDKaFEUBd2Ao+chRwPxARktpVLJijMBgEKhgEqlYs4vWbIE6enpKC8vBwCsWLEC27dvR2lpKX7yk5/g0qVL2Lx5M1566SXmHhqNBnv37sVf/vIXKJVKtLe3AwDi4uIQFRWFxsZG7N27F48++ihUKhXOnTuH1atX48EHH/QpuYh0uGC0Tl/uxvsOL+uV700NKMk2WSmDTCyE0WJDq3aAMWK+uNHLzxQemli53WhxLRg/YLbCbLXXN+OqpxXyxXRzczPa2pwNSzMyMnDo0CHU1NQgNzcXL730EkpLS/Ff//VfzJi3334bOp0Os2fPhlqtZo4PPvgAACCVSvHFF1/gkUcewZQpU/Af//EfWLRoET755JNQT593hFsVP2CyYu2fzgEAFt+bge/ckRTQ84VCATKDWCLSnbr5lsJDw9VUHloNLxYKEM3RyhkjjmBWV1d7fQwAhYWFOH36tMd7+KpcmZGRgWPHjgUzvYgn3J7W1kP1aOruhzpOjp8tmBrUPbJU0bjUaQgoB7GT954WN2UPrhotLuj+3MGtbQtCwIQzlefvV3vw7skrAIDyJ6YFLbmgK0C0BGC0OvT8jWkB3BWYcrnfIQ0xWjwnXKk8g2YrXt5/DhQF/Ev+bZg9OTnoezl3EP0XmBJPa3RwNrTgrtHil8CFMIxwpfL8+vBFXO7qQ7JShnUL7hzRvejgeyAxLb71OxxKMDGtG71GrPvoAkQiATLGRSMjIcrxbzTS46NCovficuswGmK0eM7QVJ5boYr/v+abqPjrZQDA5h9MQ9wIt8ZdBab+GN4+owUGR9rS0BLOfCGY8jTvnbyCg1+3u70mEACpsXJkjIvGbQ5jlpkQjdmTkwKSVHBdWAoQo8V7XFN5bvhI5aEoCkaLjXleMAyarfjP/edgo4Af3JOO4jv912R54rZx9s48/SYrugwmxhB7gl4aRjs6EvERZ514/4wWRVH45B/2XfnF92ZALhGhpacfLTf70dIzgAGzFW26QbTpBvHVVefz8rPG4U8r7vd7XlpGo8XNFB6AGK2IIEkpg8FoQfmBfyJaKkKf0YI+kwX9JisMRgv6jVbmsdVGQSQU4N/uy8LL8yYjWhrYf4FtRy6hodOAxBgZ1j82smUhjUwsQlpcFK5rB9Dc0+fbaDFyB356WYAzEO9vl+lz13Ro7ulHlESEVx+7k/W5URSFLoPJYcD6ce3mAFp6+lFV04Lappvo1A/6nVTOh0A8MVoRwG3jonClqw9ffNvh13irjcJ7J6/iaH0nfrkoFwUTVH4979w1LXYdty8LNy3MCem3cWZCNK5rB9DU3e+zOgRd/M+XceMygQbi6aKKxXemDPuiEQgESFLKkKSUIS9zHHP+mzY9zl3TofriDfzrDP/ybrletRQgRisiePV7d+Ljf7RCKhIiWiaGQiqCQiaGQiZCtFSMGJkY0cw5Mf5+tQdlH55HU3c/nnrnNJ6/P9un12W0WPGf+87BaqPwvVw15uWEtq5Zlioapy53+xWMjwxPy/9AvM1G4dNz9qXhY7n+p63NmZxsN1r1nX4bLUanRYwWYTS5I0WJ/3hkst/jZ09OxqHVD2LzZ9+iqqYF7528ii//2YlfLMpF4UT3XteOLxtQ39ELlUKKDd+/K1RTZwik2sMNnpZZdiUQT6vmag/a9YNQysV4aLL/GQdzpiTjt0cu4a8Xu/zO6+TD7iHRaY1RYuUSbFmUi9+9MBNpcXI09/Tj6YrTePUvF4bVa/+6VYe3qhsBABsfzxmVBF9nkwvfWi2+p/AAgYlLPzlnXxrOvSsVMrH/Gyi56XFQKaToNVpQ23TTr+doOV61FCBGa8zz4KQkHFr9IJ6emQkA+N2pJsz77XGcbOwCAJitNvznvnOw2CjMz0nFggCWJ4GQFYCnxXdhKeD0tPpMVlisNo/jLFYbDpy3yxwem54W0GsIhQI8NMnumR39Z6dfz+GD5IEYLQKUcgnKn5iG3y8rQHp8FFp6BvBMxRm88tF5vHH4Ir5p02NctAQbH8/xfbMgcXbmMTEaLE8wKTw8Xh7ShQAB7zuIJxu70dNnQoJCivs9LN29MXuKPVPhaL1vo2WzUWR5SOAXD9yRiEOrH0RJgd3r+v3pZrztWBa+9v27RnW3LlYuwTjHksRXx2nG0+JxIF4sEkLhqKLgLa5F7xrOz0n1u9aYKw/ekQihALjYYcC1m95/r71GC2yO2gVcljwQo0VgESMT4//7wTTs/WEBbhsXBQB4+M4UfD/ApUkw+NPkYsBkZTwTvqbw0DgFpu49LaPFyijgA10a0sRHS5GfZZdBVNff8DqW9rLkEmHQ4uNbATFaBLfcf3siDq16EBVLZuDNp++5JTmN/pRepiuWRklEUPJUDU/DBOM9eFrHL3ahd9CClFgZ7s323dnIE3Qye7WPJSIfNFoAMVoELyhkYjx8Z8ot+9alg/He6mp1uCRKc7Xek7/QwfheD0aLXhp+LzcNogCqwQ5ljsNo/a2hG4NmL637GI0Wd1N4AGK0CByCrmDqLaZFe1opPN45pPEmMO03WXD4G3uGQ7BLQ5qpaiVSYmUYMFvx1ZUej+P4sHMIEKNF4BBMiRovMS26JE0Sz+NZgPdKD1/+sxMDZisyEqIw/ba4Eb2OQCBgvK0vvUgf6GTpkVbtGG2I0SJwBnp52KodhNmDdqkjIj2t4UaLXho+lpsWkmWwP3Et4mkRCAGSrJRBLhHCaqNw/eaA2zE3eF78zxVnKg97eagfNOOoY6dvpEtDmgfuSIREJMDV7n5c6XLvyerGQiB+y5YtEAgEWLVqlddxWq0WGo0GarUaMpkMkyZNwoEDB1hjduzYgezsbMjlchQUFOCrr75iXR8cHIRGo4FKpUJMTAwWLVqEjg7/qhoQ+IFA4NKZx0Mwnva0+CwspfGUynP46w6YLDbcnhyDKanKkLxWjEyMmePtO5Ce1PF8SJYGRmC0ampqsGvXLp99B00mEx5++GFcvXoV+/fvR319PSoqKpCens6M+eCDD7BmzRqsX78edXV1mD59OubOnYvOTucvd/Xq1fjkk0+wb98+HDt2DK2trXjiiSeCnT6Bo2Q6chCbPeQg0jEtPld4oPGUNE3nGoZqaUhDx7U8qeOZ5WEkxrQMBgNKSkpQUVGBcePGeR27e/du9PT04KOPPsKsWbOQnZ2Nhx56CNOnT2fGvPHGG1i+fDmWLl2KO++8Ezt37kR0dDR2794NANDpdKisrMQbb7yB7373u8jPz8e7776LkydPem1NRuAfjOzBww5iJKTw0LjbPezpM+HEJXve5/emhzbPk45rnbncg37T8B3LiNZpaTQaLFiwAMXFxT7HfvzxxygsLIRGo0FKSgpycnKwefNmWK12vYjJZEJtbS3rXkKhEMXFxTh16hQAoLa2FmazmTVmypQpyMzMZMYMxWg0Qq/Xsw4C9/Gm1Ro0W5n4D59TeGiUbnYPD15oh8VG4a60WExMignp601MUiAjIQomqw0nG7qHXY/YQHxVVRXq6uqYtve+uHz5Mvbv3w+r1YoDBw5g3bp1eP3117Fp0yYAQFdXF6xWK1JS2LXGU1JS0N5uT2Fob2+HVCpFfHy8xzFDKS8vR1xcHHNkZPhXBI0QXrxpteiloUwsZOQCfMYpLnV6Pcyu4SikTbGkD26WiBFptFpaWlBaWoo9e/ZALvfvm85msyE5ORnvvPMO8vPz8dRTT+HnP/85du7cGdSE/aWsrAw6nY45WlpaRvX1CKEhi8k/7B/WeZwRlsbKea+GB4ZLHjr1gzh9xe4BLZg2OiWAaKNV/c/OYb9fZ3dpbiviA/q6qq2tRWdnJ/Ly8phzVqsVx48fx/bt22E0GiESsVM+1Go1JBIJ6/zUqVPR3t4Ok8mExMREiESiYTuBHR0dSE21l/RNTU2FyWSCVqtleVuuY4Yik8kgk/E/7jHWSI+PglAADJituNFrZC0DOyOgYqkrtLfYa7TAaqPw2fk2UBSQlxnPdN0ONYUTVZCJhWjVDeJihwGTHbuTZquNKQkUUZ5WUVERzp8/j7NnzzLHjBkzUFJSgrNnzw4zWAAwa9YsNDQ0wGZzigUvXrwItVoNqVQKqVSK/Px8HDlyhLlus9lw5MgRFBYWAgDy8/MhkUhYY+rr69Hc3MyMIUQGUrEQafH26hJD41odEVAb3hWlS7s3w6BlVJeGNHKJiKnL5bqL6Cq74PrSOyCjpVQqkZOTwzoUCgVUKhVycuwF4pYsWYKysjLmOStWrEBPTw9KS0tx8eJFfPbZZ9i8eTM0Gg0zZs2aNaioqMD777+Pb7/9FitWrEBfXx+WLl0KAIiLi8OyZcuwZs0aHD16FLW1tVi6dCkKCwtx3333heL3QOAQnnYQOyOgC48rUrEQUY5k9K/bdKhr1kIgGL2lIc0cujCgi16LXhoqZWKIg6jbdSsJuUltbm6GUOh80xkZGTh06BBWr16N3NxcpKeno7S0FGvXrmXGPPXUU7hx4wZeffVVtLe34+6778bBgwdZwflf//rXEAqFWLRoEYxGI+bOnYu33nor1NMncIDMBAX+hu5hWi1G7hABania2CgxBsxW/OEre8z1vvGqUd8ZnT0pGcDX+HvTTegHzYiVS3jR75BmxEarurra62MAKCws9KmnWrlyJVauXOnxulwux44dO7Bjx45gpkngEZ5kD3QXnkjIO6SJlUvQoTfi4AVHi7BbUmwxGhOTFGi80YcTl7rw6DQ109CC6/EsgOQeEjiIp2KAkelp2Y2E2UpBLBSEvJ+kJxh1vGOJqOdBFx4aYrQInMNTD0Q6phUpgXiAHfR+4I5EJChujdyAiWvV34DNRvFGowUQo0XgILRWq6fPxFT1HDRbmTSTSJE8AOwdxMdyR39pSHNvdgIUUhG6DEZ83apnfrfE0yIQgiBGJobK4XHQS0Q6niUVC3nhDfgLXelBKhbikbtSfIwOHVKxEA/ckQjALn3gUyCeGC0CJxm6RHQVlkaCGp5GpbB7jXMmJ7G8rluBa9UHviRLA6MgeSAQQkFWQjT+r1nLeFqdESYspXmmIBO6ATOWPTD+lr82XfXhbIsWQscXAdebWgDE0yJwlKE9ECMthYcmJVaO175/16il7XgjNU6OqepYUBRQ23QTAD88LWK0CJxkqOwhkupocYk5k5NYj4nRIhCCZGgqD+NpRdjyMNx81yF9oCG7hwRCkNCB+DbdAEwWG/G0Rom7M+JZ3hXxtAiEIEmKkSFaKoKNAq7d7Hem8BBPK6SIRUI8OMm5ROR6fXiAGC0CRxnamScSU3i4Ah3XEgqAGCn3BQXEaBE4C220GjsNuOnQEUVSsjRXmDM5GclKGfKzxkEo5L4GjvtmlTBmoYPx9Ha8VCTkRaCYb4xTSHH0p7MhE/PDhyFGi8BZaK1WzdUeAPbif5GkhucSChl/TAE/TCthTEJrtboMJgAknkWwQ4wWgbPQy0MaIncgAMRoEThMWnwURC6BYSJ3IADEaBE4jEQkRFq801ART4sAEKNF4DhZCQrmZ5LCQwBGaLS2bNkCgUCAVatWeRzz3nvvQSAQsI6h3amHXqePrVu3MmOys7OHXd+yZctIpk/gAZkucS3iaRGAEUgeampqsGvXLuTm5vocGxsbi/r6eubx0G3rtrY21uP//d//xbJly7Bo0SLW+Y0bN2L58uXMY6VSGczUCTwiy6VkC4lpEYAgjZbBYEBJSQkqKiqwadMmn+MFAoHH9vUAhl37y1/+gjlz5mDChAms80ql0ut9CJFHFvG0CEMIanmo0WiwYMECFBcX+zXeYDAgKysLGRkZePzxx/H11197HNvR0YHPPvsMy5YtG3Zty5YtUKlUuOeee7B161ZYLBaP9zEajdDr9ayDwD/oJhcSkQDjorlfVZMw+gTsaVVVVaGurg41NTV+jZ88eTJ2796N3Nxc6HQ6/OpXv8L999+Pr7/+Grfddtuw8e+//z6USiWeeOIJ1vmXXnoJeXl5SEhIwMmTJ1FWVoa2tja88cYbbl+3vLwcGzZsCPTtETjG5BQlFuXdhmxVNC/y4gi3ACoAmpubqeTkZOof//gHc+6hhx6iSktL/b6HyWSiJk6cSL3yyitur0+ePJlauXKlz/tUVlZSYrGYGhwcdHt9cHCQ0ul0zNHS0kIBoHQ6nd9zJRAItw6dTufX32hAnlZtbS06OzuRl5fHnLNarTh+/Di2b98Oo9EIkUjk9R4SiQT33HMPGhoahl3761//ivr6enzwwQc+51JQUACLxYKrV69i8uTJw67LZDLIZCQGQiBEGgEZraKiIpw/f551bunSpZgyZQrWrl3r02ABdiN3/vx5PProo8OuVVZWIj8/H9OnT/d5n7Nnz0IoFCI5OdnnWAKBEDkEZLSUSiVycnJY5xQKBVQqFXN+yZIlSE9PR3l5OQC7TOG+++7D7bffDq1Wi61bt6KpqQk//OEPWffR6/XYt28fXn/99WGve+rUKZw5cwZz5syBUqnEqVOnsHr1ajz77LMYN25cQG+YQCDwm5DXo2huboZQ6NyUvHnzJpYvX4729naMGzcO+fn5OHnyJO68807W86qqqkBRFJ5++ulh95TJZKiqqsJrr70Go9GI8ePHY/Xq1VizZk2op08gEDiOgKIoKtyTuBXo9XrExcVBp9MhNjY23NMhEAhD8PdvlOQeEggEXkGMFoFA4BX8qbE6QuhVMFHGEwjchP7b9BWxGjNGq7e3FwCQkZER5pkQCARv9Pb2Ii4uzuP1MROIt9lsaG1thVKp9NkcQa/XIyMjAy0tLREXtI/k9waQ98dnKIpCb28v0tLSWAqEoYwZT0soFLrNdfRGbGxsxP3HoInk9waQ98dXvHlYNCQQTyAQeAUxWgQCgVcQo+UGmUyG9evXR2TCdSS/N4C8v7HAmAnEEwiEyIB4WgQCgVcQo0UgEHgFMVoEAoFXEKNFIBB4BTFaBAKBVxCjNYQdO3YgOzsbcrkcBQUF+Oqrr8I9pZDw2muvDevQPWXKlHBPK2iOHz+Oxx57DGlpaRAIBPjoo49Y1ymKwquvvgq1Wo2oqCgUFxfj0qVL4ZlsgPh6b88///ywz3LevHnhmWwYIEbLhQ8++ABr1qzB+vXrUVdXh+nTp2Pu3Lno7OwM99RCwl133YW2tjbmOHHiRLinFDR9fX2YPn06duzY4fb6L3/5S2zbtg07d+7EmTNnoFAoMHfuXAwODt7imQaOr/cGAPPmzWN9ln/4wx9u4QzDzGi2BOIbM2fOpDQaDfPYarVSaWlpVHl5eRhnFRrWr19PTZ8+PdzTGBUAUH/+85+ZxzabjUpNTaW2bt3KnNNqtZRMJqP+8Ic/hGGGwTP0vVEURT333HPU448/Hpb5cAHiaTkwmUyora1ldc0WCoUoLi7GqVOnwjiz0HHp0iWkpaVhwoQJKCkpQXNzc7inNCpcuXIF7e3trM8yLi4OBQUFEfNZVldXIzk5GZMnT8aKFSvQ3d0d7indMojRctDV1QWr1YqUlBTW+ZSUFLS3t4dpVqGjoKAA7733Hg4ePIi3334bV65cwXe+8x2mzlgkQX9ekfpZzps3D7/73e9w5MgR/OIXv8CxY8cwf/58WK3WcE/tljBmStOMdebPn8/8nJubi4KCAmRlZeGPf/wjli1bFsaZEQJl8eLFzM/Tpk1Dbm4uJk6ciOrqahQVFYVxZrcG4mk5SExMhEgkQkdHB+t8R0cHUlNTwzSr0SM+Ph6TJk1y2+mb79Cf11j5LCdMmIDExMSI/CzdQYyWA6lUivz8fBw5coQ5Z7PZcOTIERQWFoZxZqODwWBAY2Mj1Gp1uKcScsaPH4/U1FTWZ6nX63HmzJmI/CyvXbuG7u7uiPws3UGWhy6sWbMGzz33HGbMmIGZM2fiN7/5Dfr6+rB06dJwT23E/PSnP8Vjjz2GrKwstLa2Yv369RCJRG6b4/IBg8HA8iyuXLmCs2fPIiEhAZmZmVi1ahU2bdqEO+64A+PHj8e6deuQlpaGhQsXhm/SfuLtvSUkJGDDhg1YtGgRUlNT0djYiJdffhm333475s6dG8ZZ30LCvX3JNd58800qMzOTkkql1MyZM6nTp0+He0oh4amnnqLUajUllUqp9PR06qmnnqIaGhrCPa2gOXr0KAVg2PHcc89RFGWXPaxbt45KSUmhZDIZVVRURNXX14d30n7i7b319/dTjzzyCJWUlERJJBIqKyuLWr58OdXe3h7uad8ySD0tAoHAK0hMi0Ag8ApitAgEAq8gRotAIPAKYrQIBAKvIEaLQCDwCmK0CAQCryBGi0Ag8ApitAgEAq8gRotAIPAKYrQIBAKvIEaLQCDwiv8fHnrOOJ8HN8gAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "OeaTeHRHhHeB",
        "outputId": "9de04ef6-fb47-45a6-c487-1b7e62bacfbc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9fd4e5a25a0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTwoStageDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mod_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-21e54a9e1038>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size, n_classes, roi_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegionProposalNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_rpn_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-92987c4a48a2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, img_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#feature_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c209c77d908b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg19\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'IMAGENET1K_V1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequres_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 14.75 GiB total capacity; 12.32 GiB already allocated; 20.81 MiB free; 13.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "# for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "#     img_data_all = img_batch\n",
        "#     gt_bboxes_all = gt_bboxes_batch\n",
        "#     gt_classes_all = gt_classes_batch\n",
        "#     break\n",
        "    \n",
        "# img_data_all = img_data_all[:2]\n",
        "# gt_bboxes_all = gt_bboxes_all[:2]\n",
        "# gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# detector.eval()\n",
        "# proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.5, nms_thresh=0.05)\n",
        "# proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "#   def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "#     super().__init__()\n",
        "#     self.n_anchors = n_anchors\n",
        "#     self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "#     self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "#     self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "#     self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "#   def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "#       mode = 'eval'\n",
        "#     else:\n",
        "#       mode = 'train'\n",
        "\n",
        "#     out = self.conv1(feature_map)\n",
        "#     out = self.droput(out)\n",
        "#     out = torch.nn.functional.relu(out)\n",
        "#     reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "#     conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "#     if mode=='train':\n",
        "#       #get confidence scores\n",
        "#       conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "#       conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "#       #get offsets for positive anchors\n",
        "#       offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "#       #generate proposals using offsets\n",
        "#       proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "#       return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "#     else:\n",
        "#       return conf_scores_pred.to(device), reg_offsets_pred.to(device)\n",
        "\n",
        "\n",
        "# class TwoStageDetector(torch.nn.Module):\n",
        "#     def __init__(self, img_size, n_classes, roi_size):\n",
        "#         super().__init__() \n",
        "#         self.rpn = RegionProposalNetwork(img_size)\n",
        "#     def forward(self, images, gt_bboxes, gt_classes):\n",
        "#         total_rpn_loss, feature_map, proposals, \\\n",
        "#         positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "#         out_c, out_h, out_w = out_size\n",
        "#         # get separate proposals for each sample\n",
        "#         pos_proposals_list = []\n",
        "#         batch_size = images.size(dim=0)\n",
        "#         for idx in range(batch_size):\n",
        "#             proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "#             proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "#             pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "#         classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "#         cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "#         total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "#         return total_loss\n",
        "    \n",
        "#     def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "#         batch_size = images.size(dim=0)\n",
        "#         proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "#         out_c, out_h, out_w = out_size\n",
        "#         print(4444444444444444444, proposals_final)\n",
        "#         classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "#         cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "#         # convert scores into probability\n",
        "#         cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "#         # get classes with highest probability\n",
        "#         classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "#         classes_final = []\n",
        "#         # slice classes to map to their corresponding image\n",
        "#         c = 0\n",
        "#         for i in range(batch_size):\n",
        "#             n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "#             classes_final.append(classes_all[c: c+n_proposals])\n",
        "#             c += n_proposals\n",
        "            \n",
        "#         return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "\n",
        "\n",
        "# class RegionProposalNetwork(torch.nn.Module):\n",
        "#     def __init__(self, img_size):\n",
        "#         super().__init__()\n",
        "        \n",
        "#         self.img_height, self.img_width = img_size\n",
        "#         # scales and ratios for anchor boxes\n",
        "#         self.anc_scales = [2, 4, 6]\n",
        "#         self.anc_ratios = [0.5, 1, 1.5]\n",
        "#         self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "#         # IoU thresholds for +ve and -ve anchors\n",
        "#         self.pos_thresh = 0.7\n",
        "#         self.neg_thresh = 0.3\n",
        "        \n",
        "#         # weights for loss\n",
        "#         self.w_conf = 1\n",
        "#         self.w_reg = 5\n",
        "        \n",
        "#         self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "#     def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "#         batch_size = images.size(dim=0)\n",
        "#         block, feature_map = self.feature_extractor(images)\n",
        "#         out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "#         # downsampling scale factor \n",
        "#         width_scale_factor = self.img_width // out_w\n",
        "#         height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "#         # generate anchors\n",
        "#         anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "#         anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "#         anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "#         # get positive and negative anchors amongst other things\n",
        "#         gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "#         positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "#         GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "#         negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "#         # pass through the proposal module\n",
        "#         proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "#         conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "#         cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "#         reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "#         total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "#         device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#         return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "#     def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "#         with torch.no_grad():\n",
        "\n",
        "#             batch_size = images.size(dim=0)\n",
        "#             block, feature_map = self.feature_extractor(images)\n",
        "#             out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "#             # downsampling scale factor \n",
        "#             width_scale_factor = self.img_width // out_w\n",
        "#             height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "#             # generate anchors\n",
        "#             anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "#             anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "#             anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "#             anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "#             # get conf scores and offsets\n",
        "#             proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "#             conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "#             conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "#             offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "#             # filter out proposals based on conf threshold and nms threshold for each image\n",
        "#             proposals_final = []\n",
        "#             conf_scores_final = []\n",
        "#             for i in range(batch_size):\n",
        "#                 conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "#                 offsets = offsets_pred[i]\n",
        "#                 anc_boxes = anc_boxes_flat[i]\n",
        "#                 proposals = generate_proposals(anc_boxes, offsets)\n",
        "#                 # filter based on confidence threshold\n",
        "#                 conf_idx = torch.where(conf_scores >= 0.99)[0]\n",
        "#                 conf_scores_pos = conf_scores[conf_idx]\n",
        "#                 proposals_pos = proposals[conf_idx]\n",
        "#                 print(5252, proposals_pos)\n",
        "#                 # filter based on nms threshold\n",
        "#                 nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "#                 print(12121212, nms_idx)\n",
        "#                 conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "#                 proposals_pos = proposals_pos[nms_idx]\n",
        "#                 proposals_final.append(proposals_pos)\n",
        "#                 conf_scores_final.append(conf_scores_pos)\n",
        "#         # print(23232323, proposals_final, conf_scores_final, feature_map.shape)\n",
        "#         return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "# def generate_proposals(anchors, offsets):\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#     # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "#     anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "#     # apply offsets to anchors to create proposals\n",
        "#     proposals_ = torch.zeros_like(anchors)\n",
        "#     # print(anchors.shape, offsets[:, 0].shape)\n",
        "#     proposals_[:,0] = anchors[:,0] + offsets[:,0][: 10800]*anchors[:,2]\n",
        "#     proposals_[:,1] = anchors[:,1] + offsets[:,1][: 10800]*anchors[:,3]\n",
        "#     proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2][: 10800])\n",
        "#     proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3][: 10800])\n",
        "\n",
        "#     # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "#     proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "#     return proposals.to(device)\n",
        "\n",
        "\n",
        "# detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "# detector.to(device)\n",
        "# clear_output()\n",
        "# learning_rate = 1e-1\n",
        "# n_epochs = 1\n",
        "# loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)\n",
        "\n",
        "# detector.eval()\n",
        "# proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.5, nms_thresh=0.05)\n",
        "# proposals_final, conf_scores_final, classes_final"
      ],
      "metadata": {
        "id": "_X2S_JjJMARK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "# feature_extractor = FeatureExtractor() \n",
        "# out = feature_extractor(img_data_all)\n",
        "# out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "# out_c, out_h, out_w\n",
        "\n",
        "# width_scale_factor = img_width // out_w\n",
        "# height_scale_factor = img_height // out_h \n",
        "# prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# # prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# # get classes\n",
        "# classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "# classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "# nrows, ncols = (1, 2)\n",
        "# fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "# fig, axes = display_img(img_batch, fig, axes)\n",
        "# fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "# fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FF2pFCXWgHC4",
        "CVyQczF6VjNJ"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}