{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc35a520-f81b-4ff3-8d10-a87267f784ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efcd3ee7-c944-4bbd-9198-f7d27eddaf69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "# def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "#     model.train()\n",
        "#     loss_list = []\n",
        "    \n",
        "#     for i in tqdm.tqdm(range(n_epochs)):\n",
        "#         total_loss = 0\n",
        "#         counter = 1\n",
        "#         for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "#             # forward pass\n",
        "#             loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "#             if not np.isnan(loss.item()):\n",
        "#               total_loss += loss.item()\n",
        "\n",
        "#             # backpropagation\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             # break \n",
        "#             counter += 1\n",
        "        \n",
        "#         loss_list.append(total_loss/counter)\n",
        "        \n",
        "#     return loss_list\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def training_loop(model, learning_rate, train_dataloader, n_epochs, steps=1):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if counter == steps:\n",
        "              break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20"
      ],
      "metadata": {
        "id": "87qVg5W5OQz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277ed38b-fa10-4aeb-b097-3d809446a495"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:04<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation"
      ],
      "metadata": {
        "id": "SIPiAJUAXAse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=1)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "rckt7ValUh6_",
        "outputId": "37524db5-2207-4952-c0fc-911bfb49a7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|██████████| 20/20 [00:51<00:00,  2.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.5087099075317383, 1.4882389307022095, 1.4786343574523926, 1.489471197128296, 1.5656864643096924, 1.4998530149459839, 1.4959864616394043, 1.4910469055175781, 1.5144405364990234, 1.518667459487915, 1.49863862991333, 1.4944767951965332, 1.4972695112228394, 1.4877077341079712, 1.4773961305618286, 1.4722511768341064, 1.4388813972473145, 1.5008729696273804, 1.4926836490631104, 1.4464439153671265]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "98dcb357-413e-4090-cf5c-80ff77217f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|██████████| 10/10 [00:56<00:00,  5.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.127230584621429, 4.129290282726288, 4.119967818260193, 4.217435657978058, 4.246798038482666, 4.104306519031525, 4.048725605010986, 4.149570167064667, 4.287768304347992, 4.056561350822449]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs=10, steps=2)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "d66e819c-2940-47a1-e719-428e24473195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [00:12<04:05, 12.94s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-4a38214e8fc1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mod_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-61e0a849872b>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, learning_rate, train_dataloader, n_epochs, steps)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 14.75 GiB total capacity; 12.39 GiB already allocated; 18.81 MiB free; 13.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=3)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=4)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=5)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6f_i-NxdGUov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=6)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XYcFFSIwUxqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=7)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ft8tNtlWUxjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=8)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5sVS2QlKUxhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=9)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EVOnLuw8Uxee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs, steps=10)\n",
        "\n",
        "print(loss_list)\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7FCL4eHLUxb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJ-VAHKcVbC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FF2pFCXWgHC4",
        "CVyQczF6VjNJ",
        "2VF-O12KLyIk"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}