{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2745796e-ab78-4770-dc38-6703509fefd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d445b59-b714-4e14-970f-000e9c6ea6ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 10\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ],
      "metadata": {
        "id": "87qVg5W5OQz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d340de7-c756-42cd-e721-955738288bee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.870929718017578,\n",
              " 2.7794172763824463,\n",
              " 3.095012664794922,\n",
              " 2.7614693641662598,\n",
              " 3.039994955062866,\n",
              " 3.123002052307129,\n",
              " 2.9188485145568848,\n",
              " 3.033865451812744,\n",
              " 2.9790866374969482,\n",
              " 2.813352584838867]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "8e09af18-4557-4bcd-c61e-57aaa5b1d457"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f70a6e82c80>]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6f_i-NxdGUov"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}