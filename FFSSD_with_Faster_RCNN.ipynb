{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "390176d2-e81a-4f11-cfc1-388aa98e95a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# !pip install torch_intermediate_layer_getter\n",
        "# from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFv4RLO6zEAX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Part"
      ],
      "metadata": {
        "id": "9zLoTK3ndnQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6Zvz3aj_L9",
        "outputId": "25a5c1f4-c878-4529-c9c5-1f227ca6b368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# x = x.to(device)\n",
        "# fe = FeatureExtractor()\n",
        "# block, res = fe(x)    \n",
        "# res.shape    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "270f59ea-68a2-4f05-a888-f1775729783a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 90.2MB/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|██████████| 20/20 [00:53<00:00,  2.69s/it]\n"
          ]
        }
      ],
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2562878d-a479-48d4-9117-832ab033ef76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.643418788909912,\n",
              " 4.634223937988281,\n",
              " 4.576550006866455,\n",
              " 4.623342514038086,\n",
              " 4.581556797027588,\n",
              " 4.661684036254883,\n",
              " 4.666585922241211,\n",
              " 4.617688179016113,\n",
              " 4.640999794006348,\n",
              " 4.656900405883789,\n",
              " 4.654881954193115,\n",
              " 4.582062721252441,\n",
              " 4.67889928817749,\n",
              " 4.594639301300049,\n",
              " 4.658738613128662,\n",
              " 4.68831729888916,\n",
              " 4.568222999572754,\n",
              " 4.579838275909424,\n",
              " 4.6679158210754395,\n",
              " 4.6847004890441895]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "8747c99b-39d7-4820-9ed3-be79b3741399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc003706740>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "b141c4d8-ac11-41e6-effd-023923617840"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-3260ab0b1a5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-21e54a9e1038>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-92987c4a48a2>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0manc_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc_boxes_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# filter based on confidence threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mconf_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_scores\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-74910fceec3f>\u001b[0m in \u001b[0;36mgenerate_proposals\u001b[0;34m(anchors, offsets)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# apply offsets to anchors to create proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mproposals_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (21600) must match the size of tensor b (10800) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "\n",
        "\n",
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        print(6767676767, anc_boxes_all.shape, gt_bboxes_proj.shape, gt_classes.shape)\n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "def generate_proposals(anchors, offsets):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    print(anchors.shape, offsets[:, 0].shape)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:, 0:10800, 0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:, 0:10800, 1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:, 0:10800, 2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:, 0:10800, 3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)\n",
        "\n",
        "\n",
        "# detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "# detector.to(device)\n",
        "# clear_output()\n",
        "# learning_rate = 1e-1\n",
        "# n_epochs = 20\n",
        "# loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)\n",
        "\n",
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "_X2S_JjJMARK",
        "outputId": "4058e0b9-7eb9-475b-aa49-e49f1afc39c2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d2bcaf90547e>\u001b[0m in \u001b[0;36m<cell line: 213>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-a8bb95f61039>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-a8bb95f61039>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-dff34ae985ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-dff34ae985ab>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(block, model, images, layers_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mrfem21\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem21_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv4_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mrfem22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem21_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv5_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mrfem23\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem21_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv7_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mrfem24\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfem21_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv6_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.75 GiB total capacity; 13.38 GiB already allocated; 12.81 MiB free; 13.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}