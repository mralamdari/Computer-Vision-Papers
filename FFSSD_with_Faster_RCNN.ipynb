{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "1eeb1c5c-107f-4843-fc3f-5d19c97d02c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# !pip install torch_intermediate_layer_getter\n",
        "# from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFv4RLO6zEAX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Part"
      ],
      "metadata": {
        "id": "9zLoTK3ndnQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6Zvz3aj_L9",
        "outputId": "99001d70-f9ca-4a83-d254-f80bee3a8ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# x = x.to(device)\n",
        "# fe = FeatureExtractor()\n",
        "# block, res = fe(x)    \n",
        "# res.shape    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "560a5219-e979-42ce-b818-0e706c766dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|| 548M/548M [00:07<00:00, 76.7MB/s]\n",
            "  0%|          | 0/20 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:459: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1003.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "100%|| 20/20 [00:51<00:00,  2.58s/it]\n"
          ]
        }
      ],
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ba549f-09b9-4f61-ee7e-50d93c2c2000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.7026591300964355,\n",
              " 4.587793827056885,\n",
              " 4.583062171936035,\n",
              " 4.674246788024902,\n",
              " 4.606556415557861,\n",
              " 4.616434574127197,\n",
              " 4.600011348724365,\n",
              " 4.555309772491455,\n",
              " 4.603033065795898,\n",
              " 4.674356460571289,\n",
              " 4.714174270629883,\n",
              " 4.688354015350342,\n",
              " 4.577860355377197,\n",
              " 4.557159423828125,\n",
              " 4.623183250427246,\n",
              " 4.5909528732299805,\n",
              " 4.650949478149414,\n",
              " 4.616343975067139,\n",
              " 4.657010078430176,\n",
              " 4.631278038024902]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "a026b57c-7939-4e74-8b28-86e45ec61b63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff9a1835180>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAESCAYAAACoz4OWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABHEUlEQVR4nO2de1xT9/3/X7mHS0K4CASM4qV4Q6xipWjX2oJa6/ZrV79dL+xr55zdXGxRt1+dv611bf2K3/rt1q+t32qdtt2vWrr663pZnc5apXXeGAxvsyiogEIAwRDCJSHJ+f0RzkkCCclJTi4nfJ6Px3k8IPnk5HNIeJ335/15XwQURVEgEAgEniAM9wQIBAKBDUS0CAQCryCiRSAQeAURLQKBwCuIaBEIBF5BRItAIPAKIloEAoFXiMM9gVBhs9nQ1NQEhUIBgUAQ7ukQCIRBUBSFrq4uZGRkQCj0bE+NGNFqamqCRqMJ9zQIBIIXGhsbMXr0aI/PjxjRUigUAOx/EKVSGebZEAiEwRgMBmg0GuZ/1RMjRrToJaFSqSSiRSBEMN7cN8QRTyAQeAURLQKBwCuIaBEIBF5BRItAIPAKIloEAoFXENEiEAi8gogWIaro6Dajtasv3NMgBBEiWoSowWK14XtvHMeDr3+Dzt7+cE+HECSIaBGihsstRtzU96Kj24wjl1rCPR1CkCCiRYgazt7QMz8fOK8L30QIQYWIFiFqqG7QMz9/faUNRpMlfJMhBA0iWoSogba0BALAbLHh6Let4Z0QISgQ0SJEBd0mCy63dAEAHp1pL2vy1wvN4ZwSIUgQ0SJEBRdudsJGAeoEOX40NwsAcPTbNvSareGdGIFziGgRogJ6aThjtAo5mUqMToxBb78V5ZfJEjHaIKJFiArONnYCAGZoVBAIBFickw4A+OsFsosYbRDRIkQF1Y16AMAMTQIAYPF0NQDgyKVWmCxkiRhNENEi8J62LhNu6nshEADTM+2idedoFdKVchhNFhy/civMMyRwCREtAu85N+DPmjgqHgq5BAAgFArw4MASkQSaRhdEtAi8x7E0VLk8Tvu1vrzUgn6rLcSzIgQLIloE3uNJtGZnJSElXorO3n6crGsP/cQIQYGIlhtMFiv0PeZwT4PgAxRF4eyAaM0cJFoioQCLptG7iCTQNFogojWIozWtmPSbg/j33WfCPRWCD1xv74GhzwKpWIhJ6UP75S3Ose8i/u1iCyxkiRgVBCRaW7ZsgUAgwJo1azyOmT9/PgQCwZBjyZIlzBiKovDiiy9CrVYjJiYGRUVFuHLlist5Ojo6UFxcDKVSCZVKhRUrVsBoNAYyfbckxNgdubeJpcULaCsrJ0MJiWjo1zl/fBJUsRK0d5tx5npHiGdHCAZ+i1ZFRQV27tyJ3NzcYcd9/PHHaG5uZo4LFy5AJBLhscceY8a8+uqr2LZtG3bs2IHTp08jLi4OixYtQl+fowJlcXExLl68iMOHD+Mvf/kLvv76azzzzDP+Tt8jqgHR0veQInJ8wJM/i0YiEmLh1DQAwEESaBoV+CVaRqMRxcXF2LVrFxITE4cdm5SUhPT0dOY4fPgwYmNjGdGiKAqvv/46fvOb3+Dhhx9Gbm4u/vjHP6KpqQmffPIJAODSpUs4ePAg/vCHPyA/Px/33HMP3njjDZSVlaGpqcmfS/BIYqzUfo0mC8wWspyIdOj0nTs9iBbgWCIevKCDzUaFYFaEYOKXaGm1WixZsgRFRUWsX7t792488cQTiIuLAwBcu3YNOp3O5VwJCQnIz8/HyZMnAQAnT56ESqXC7NmzmTFFRUUQCoU4ffq02/cxmUwwGAwuhy8oYySgu3KTkr2Rjdliw8Um++c6Y7TK47i5E5OhkIvR2mVCVcPtEM2OECxYi1ZZWRmqqqpQWlrK+s3OnDmDCxcu4Cc/+QnzmE5nN9nT0tJcxqalpTHP6XQ6pKamujwvFouRlJTEjBlMaWkpEhISmEOj0fg0R5FQwPi1yA5iZFOj64LZYkNCjARjk2M9jpOJRSiaYv9+kVxE/sNKtBobG1FSUoK9e/dCLpezfrPdu3dj+vTpmDNnDuvXsmXDhg3o7OxkjsbGRp9fq2Kc8cTSimSq6coOA0nSw0EHmh68oANFkSUin2ElWpWVlWhtbcWsWbMgFoshFotRXl6Obdu2QSwWw2r1nJja3d2NsrIyrFixwuXx9HT7l6mlxbURQUtLC/Nceno6WltdS4xYLBZ0dHQwYwYjk8mgVCpdDl9RDfi1yA5iZEOXV75zdILXsfdmj0KsVISb+l6cu9EZ5JkRggkr0SosLMT58+dRXV3NHLNnz0ZxcTGqq6shEok8vvajjz6CyWTCD3/4Q5fHx40bh/T0dBw5coR5zGAw4PTp0ygoKAAAFBQUQK/Xo7Kykhnz1VdfwWazIT8/n80l+ERiLFke8gHGCT9G5XWsXCLC/ZPtLoYDJNCU14jZDFYoFMjJyXF5LC4uDsnJyczjy5YtQ2Zm5hCf1+7du/HII48gOTnZ5XE6zmvTpk244447MG7cOLzwwgvIyMjAI488AgCYMmUKHnzwQaxcuRI7duxAf38/Vq9ejSeeeAIZGRlsr9kr9A4iCXuIXAx9/ahrs8fp5Q7jhHfmoRw1vjjXjIMXdPjVg5O9LikJkQkr0fKFhoYGCIWuBlxNTQ2OHz+Ov/3tb25f8/zzz6O7uxvPPPMM9Ho97rnnHhw8eNDFb7Z3716sXr0ahYWFEAqFWLp0KbZt28b19AEACbHEpxXpXLjRCYoCRifGICVe5tNr5k8aBZlYiPr2Hvyr2YBpGd6XlYTII2DROnbs2LC/A8CkSZOGdX4KBAK8/PLLePnllz2OSUpKwr59+/ydJisclhZZHkYqzk54X4mTiTF/0igcutiCgxd0RLR4Csk9dENiLEnliXTo9J07fVwa0tCBpgfOE78WXyGi5QYV8WlFPM414dnwwJRUSEVC1LV148pAyzECvyCi5QZVLMk/jGR0nX3QGfogFAA5mb6HsgCAUi7BPXekACCBpnyFiJYbEkmcVkRDhzpkpykQK2XvlnWUYSZLRD5CRMsNzpYWiZ6OPOjKDsMlSQ/HwqlpEAsF+FbXhWu3urmbGCEkENFyA21pma029PaT9lORxtkARUsVK0XBBHu8IKloyj+IaLkhViqCRGQPPCSxWpGFzUYxaThsnfDOOJerIfALIlpuEAgEjvzDbuLXiiSu3jLCaLIgRiLCHanxfp9n4bQ0CAXAuRudaNL3cjhDQrAhouWBRLKDGJFUD4Q6TM9MgNhNeWVfSYmXISvZXtOtvr2Hk7kRQgMRLQ8wsVq9xNKKJM4y5ZUDj2ZPjCOZD3yEiJYHSE2tyOSsH+k7nqA3XDqIaPEKIloeYPIPiU8rYujrt+JSs/fyyr5CXAD8hIiWB1RxxNKKNC41G9BvpZAcJ8XoxJiAz5c0sDzsIDcmXkFEywOJxKcVcTgHlXJRC4tUqOUnRLQ8QPofRh5nvfQ4ZEsSbU0TS4tXENHyALkLRx5nOQgqdcbxGZMbE58gouUB4qSNLPQ9ZiZPcIYPjSx8gfZpkRsTvyCi5QESwxNZ0Kk7WcmxjIUUKEyxR7I85BVEtDxA+7Q6e/tJK/UIgGt/FuDYbDH0WWCx2jg7LyG4ENHyAH03t1H2zi+E8MIElXIQn0VDdxIHAH0v+Yz5AhEtD0jFQsRJ7X0ciaM2vFAUxeQccmlpiUVCRrjIEpE/ENEaBhXpyhMRNHX24ZbRBLFQgGkZ7MoreyORtIvjHQGJ1pYtW5hmq8Oh1+uh1WqhVqshk8mQnZ2NAwcOMM9nZWVBIBAMObRaLTNm/vz5Q57/2c9+Fsj0vUJqxUcG1Q16AMAUtRJyiecu5v6QSKLieYfffQ8rKiqwc+dO5ObmDjvObDZjwYIFSE1Nxf79+5GZmYn6+nqoVCqXc1mtjgqhFy5cwIIFC/DYY4+5nGvlypUuvRFjY2P9nb5PkFrxkYEjSZr7PoWkxyX/8Eu0jEYjiouLsWvXLmzatGnYsXv27EFHRwdOnDgBicRuuWRlZbmMGTVqlMvvW7ZswYQJE3Dfffe5PB4bG4v09HR/puwXKrJ0iAjo9B0unfA0pNID//BreajVarFkyRIUFRV5HfvZZ5+hoKAAWq0WaWlpyMnJwebNm10sK2fMZjPef/99/PjHPx6SX7Z3716kpKQgJycHGzZsQE+P5+JtJpMJBoPB5WAL/YXuDMMX+l9NBnxxjtQvt1htOD8Qo+VvTfjhIEHE/IO1pVVWVoaqqipUVFT4NP7q1av46quvUFxcjAMHDqC2thY///nP0d/fj40bNw4Z/8knn0Cv1+NHP/qRy+NPPfUUxo4di4yMDJw7dw7r169HTU0NPv74Y7fvW1paipdeeont5bkQTkvr2Q+qUNfWjYmp92JSuiLk7x8pNHT0oLffihiJCONH+V9e2RPEp8U/WIlWY2MjSkpKcPjwYcjlcp9eY7PZkJqairfffhsikQh5eXm4efMmtm7d6la0du/ejcWLFyMjI8Pl8WeeeYb5efr06VCr1SgsLERdXR0mTJgw5DwbNmzAunXrmN8NBgM0Go2vlwogfPmHFEWhscNet7y21TiiRevmQP12TVIMRMLAKzsMhvi0+Acr0aqsrERraytmzZrFPGa1WvH111/jzTffhMlkgkjkurujVqshkUhcHp8yZQp0Oh3MZjOkUkdKRn19Pb788kuP1pMz+fn5AIDa2lq3oiWTySCTydhc3hDCtXQw9FpgHojQvqkf2fXL6aYTGarA62e5g670QCwt/sBKtAoLC3H+/HmXx5YvX47Jkydj/fr1QwQLAObNm4d9+/bBZrNBKLS70C5fvgy1Wu0iWADwzjvvIDU1FUuWLPE6l+rqagB2UQwW4aqp1WY0MT/fuD2yO8XcHLj+zCCJliMWj/i0+AIr0VIoFMjJyXF5LC4uDsnJyczjy5YtQ2ZmJkpLSwEAq1atwptvvomSkhI8++yzuHLlCjZv3oznnnvO5Tw2mw3vvPMOnn76aYjFrtOqq6vDvn378NBDDyE5ORnnzp3D2rVrce+993oNuQiEBCahNrRf6LYuIlo0N/V9AIJpaZHdQ77hd5yWJxoaGhiLCgA0Gg0OHTqEtWvXIjc3F5mZmSgpKcH69etdXvfll1+ioaEBP/7xj4ecUyqV4ssvv8Trr7+O7u5uaDQaLF26FL/5zW+4nr4L4fJ3uFpaZHkIBNPSciTGW21UUPxmBG4JWLSOHTs27O8AUFBQgFOnTg17noULF4Ki3FdT0Gg0KC8v93eKfkP7tLrNVpgtNkjFocl6ujXI0qIoipPywnykqTO4Pi36xkRRduGiLS9C5EJyD4dBKZeA1opQ+rWcLa0es3XEBrfabBSameWhb7vVbJGIhFDI7PdukvnAD4hoDYNQKGCqAITSUevs0wIczuiRxi2jCWarDUIBkK4MjmgBjlgtUumBHxDR8gKTfxjCL/Qto6tojVS/Fh2jla6UQywK3leVVHrgF0S0vBCOqHja0lLI7cuWkbqD2BTknUMaYmnxCyJaXgjHDiJtadG5diPV0gp2YCkNqebBL4hoeYGpqRWicrw2G4VbRvs/z0xGtEampXUzxKJFYrX4AREtL6hiQnsXvt1jhnWgkUbuQCmWkS5amYnBFq2BG1OIg4gJ/kFEywuh/kLTVlZirARZKXEA7P+8nmLYohlHYGnwdg4Bp0oPxNLiBUS0vKAKcUNP2gk/SiHD6AELw2iyoHMEdosJtU+LVHrgB0S0vJAYYp9Wm9G+Y5YSL4NcIkJKvL1SxUhbIvaYLcyObfB3D0mlBz5BRMsLtE8rVHfhW1329xmlsIsVbW2NtB1EOtxBIRNDKZd4GR0YiaTSA68gouWFUMdp0Sk8o+IHi9bIsrRC5YQHHJUebveYSTdxHkBEywu0k1bfYw6JM5z2aaUMWFqZI1S0QuXPAhw3JhsFdPVZgv5+hMAgouUF2qfVb6XQY3bfjINLbg2xtOxt0kauaAV35xAAZGIR002c7CBGPkS0vBAjEUE6kPcWih1E591DYOT6tEIVWEoTrn4ABPYQ0fKCQCAIaadp2tKidw01A6J18/bIitUKdvG/wSSR/EOf2Xe6AX/45mrY3p/zyqXRSGKsFK1dpqDfhS1WG9q7XXcPM1X25WGXyQJDr4UpAR3t3AyxaJHGvL5xu9uM//Nne5+IeRNTMEWtDPkciKXlA6GytDq6zaAoQChw3PljpCKkxNt/bhwhS0SrjYKuMzQVHmiIpeUb5252Mj//9YIuLHMgouUDDtEK7heaDndIipO51CqnrQ3a+oh2bhlN6Lfa67WnKgJrA+crpNKDb5xr1DM/H7wQng7oRLR8wPGFDq6lNdgJTzPSdhBDVfzPGSJavnH2hsPSutxiRG1rV8jnQETLB0K1s8TEaMW7NlcYaTuIdHnpUIQ70NCpPKFuF8c3zt3QA3DcWP96PvRLRCJaPkDHanUG2dKiKzwMtbRGVoBpqHcOAVJTyxd0nX1o7TJBKABW3z8RAHAgDH6tgERry5YtEAgEWLNmzbDj9Ho9tFot1Go1ZDIZsrOzceDAAeb53/72txAIBC7H5MmTXc7R19cHrVaL5ORkxMfHY+nSpWhpaQlk+j7j2FkKjaVFB5bSjLTlYSij4WlIpQfvnB2wsrLTFPhfMzIgEgpwqdmA67e6QzoPv0WroqICO3fu9Nrh2Ww2Y8GCBbh+/Tr279+Pmpoa7Nq1C5mZmS7jpk2bhubmZuY4fvy4y/Nr167F559/jo8++gjl5eVoamrCo48+6u/0WaEKkU+LiYb3aGmNkOVhiGrDO+Oo9BAdy0OrjcJ/HarBN1faODsnvTTMHZ2AxDgp5k5IBhD6XUS/4rSMRiOKi4uxa9cubNq0adixe/bsQUdHB06cOAGJxP7FyMrKGjoRsRjp6eluz9HZ2Yndu3dj3759eOCBBwAA77zzDqZMmYJTp07h7rvv9ucyfCZUd2FPjng6/7Crz15Xi25rFq2Ec3lI55jyvTnuN1fa8ObRWvy/KjlO/OoBTq7n3IATnq6ouzhHjW+u3MJfLzRj1fwJAZ/fV/yytLRaLZYsWYKioiKvYz/77DMUFBRAq9UiLS0NOTk52Lx5M6xW1zy+K1euICMjA+PHj0dxcTEaGhqY5yorK9Hf3+/yfpMnT8aYMWNw8uRJt+9rMplgMBhcDn8JVU2ttkHR8DSxUjETRzQSeiCGOoUHcIiWxUahy8T/pOmrbfYlW3NnHyduBYqiGNGaMSBaC6elQSiwi1koVwGsRausrAxVVVUoLS31afzVq1exf/9+WK1WHDhwAC+88AJee+01FwstPz8f7777Lg4ePIi33noL165dw3e+8x10ddm3U3U6HaRSKVQqlcu509LSoNO5N01LS0uRkJDAHBqNhu2lMtBR6J29/Uz99mDgaXkIjJwlonOV1lDuHsZIRZBL7P8O0VArvr7d4Wc6dbWdg/P1oLO3H1KREJPSFQDsN9c545IAAAdDuERkJVqNjY0oKSnB3r17IZf79oWy2WxITU3F22+/jby8PDz++OP49a9/jR07djBjFi9ejMceewy5ublYtGgRDhw4AL1ejz/96U/srsaJDRs2oLOzkzkaGxv9PhddCJCiAEOQrC2TxcpE3A92xAMjZwexecDKUsrFUAS5+N9gkqIoVut6u+PmduZaR8Dno53wUzKUkIodsvHQdDUA4MD50AWashKtyspKtLa2YtasWRCLxRCLxSgvL8e2bdsgFouHLPkAQK1WIzs7GyKRiHlsypQp0Ol0MJvdfzlUKhWys7NRW1sLAEhPT4fZbIZer3cZ19LS4tEPJpPJoFQqXQ5/kYqFiJfZ3X/B+kK3D4Q7iIUCtz6rkbKDGI6lIY0qisIenC2tM9cDFy3H0jDB5fFF09IhEABVDXo0d4bmu8lKtAoLC3H+/HlUV1czx+zZs1FcXIzq6moXYaKZN28eamtrYbPZmMcuX74MtVoNqVQ6ZDxgd/TX1dVBrbareF5eHiQSCY4cOcKMqampQUNDAwoKCthcgt8Eu/+hc3UHoXCo05R2Skf78pAusxxKJzxNUlx0hD30W20uN7f69h4ml9NfHDuHKpfH05Ry5I1JBAAcCtESkZVoKRQK5OTkuBxxcXFITk5GTk4OAGDZsmXYsGED85pVq1aho6MDJSUluHz5Mr744gts3rwZWq2WGfPLX/4S5eXluH79Ok6cOIHvf//7EIlEePLJJwEACQkJWLFiBdatW4ejR4+isrISy5cvR0FBQdB3DmmCnX/oqFjqXsjp5WG05x/e1NtFOTyWVnSEPTTpe2GxUZCJhZiWYV9hnL7mv1/LYrXhwk37RtZgSwsAFtNLxEgULV9oaGhAc7NjfavRaHDo0CFUVFQgNzcXzz33HEpKSvCrX/2KGXPjxg08+eSTmDRpEn7wgx8gOTkZp06dwqhRo5gxv//97/Hd734XS5cuxb333ov09HR8/PHHXE/fI0xuWpC+0IMrlg5mpCwPGUsrBLXhBxMtllb9gD9rbHIs7h5vj6UKxK9V22ZEb78VcVIRxo+KH/L8gzl2F03F9Q60dgVm0flCwPW0jh07NuzvAFBQUIBTp055PEdZWZnX95HL5di+fTu2b9/OdoqcEOz8Q08xWjT0P3Fnbz8Mff1B71ATLiLCp8Xz8jS0P2tschzmjEvC7uPXAhKtc412f1ZOZoJL9RGaTFUMZmhUONuox6GLLfj3u8f6/V6+QHIPfSTRKewhGDiSpd2LVrxMzMwhmmO1QtVV2h1JIaxQG0zoncOs5FjclWUPSbjSakT7gDXPFnrncIZG5XHMQwPWVijK1RDR8hFVTHDzDz0lSzsT7UvEcBT/c4buvBRNllZSnBTZafYlXYWfu4iOSPih/iyaxTl2v9apqx1+i6OvENHykWDnH3pbHgLRv4PY2tUHi42CWChAqiL0lla01NRyWFpxAID8cXa/1mk/logmixXf6mgnvMrjuDHJscjJVMJqo3D4X8EtZEBEy0fohNqg7R56SOFxZrRTk4tohF4apifI3fpOgk00iJbVRqHByREPgIla98evdam5C/1WComxEub75wna2gr2LiIRLR9RBbl1+i0fLK1oj4oPR3UHZ5hCgD39vO18pDP0wWy1QSISMH9HWrT+1Wxg7ZN1js/ylnS9eMCvdaL2VlBrzxHR8hHapxUM0errtzJJusNbWgM+LX10Lg/DUd3BGdrSMltsIWnMGwzqB2pbaZJiGWs1TSlHVnIsKAqorGdnbZ1tdB8J747xo+IxOV0Bi43C4UvBWyIS0fKRYC4daH+WVCyEUu45CmV0UnRbWqHsKu2OWKmIyavj6xKR9meNTYp1edxfv5anSHhP0DFbfw1iLiIRLR+hRavHbIXJwu1duM0psHQ4E5y2QPQ9/TBGQfmUwThqw4fH0hIIBExYCV9rxTvvHDrjj1/LaLKgts0IAMjVeLe0AEcC9TdXbqGrLzh/QyJaPqKQi0H7hrleIjpSeIZvl6WQS5hk6mh0xoe6Qas7+O6Mvz4gWlnJrpYWLVrnb3Six+zbDe/CzU5QFKBOkPu8m3tHajwmjIqD2WrDV9+2spi57xDR8hGhU/UFrkXLWwqPM9FcVyvcPi2A/6LFpPCkuFpamqRYZKpiYLFRqKrX+3Qu5/LKviIQCIJeroaIFguC9YX2JUaLJlp3ELv6+mHos1sA6jCKFp87TVMUxYhW1qDlIeC8RPQtefrsoPLKvkL7tY7VtKE7CG4MIlosCFalB0cXHvcVHpxxRMVHl6XVPBAJnxAjYWqXhQOm0gMPU3naukzo7bdCJBS4tVZp0fLVGU9bWsMFlbpjqlqJscmxMFlsOFbDXWMNGiJaLAhWp+nhyiwPJlotrXA74Wn4XOmB3jnMVMW4VBeloUXrn4169PUPv5l0u9uMxg77ZzKdxfIQsC8RHYGm3C8RiWixICFICbXekqWdidb8w0hwwgP8rvRwndk5jHX7/PiUOKTEy2C22Jh8Qk+cu2l/flxKnF/dn+hA06PftqKX45g3IlosCFYrMV+SpWnof+poKwYYzuoOziTF8bfSQz2zczjUnwXYLaB8H/1a5xr1ANg54Z3JHZ2ATFUMesxWlF/mdolIRIsFiUHoNE1RFCtHPF1Xq6PbHBQnZ7gIR1dpd/Db0nLNOXSHr34tf53wNPYlYnDK1RDRYkEwKj10m63oHfAv+LI8TIiRMFHz0WRtNYU575AmKUSNeYOBN0sLcIhWZf1t9FttHsc5nPD+WVqAowzzl5daOQ3IJqLFAnpnictkUDpROlYqQpyPu2bRuIMYzoqlziTytCMPRVGov+Xd0pqUpkBCjAQ9ZisuNrlvYKzr7ENrlwlCATA1w/8uVjM1KqQpZTCaLDh+5Zbf5xkMES0WBCNOq43FziFNtO0gWqw26Ax2S8tb+ZNgQ1d66Ou3ce5ADiYd3WZ0mSwQCOyBpJ4QCgVMNVNPfi26Uml2mgKxUv/DT4RCAX6xcBLeeHImU6ueC4hosUAV6yhdwhVsdg5pMqNMtFq7TLDaKEhEAp+yAoJJvEwM8UC+Fp+i4ml/lloph1wytJWfM/le8hD9iYT3xA9ma/C9GRk+ryJ8gYgWC5x3D7mqt8QmhYeGXh5GS/6hc/E/dz0fQ4lAIGDKLvNJtDwlSrsjf7xDtKy2od/jcwE64YMNES0W0JaWxUahm6Olg7d+h+6ItvxDxp+VEN6lIQ0fKz0w6TspnpeGNFPVSsRJRTD0WVCj63J5jqIop27SKs7nyQUBidaWLVsgEAiwZs2aYcfp9XpotVqo1WrIZDJkZ2fjwIEDzPOlpaW46667oFAokJqaikceeQQ1NTUu55g/fz4EAoHL8bOf/SyQ6bMmRuJUb4mjLXGHpeV7fFK0+bSYwNIw+7No+Jg0zcbSEouEyPPg16pv70Fnbz+kIiEmpSu4nygH+C1aFRUV2LlzJ3Jzc4cdZzabsWDBAly/fh379+9HTU0Ndu3ahczMTGZMeXk5tFotTp06hcOHD6O/vx8LFy5Ed3e3y7lWrlyJ5uZm5nj11Vf9nb5fONdb4ir4kE2MFg29PGzvNvtcZiSSiYTqDs7wUbSc24b5Qr6HeC3aCT8lQ+k2FSgS8Ms7ZjQaUVxcjF27dmHTpk3Djt2zZw86Ojpw4sQJSCT2f/isrCyXMQcPHnT5/d1330VqaioqKytx7733Mo/HxsYiPT3dnylzRmKsFC0GE2dfaIcj3vflYUKMBAq5GF19Fty83Ys70iLzjugrkRKjRcP4tHi1PPTd0gJcnfEURTHFJx1Lw8Cd8MHCLynVarVYsmQJioqKvI797LPPUFBQAK1Wi7S0NOTk5GDz5s2wWj37hDo77X+4pKQkl8f37t2LlJQU5OTkYMOGDejp8ezTMZlMMBgMLgcXMDW1OGrayiaFxxlHOzH+LxEjJRqeJhiZD8Gks6ef2dEeLkbLmemjEyATC9HebUZdm2NFw7a8cjhgbWmVlZWhqqoKFRUVPo2/evUqvvrqKxQXF+PAgQOora3Fz3/+c/T392Pjxo1DxttsNqxZswbz5s1DTk4O8/hTTz2FsWPHIiMjA+fOncP69etRU1ODjz/+2O37lpaW4qWXXmJ7eV7hMv/QOYWHTcgDYF8ifqvrwo0oiIqnd0HDnXdIk8Sz3cP6DrvopCpkPsdVycQizByjwqmrHThzrQMTU+Nhsdpw4Sbd4zByLS1WotXY2IiSkhIcPnwYcrlvXzCbzYbU1FS8/fbbEIlEyMvLw82bN7F161a3oqXVanHhwgUcP37c5fFnnnmG+Xn69OlQq9UoLCxEXV0dJkyYMOQ8GzZswLp165jfDQYDNBqNr5fqEabNFAdLB0OvBeaBVAq2lla07CAa+vqZTkSRYmmFMv+wsr4Dz31QjU3fz8H9k1L9Osfg5qy+MmdcMk5d7cDpa+14Kn8MatuM6O23Ik4qwvhR8X7NJRSwWh5WVlaitbUVs2bNglgshlgsRnl5ObZt2waxWOx2yadWq5GdnQ2RyBHwNmXKFOh0OpjNrl+K1atX4y9/+QuOHj2K0aNHDzuX/Px8AEBtba3b52UyGZRKpcvBBSoOnbR0NLxCLvYaEDiYaNlBpJeGibGSgKKvuSSUlR4+rW7CTX0v9hy/5vc56LZhY3xcGtLcTTvjr9r9WucG2oXlZCaEpVmur7D6lhQWFuL8+fMujy1fvhyTJ0/G+vXrXYSJZt68edi3bx9sNhuEQrtGXr58GWq1GlKpXQAoisKzzz6LP//5zzh27BjGjRvndS7V1dUA7KIYSuj+h2ybXrrDUbGUfRR4tNTVijR/FhBaS4v+/Cqud8BksUImZnfzAtjvHNLMHJMIsVAAnaEPN273MjuHMzQq1nMIJawsLYVCgZycHJcjLi4OycnJjP9p2bJl2LBhA/OaVatWoaOjAyUlJbh8+TK++OILbN68GVqtlhmj1Wrx/vvvY9++fVAoFNDpdNDpdOjttX+gdXV1eOWVV1BZWYnr16/js88+w7Jly3Dvvfd6DbngGi63w+kYLW9deNxBW1o3eb48DHdXaXeEstJDY4f98+vrt+GfDXq/zsF255AmRipiUnVOX+twioSPXH8WEISI+IaGBjQ3O+rnaDQaHDp0CBUVFcjNzcVzzz2HkpIS/OpXv2LGvPXWW+js7MT8+fOhVquZ48MPPwQASKVSfPnll1i4cCEmT56MX/ziF1i6dCk+//xzrqfvFS7zD/2J0aKhReuW0ey1dG4k43DCR45o0Tem7iD0uHSGoigXS/lErX+VEPz1aQF2vxYAfHOlDd/qaCe8yq95hIqAnQjHjh0b9ncAKCgowKlTpzyew1sen0ajQXl5uT/T45xEDmuIt/mRd0hDN4Awmiy4cbsXE1Mj13E6HJEWWArYfYwioQBWGwV9Tz/SlOyXbL7Q3m1maqkBwN/r2rFumPHu6DZZGIudrU8LsOch7iivw1/P69BvpZAYKwl7pQ1vRGbIawSj4rD34a0ALC2BQBAVO4iR6NMSCgXM5xxMvxa9NIwZ2IQ526hn3TmczjlMipP6Vcs9b2wihAIwu9i5o1XDdjmPBIhosYR20hr6+t1myLOBtrTYRMM7Ew07iA7RiowYLZpQVHqgP7fpmQnQJNkbqVawaFsPOPuz2FtZAKCUS1wK/UVyfBYNES2W0D4tigp8BzEQnxbg/w5ioGLLFc7F/yJpeQiEptJD44CFPDoxBvMmpAAA/s7SrxWIP4smf5yjQF8kR8LTENFiiUQkhGKgoFmgd2F/Kjw448/ycOuhbzH5hb/i6Letfr0nl+gMfbBRgFQkZJ0REGxCkTRN9xUcnRSLuRMHRKvOt+7PNIFaWoCjbjwA5GqIpRWVcNH/0GajmLxDNrW0nGHbTuz9U/XYfrQO/VYKm774V9gtLjpRWq0Kf/G/wTCiFUSfFn2z0STGoGCgHPGlZgPaB25mvnDdh2YW3iiYkAx1ghxzxiUhVRFZy3R3ENHyAy7yD/W9Dp9Yclzwl4dfX27Dxs8uAgBEQgHq2rrxafVNv96XK5oirPifMw6fVvCWh/TnNjoxFqMUMkwaqNZx8qrv1la9D23DvKGUS1D+v+/HByvv9vscoYSIlh9wEatF+7MSYyV+1y2il4dtXaZhY7Uut3RBu7cKVhuFR2dl4hcLswEA/33kyrBtpIJNpHTgcUewKz3YbBQTo6ZJsl//3Il2a+uEj0vEvn4rmjvt1irbwNLBSMXCiE7dcYaIlh9wYWn5W93BGVWsBHFS+3a5pyViW5cJy9+pQJfJgjnjklD66HQ8XZCF5Dgp6tt78HHVDb/fP1Aipau0O4K9e9jaZYLZaoNYKEC60n79tDPe1yDThoGQCYVczIjsSICIlh+oOPBp3fKjddhg7LFanpeIff1WPPN//4Gb+l5kJcdi5w/zIBPb+yuumm+vjLHtSG1Qo76HI7ItreD6tOidQ7VKDrHI/m+YPz4JIqEA19t7fPJTXr/l8GdFemwVlxDR8gMuKj1wYWkBzu3EXHcQbTYKv/zoLP7ZoEdCjAR7fnQXYz0AwA/vHotUhQw39b34U0VjQHPwl6YIqw3vDF3pIVg+LTqwVJPo8EUp5BIm78+X0Acu/Fl8hIiWH3BRJ96fJq3ucCROu96Zf//lZfzlXDMkIgF2/nvekPpIcokIqx+YCAB482htyPMXKcrh04lES0sVZEvL4YR3vXY2S0Qudg75CBEtP+AihieQFB5n3EXF76+8gTe+stcZ2/z96R67+z5+lwYZCXK0GEzYe7ohoHmwxdBnYdqwReLuIV3poctkCcpmhTtLCwDmTrB/Vn+va/eak0v7tIilRfAKJ7uHRm6Whw6flv0LfOpqOzZ8fA4AoL1/Ah6b7blaq0wswrOFdwAA3jpWG9LOPvTSMClOihhpcBKSA0EZIwG9mRYMZ/wNZufQVXBmjU2ETCxEW5cJdW3GYc/BWFopxNIieIFeOnRy4NPi0tK62mbET/9vJfqtFJZMV+MXCyZ5ff2/5Y3GmKRY3DKa8ceT9QHNhQ2OpWHk7RwC9li2BA6T4wfjnMLjjFwiwuysRADA32s9hz6YLTbmb0gsLYJXEjmwtG4FUJbGGdrSau0y4cfvVqCztx93alR47QczfIoyl4iEeG7A2tpRXoeuvtC0zWrqjLySNIOhNy64rvRgsdqY+KrBlhYAzPUhD/HG7R7YKCBWKgr4O8Q3iGj5AW1p9fZb/XJgW6w2tHcHlsJDkxgrYUqbXG/vQaYqBruWzWZVc/6ROzMwflQc9D39eOfv1wOaj69EcrgDDZedl5xp7uyD1UZBKha6FZx5A3mIp662e0y1cuwcjqxwB4CIll8oB4rEAf4tHTp6zKAoQCjwP4WHxrmuVrxMjD0/uov1klMsEmJNkT1Kftc3V9EZgoYOdN5hRFtaTK14bv8ezNJQFePWGp6emQCFXAxDnwUXbna6PQftzxrrxlKLdoho+YFA4OTv6GV/F6b9WUlxUk5SJxZMTYNCJsb24lmYlO5ft+nvTldjUpoCXX0W/OH41YDn5I1ILP43mGCl8txwqu7gDpFQwOz4/r3O/RKRsbRSiGgRfEQVQL0lrgJLaZ5/cDL++eIC3Jc9yu9zCIUCrF1g923tOX4t6J1oIjlGi4Zp2srx3+KGBye8M3TowwkPzviRGqMFENHym0D8HXRJmkB3Dp2hU0ECYdG0dEzLUKLbbMXO8joOZuWefqsNLV2Rvzx0ZD5wvTwcCHdI9Gwl0X4turXYYEZqNDxARMtvAtlBDKTfYTARCARMBYj3Tl5H64CwcI2usw8UZa8skBwX2EZEMHGk8nBraTGBpUmeBfuO1HiMUshgsthQVa93ec5itTHnIJYWwWcSYgYsLT98WlwkSweL+yel4k6NCn39Nrx1LDjWFh1YmZEQecX/nOGym7gzznW0PCEQCBxLxEF+rSZ9HywDu490hYiRRECitWXLFggEAqxZs2bYcXq9HlqtFmq1GjKZDNnZ2Thw4IDLmO3btyMrKwtyuRz5+fk4c+aMy/N9fX3QarVITk5GfHw8li5dipaWlkCmHxCB5B9yFVgaDJytrb2nG9DcyX3TjCutXQCACaMiu+1ZMHxaJouVWRprvCSKe6obX9/h2DmMZNEPFn6LVkVFBXbu3Om1w7PZbMaCBQtw/fp17N+/HzU1Ndi1axcyMzOZMR9++CHWrVuHjRs3oqqqCjNmzMCiRYvQ2uqoY7527Vp8/vnn+Oijj1BeXo6mpiY8+uij/k4/YBID+EJz7YjnmnsmpmBOVhLMFhu2H63l/Pzf6uyi5e9OZ6jgIoh4ME16+9I4VipiRNETdFHAszc6XYJ+rzvFaI1E/BIto9GI4uJi7Nq1C4mJicOO3bNnDzo6OvDJJ59g3rx5yMrKwn333YcZM2YwY373u99h5cqVWL58OaZOnYodO3YgNjYWe/bsAQB0dnZi9+7d+N3vfocHHngAeXl5eOedd3DixIlhm8AGk0DyDyN5eQi4WlsfVjQy/hOuqOGNaA2ka/X2w8JR0jT9txydGOM1KHR0YizGJMXCaqNwxqm1WD1TR2vkOeEBP0VLq9ViyZIlKCoq8jr2s88+Q0FBAbRaLdLS0pCTk4PNmzfDarXviJjNZlRWVrqcSygUoqioCCdPngQAVFZWor+/32XM5MmTMWbMGGbMYEwmEwwGg8vBJaoY+gvth6XFUbJ0MMkfn4x7Jqag30rh/VPc5SRSFMWI1uR0pZfR4cW5+Wmg7eJoGplmFr4Jzjw3JZgZS2uEJUrTsBatsrIyVFVVobS01KfxV69exf79+2G1WnHgwAG88MILeO2117Bp0yYAwK1bt2C1WpGWlubyurS0NOh0OgCATqeDVCqFSqXyOGYwpaWlSEhIYA6NxnO1A3/wd+lgttgYP1ikWlo0j8y0L+HP3tBzds4bt3thNFkgEQkwflRk/9OJRUJGuLhyxnuqo+UJd3mI9e3E0vKZxsZGlJSUYO/evZDLfdu1sNlsSE1Nxdtvv428vDw8/vjj+PWvf40dO3b4NWFf2bBhAzo7O5mjsZHb6pwqP+O02rvtVpbYqfV6pDJVbbeE/tVk8FrbyVdoK2vCqHhIOIgtCzZc+7Uc4Q6+CQ69g/itrgu3jCbYbBTq6TpaSZEt+sGC1bemsrISra2tmDVrFsRiMcRiMcrLy7Ft2zaIxWJmyeeMWq1GdnY2RCJHAu+UKVOg0+lgNpuRkpICkUg0ZCewpaUF6enpAID09HSYzWbo9XqPYwYjk8mgVCpdDi5JjHPsHrL5h6ad8Mnx0ojf+ZmYGg+pSAhDn8Xn3oreqGmhl4aR7c+i4brSQ6MP4Q7OJMfLmL/Vybp26Ax9MFvsDTEitaxPsGElWoWFhTh//jyqq6uZY/bs2SguLkZ1dbWLMNHMmzcPtbW1sNkcjszLly9DrVZDKpVCKpUiLy8PR44cYZ632Ww4cuQICgoKAAB5eXmQSCQuY2pqatDQ0MCMCTW0T8tio2A0+V48L9Kd8M5IxULckWYPS7jYxI1P0LFzGNn+LBquKz3c9CGFZzB0dPyJultM+o4mKZaTLAg+wuqqFQoFcnJyXI64uDgkJycjJycHALBs2TJs2LCBec2qVavQ0dGBkpISXL58GV988QU2b94MrVbLjFm3bh127dqF9957D5cuXcKqVavQ3d2N5cuXAwASEhKwYsUKrFu3DkePHkVlZSWWL1+OgoIC3H13eBpMxkhFkA30K2QTqxWp0fCecF4ickGNzn4e3lhaHFZ66DFbmBQuX5eHgMMZ//fa9hGdvkMj5vqEDQ0NEAodWqjRaHDo0CGsXbsWubm5yMzMRElJCdavX8+Mefzxx9HW1oYXX3wROp0Od955Jw4ePOjinP/9738PoVCIpUuXwmQyYdGiRfif//kfrqfPisRYKXSGPtzuMfv8JYz0GK3BTM1QApXAv5oDFy2zxYarbXZLIdLDHWgcQcSBW1q0E14pF7vsTHrjrix7a7GGjh4cH3DIj8T0HZqARevYsWPD/g4ABQUFXuOpVq9ejdWrV3t8Xi6XY/v27di+fbs/0wwKqljJgGj5fhcORrJ0MOHS0qprM8Jio6CQi6FO4Ic/hsumrY7qDuysJIVcghmjE1DVoMffLtp3y0eypTUyF8UcofLjLsw3S2tKhl20bup7Ay4O+K3T0pAv1Ta5XB42dtDNLNhXtqD9Wv1W+6bPSLa0iGgFgMNJ64dPiyeWllIuwZiBpe/FZvdVNH2FL+k7ziTFcbk89M/SAhzxWjTE0iL4hT9VAPi0e0jD1RKRL5HwztCfcQcHosVYWn501J41VgW5xP7vKhT4J3zRAhGtAPCn0gPflofAgDMegTvjHaLFJ0uLvTXtCSaFx4+67jKxCHdlJQEAMhNjIBWP3H/dkXvlHMDWp9XXb0XXQEzXSLO0Onv6mbZZ2TwSLefP2OahM46v+FJHazjoJeK4lMgu6RNsOA95GEmwLcdLW1lSkRBKOX/+9LSlVdtqhMlihUzMviM0HQmfqYqBUh7Z6UvO0H5LGwUY+vqZz5wthr5+JumaTWCpM/9eMBYthj58f2am98FRDLG0AoBttHSbkz+LL7tnAKBOkCMxVgKLjcKVluFbtXuCDirlkxMesDezVcjsN5hAUnnonMPkOCniZP7dsOJlYvz2f03DDI3K73lEA0S0AoBtMu0t2p/Fo6UhYK+vxfi1/FwiXuLhziGNI1bLf78W2+oOBM8Q0QoAeqnQpO/F/xyr9dptmrG04iO3mYMnGL+Wn854PjrhaZibEweWlqdehwTfIaIVAFnJsZg9NhEWG4VXD9ag8LVyfHa2yWPVB77FaDkTiKVFURQu8zDcgYaLqHhiaXEHEa0AEIuE+NNPC/C7H8xAulKOm/pePPfBP/HoWydQWX97yHgmRotH4Q40U9UJAOyWFttdtJv6XnTxpPCfOxI56Mpzg2XFUoJniGgFiFAowKOzRuPoL+fjFwuyESsV4Z8Neix96wRW76tyqa/exlOfFgCMHxUHqVgIo8nCxBv5Ct8K/w0mkYOmrY4UHiJagcK/b1CEEiMV4dnCO3Dsl/Px+GwNBALgL+eaUfi7cmz567fo6ut3JEvz0NKSiISMP4rtEpGP6TvOBOrToiiKEXqyPAwcIlock6qU4z//LRd/efYezJ2QDLPFhh3ldZi/9Rjj1+GjpQX474znS/cdTwTq07rd048es32TJlNFRCtQiGgFiWkZCdj7k3zsfno2xo+KQ3u32RENz0NLC/DfGf8tzwr/DYZZHvpZ6YF2EaQqZJBL2AfmElzhT1g2DxEIBCickoZ7s0dh3+kGvP7lZcRIRMjg6d2WtrTYlF52LfzHv51DwNEPwF9Li945JP4sbiCiFQIkIiGenpuFp/LHwGqjeJvsOlmthEAA6Ax9aDeakOyDxehc+C+DJ4X/BhPo7qGj1yE/b1aRBj//e3iKRCTk9fIgXiZmis9dau7y6TXOQaV8Sl1yJskpIt6fVmqOrtLE0uICIloEVjic8b4VBOT7ziHgqPRgtVEw9PneeYnGsTwklhYXENEisIJ2xvvq13IkSvPTnwXYa1nFSe0Wsj8VTBtJYCmnENEisIJtbS0+5xw6w1QwZRmrZbNRAdfRIrhCRIvACtrSqmszek0Q7+ztRxNd+C+N36JF+7XoAGFfuWU0wWyxQSgA1CO0IzTXBCRaW7ZsgUAgwJo1azyOeffddyEQCFwOudz1wxv8PH1s3bqVGZOVlTXk+S1btgQyfYIfpCpkSImXwkY5rChP0M9nJMhZ9fmLRGhL8dBACy9foZeG6oQYXqYwRSJ+/xUrKiqwc+dO5Obmeh2rVCrR3NzMHPX19S7POz/X3NyMPXv2QCAQYOnSpS7jXn75ZZdxzz77rL/TJ/iJQCDAFB8j4/la+M8dT8wZAwD4/GwTq1ZqdM4hSd/hDr9Ey2g0ori4GLt27UJiYqLX8QKBAOnp6czh3DkagMtz6enp+PTTT3H//fdj/PjxLuMUCoXLuLg4/lUMiAYczvjhdxDpncPJav464WlmjVFhcroCJosN/6/qhs+vuxFAMwuCe/wSLa1WiyVLlqCoqMin8UajEWPHjoVGo8HDDz+Mixcvehzb0tKCL774AitWrBjy3JYtW5CcnIyZM2di69atsFg8bz+bTCYYDAaXg8ANvjrjo8UJD9hvvMV3jwUA7D1d73O8FrG0uIe1aJWVlaGqqgqlpaU+jZ80aRL27NmDTz/9FO+//z5sNhvmzp2LGzfc363ee+89KBQKPProoy6PP/fccygrK8PRo0fx05/+FJs3b8bzzz/v8X1LS0uRkJDAHBqNxveLJAzLtAFL61tdF6weamtRFMU0s4iG5SEAPHJnBmKlItS1deP0tQ6fXnNDT8IduIZVGk9jYyNKSkpw+PDhIc50TxQUFKCgoID5fe7cuZgyZQp27tyJV155Zcj4PXv2oLi4eMj5161bx/ycm5sLqVSKn/70pygtLYVMNjSdZMOGDS6vMRgMRLg4YlxKPOQSIXrMVtS3d2P8qKEtrZo6+9DVZ4FYKMD4KGl5pZBL8PCdmfjgTAP2nm7A3eOTvb6G1NHiHlaWVmVlJVpbWzFr1iyIxWKIxWKUl5dj27ZtEIvFsFqH3wIHAIlEgpkzZ6K2tnbIc9988w1qamrwk5/8xOt58vPzYbFYcP36dbfPy2QyKJVKl4PADSKhgCmb7MkZTzvhJ4yK522upTuK8+0O+YMXmplKtJ6w2ig06cnykGtYfZsKCwtx/vx5VFdXM8fs2bNRXFyM6upqiETe8+qsVivOnz8PtVo95Lndu3cjLy8PM2bM8Hqe6upqCIVCpKamsrkEAkd4i4yncxOjZWlIk5OZgDs1KvRbKXz0j+Ed8jpDHyw2ChKRAGlKEqPFFayWhwqFAjk5OS6PxcXFITk5mXl82bJlyMzMZHxeL7/8Mu6++25MnDgRer0eW7duRX19/RBrymAw4KOPPsJrr7025H1PnjyJ06dP4/7774dCocDJkyexdu1a/PCHP/Rp95LAPd6c8Xwv/DccxfljUN2ox74z9fjpveMhFLpPBKcTpTNUMRB5GENgD+d2e0NDA5qbm5nfb9++jZUrV2LKlCl46KGHYDAYcOLECUydOtXldWVlZaAoCk8++eSQc8pkMpSVleG+++7DtGnT8B//8R9Yu3Yt3n77ba6nT/ARpiCgx+WhXbSmqKNPtL6bmwGlXIzGjl58faXN4zhatIgTnlsElD+1NniIwWBAQkICOjs7iX+LA3rMFkzbeAgUBZz5dSFSFY7lj9liw9QXD8Jio/D3Xz0QlSWGX/r8It75+3UsmJqGXctmux3z+8OX8d9HruDJORqUPuo9CHuk4+v/aPR4SAkhJVYqxvgU97W1rt7if+E/b9AO+SOXWtDc2et2jKOZBbG0uISIFsFvpmbYeyEOjoxn/Flp/C38542JqQrkj0uCjQLKzjS6HXODBJYGBSJaBL/x5IyPhsJ/vkBHyJdVNMBitQ15nqTwBAciWgS/8eSM/7aZ3913fGXRtDQkx0nRYjDhyLetLs+ZLTY0G+xleYilxS1EtAh+Q1ta1251o8fsyAN1hDtE94aHTCzCY7PtWRZ7Tze4PNfc2QuKAmRiIW9bxkUqRLQIfjNKIcMohQwU5XDGOxf+i/blIQA8NVCy5uvLbWho72Eed06Ujla/XrggokUIiGmDloiXW6Kn8J8vjEmOxb3ZowAA+844rK1G4s8KGkS0CAEx2Bk/UpzwztDhDx/9oxEmiz3/9gZpZhE0iGgRAmKwMz4auu+wpXByKtKUMrR3m3HoYgsAUkcrmBDRIgQEbWl922yAxWqLqsJ/viIWCfHEXXZra+8peylxsjwMHkS0CAGRlRyHWKkIJosNV291j8jlIQA8MUcDkVCA09c6UNva5WjQSpaHnENEixAQQqGj0cWXl1qYwn8T3BQGjGbUCTEonGwvk7T7+HW0ddlrbZHlIfcQ0SIEDL1E/HPVTQDRV/jPV+gI+T/9w57WEy8TQxUb/TuooWbkfbMInEM746+0GgGMvKUhzXcmpkCTFMPUzScxWsGBiBYhYKYOahE2UkVLKBTgqTljmd9JdYfgQESLEDCT0hVwLsw5knYOB/PY7NGQiOx/DE0S8WcFAyJahICRS0QujveRamkBQEq8DN+bkQEAyBko3UPgFlY14gkET0zLUOJKqxEKmTgqK5WyYfP3p+P7MzMxd0JKuKcSlRBLi8AJtDM+Oz16C//5ilwiwnfuGEWaWQQJYmkROOHf8jT4x/XbeGogD49ACBZEtAickBQnxdseGjwQCFxClocEAoFXBCRaW7ZsgUAgwJo1azyOeffddyEQCFwOudy1Q8uPfvSjIWMefPBBlzEdHR0oLi6GUqmESqXCihUrYDQaA5k+gUDgIX4vDysqKrBz507k5nrv56ZUKlFTU8P87s5R++CDD+Kdd95hfpfJXEvUFhcXo7m5GYcPH0Z/fz+WL1+OZ555Bvv27fP3EggEAg/xS7SMRiOKi4uxa9cubNq0yet4gUCA9PT0YcfIZDKPYy5duoSDBw+ioqICs2fb/SZvvPEGHnroIfzXf/0XMjIy2F8EgUDgJX4tD7VaLZYsWYKioiKfxhuNRowdOxYajQYPP/wwLl68OGTMsWPHkJqaikmTJmHVqlVob29nnjt58iRUKhUjWABQVFQEoVCI06dPu31Pk8kEg8HgchAIBP7DWrTKyspQVVWF0tJSn8ZPmjQJe/bswaeffor3338fNpsNc+fOxY0bN5gxDz74IP74xz/iyJEj+M///E+Ul5dj8eLFsFrtpWt1Oh1SU1NdzisWi5GUlASdTuf2fUtLS5GQkMAcGo2G7aUSCIQIhNXysLGxESUlJTh8+PAQZ7onCgoKUFBQwPw+d+5cTJkyBTt37sQrr7wCAHjiiSeY56dPn47c3FxMmDABx44dQ2FhIZspMmzYsAHr1q1jfjcYDES4CIQogJVoVVZWorW1FbNmzWIes1qt+Prrr/Hmm2/CZDJBJBINew6JRIKZM2eitrbW45jx48cjJSUFtbW1KCwsRHp6OlpbXZthWiwWdHR0ePSDyWQyF2c+RdnLhZBlIoEQmdD/m/T/qidYiVZhYSHOnz/v8tjy5csxefJkrF+/3qtgAXaRO3/+PB566CGPY27cuIH29nao1WoAdmtNr9ejsrISeXl5AICvvvoKNpsN+fn5Ps29q8teBphYWwRCZNPV1YWEBM/J5gLKm6x5Yf78+bjzzjvx+uuvAwCWLVuGzMxMxuf18ssv4+6778bEiROh1+uxdetWfPLJJ6isrMTUqVNhNBrx0ksvYenSpUhPT0ddXR2ef/55dHV14fz584y1tHjxYrS0tGDHjh1MyMPs2bN9Dnmw2WxoamqCQuE9N45eSjY2NkKpjK6uMtF8bQC5Pj5DURS6urqQkZEBodCzu53zNJ6GhgaXN7x9+zZWrlwJnU6HxMRE5OXl4cSJE5g6dSoAQCQS4dy5c3jvvfeg1+uRkZGBhQsX4pVXXnFZ3u3duxerV69GYWEhhEIhli5dim3btvk8L6FQiNGjR7O6FqVSGXVfDJpovjaAXB9fGc7CognY0opGDAYDEhIS0NnZGXVfjGi+NoBc30iA5B4SCAReQUTLDTKZDBs3bhySShQNRPO1AeT6RgJkeUggEHgFsbQIBAKvIKJFIBB4BREtAoHAK4hoEQgEXkFEi0Ag8AoiWoPYvn07srKyIJfLkZ+fjzNnzoR7Spzw29/+dkhJ68mTJ4d7Wn7z9ddf43vf+x4yMjIgEAjwySefuDxPURRefPFFqNVqxMTEoKioCFeuXAnPZFni7dp8KU8ezRDRcuLDDz/EunXrsHHjRlRVVWHGjBlYtGjRkAoTfGXatGlobm5mjuPHj4d7Sn7T3d2NGTNmYPv27W6ff/XVV7Ft2zbs2LEDp0+fRlxcHBYtWoS+vr4Qz5Q93q4NsNegc/4sP/jggxDOMMxQBIY5c+ZQWq2W+d1qtVIZGRlUaWlpGGfFDRs3bqRmzJgR7mkEBQDUn//8Z+Z3m81GpaenU1u3bmUe0+v1lEwmoz744IMwzNB/Bl8bRVHU008/TT388MNhmU8kQCytAcxmMyorK11KSAuFQhQVFeHkyZNhnBl3XLlyBRkZGRg/fjyKi4vR0NAQ7ikFhWvXrkGn07l8lgkJCcjPz4+az3K48uTRDhGtAW7dugWr1Yq0tDSXx9PS0jyWdOYT+fn5ePfdd3Hw4EG89dZbuHbtGr7zne8wdcaiCfrzitbP0lt58miHdJgeISxevJj5OTc3F/n5+Rg7diz+9Kc/YcWKFWGcGYEtwShPzieIpTVASkoKRCIRWlpaXB5vaWnx2v6Mj6hUKmRnZw9b9pqv0J/XSPksncuTjwSIaA0glUqRl5eHI0eOMI/ZbDYcOXLEpTFHtGA0GlFXV8eUtI4mxo0bh/T0dJfP0mAw4PTp01H5WQ4uTx7tkOWhE+vWrcPTTz+N2bNnY86cOXj99dfR3d2N5cuXh3tqAfPLX/4S3/ve9zB27Fg0NTVh48aNEIlEePLJJ8M9Nb8wGo0ulsW1a9dQXV2NpKQkjBkzBmvWrMGmTZtwxx13YNy4cXjhhReQkZGBRx55JHyT9pHhri0pKcltefKJEydi0aJFYZx1CAn39mWk8cYbb1BjxoyhpFIpNWfOHOrUqVPhnhInPP7445RaraakUimVmZlJPf7441RtbW24p+U3R48epQAMOZ5++mmKouxhDy+88AKVlpZGyWQyqrCwkKqpqQnvpH1kuGvr6emhFi5cSI0aNYqSSCTU2LFjqZUrV1I6nS7c0w4ZpJ4WgUDgFcSnRSAQeAURLQKBwCuIaBEIBF5BRItAIPAKIloEAoFXENEiEAi8gogWgUDgFUS0CAQCryCiRSAQeAURLQKBwCuIaBEIBF7x/wGnPLzcMlB8+wAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "d4535356-7440-4c2e-950c-9018b418e702"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-3260ab0b1a5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-21e54a9e1038>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-92987c4a48a2>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0manc_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc_boxes_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# filter based on confidence threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mconf_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_scores\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-74910fceec3f>\u001b[0m in \u001b[0;36mgenerate_proposals\u001b[0;34m(anchors, offsets)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# apply offsets to anchors to create proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mproposals_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (21600) must match the size of tensor b (10800) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "    print(66666666666666, reg_offsets_pred.shape, conf_scores_pred.shape)\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "\n",
        "\n",
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                print(offsets_pred.shape, anc_boxes_flat.shape)\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "def generate_proposals(anchors, offsets):\n",
        "    print(anchors.shape, offsets.shape)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)\n",
        "\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "# learning_rate = 1e-1\n",
        "# n_epochs = 20\n",
        "# loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)\n",
        "\n",
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ],
      "metadata": {
        "id": "_X2S_JjJMARK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}