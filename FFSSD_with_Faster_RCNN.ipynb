{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e684280a-818d-45b1-f4d7-167d72d3a049"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc459f7-460b-4e44-da02-f4db90986745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87qVg5W5OQz6",
        "outputId": "4a4ff91e-e5f2-4162-be8e-9e53bee6a3cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:45<00:00,  2.28s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5dcd45a-810b-47c6-c527-e092ac9dc61e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.67214822769165,\n",
              " 4.628778457641602,\n",
              " 4.644930839538574,\n",
              " 4.6545329093933105,\n",
              " 4.551773548126221,\n",
              " 4.659956455230713,\n",
              " 4.580636978149414,\n",
              " 4.638859272003174,\n",
              " 4.672650337219238,\n",
              " 4.763477802276611,\n",
              " 4.635280609130859,\n",
              " 4.68095064163208,\n",
              " 4.810985088348389,\n",
              " 4.618777275085449,\n",
              " 4.638202667236328,\n",
              " 4.663118839263916,\n",
              " 4.6583333015441895,\n",
              " 4.629514694213867,\n",
              " 4.601409912109375,\n",
              " 4.5630035400390625]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "6a854559-91c9-4b29-e3a5-cc3fa8beaaa5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd0d0781ed0>]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ssss"
      ],
      "metadata": {
        "id": "kLANsPXHCdhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "372Vl8T9CeJl",
        "outputId": "e7d0fa95-585e-42e7-de86-23cd5b88743c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out"
      ],
      "metadata": {
        "id": "gwxJnIQAVd2F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = FeatureExtractor()\n",
        "backbone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J28d3rTFVgOy",
        "outputId": "7da2eaa8-b1bb-48ef-ecf2-ee6636616935"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FeatureExtractor(\n",
              "  (model): VGG(\n",
              "    (features): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (17): ReLU(inplace=True)\n",
              "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (24): ReLU(inplace=True)\n",
              "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (26): ReLU(inplace=True)\n",
              "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (29): ReLU(inplace=True)\n",
              "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (31): ReLU(inplace=True)\n",
              "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (33): ReLU(inplace=True)\n",
              "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (35): ReLU(inplace=True)\n",
              "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "    (classifier): Sequential(\n",
              "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Dropout(p=0.5, inplace=False)\n",
              "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "      (4): ReLU(inplace=True)\n",
              "      (5): Dropout(p=0.5, inplace=False)\n",
              "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (block): ParameterList(\n",
              "      (0): Object of type: Conv2d\n",
              "      (1): Object of type: ReLU\n",
              "      (2): Object of type: Conv2d\n",
              "      (3): Object of type: ReLU\n",
              "      (4): Object of type: MaxPool2d\n",
              "      (5): Object of type: Conv2d\n",
              "      (6): Object of type: ReLU\n",
              "      (7): Object of type: Conv2d\n",
              "      (8): Object of type: ReLU\n",
              "      (9): Object of type: MaxPool2d\n",
              "      (10): Object of type: Conv2d\n",
              "      (11): Object of type: ReLU\n",
              "      (12): Object of type: Conv2d\n",
              "      (13): Object of type: ReLU\n",
              "      (14): Object of type: Conv2d\n",
              "      (15): Object of type: ReLU\n",
              "      (16): Object of type: Conv2d\n",
              "      (17): Object of type: ReLU\n",
              "      (18): Object of type: MaxPool2d\n",
              "      (19): Object of type: Conv2d\n",
              "      (20): Object of type: ReLU\n",
              "      (21): Object of type: Conv2d\n",
              "      (22): Object of type: ReLU\n",
              "      (23): Object of type: Conv2d\n",
              "      (24): Object of type: ReLU\n",
              "      (25): Object of type: Conv2d\n",
              "      (26): Object of type: ReLU\n",
              "      (27): Object of type: MaxPool2d\n",
              "      (28): Object of type: Conv2d\n",
              "      (29): Object of type: ReLU\n",
              "      (30): Object of type: Conv2d\n",
              "      (31): Object of type: ReLU\n",
              "      (32): Object of type: Conv2d\n",
              "      (33): Object of type: ReLU\n",
              "      (34): Object of type: Conv2d\n",
              "      (35): Object of type: ReLU\n",
              "      (36): Object of type: MaxPool2d\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "# backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# FasterRCNN needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "# backbone.out_channels = 1280\n",
        "backbone.out_channels = 512\n",
        "\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                output_size=7,\n",
        "                                                sampling_ratio=2)\n",
        "\n",
        "# put the pieces together inside a FasterRCNN model\n",
        "model = FasterRCNN(backbone,\n",
        "                   num_classes=1,\n",
        "                   rpn_anchor_generator=anchor_generator,\n",
        "                   box_roi_pool=roi_pooler)"
      ],
      "metadata": {
        "id": "FUt8uQY1CjKJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSfFG1NGV2Ch",
        "outputId": "b96cd5d4-ab01-4162-dc69-6dc1920eae3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): FeatureExtractor(\n",
              "    (model): VGG(\n",
              "      (features): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): ReLU(inplace=True)\n",
              "        (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (6): ReLU(inplace=True)\n",
              "        (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (8): ReLU(inplace=True)\n",
              "        (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (11): ReLU(inplace=True)\n",
              "        (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (13): ReLU(inplace=True)\n",
              "        (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (15): ReLU(inplace=True)\n",
              "        (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (17): ReLU(inplace=True)\n",
              "        (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (20): ReLU(inplace=True)\n",
              "        (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (22): ReLU(inplace=True)\n",
              "        (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (24): ReLU(inplace=True)\n",
              "        (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (26): ReLU(inplace=True)\n",
              "        (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "        (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (29): ReLU(inplace=True)\n",
              "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (31): ReLU(inplace=True)\n",
              "        (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (33): ReLU(inplace=True)\n",
              "        (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (35): ReLU(inplace=True)\n",
              "        (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "      (classifier): Sequential(\n",
              "        (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "        (1): ReLU(inplace=True)\n",
              "        (2): Dropout(p=0.5, inplace=False)\n",
              "        (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "        (4): ReLU(inplace=True)\n",
              "        (5): Dropout(p=0.5, inplace=False)\n",
              "        (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (block): ParameterList(\n",
              "        (0): Object of type: Conv2d\n",
              "        (1): Object of type: ReLU\n",
              "        (2): Object of type: Conv2d\n",
              "        (3): Object of type: ReLU\n",
              "        (4): Object of type: MaxPool2d\n",
              "        (5): Object of type: Conv2d\n",
              "        (6): Object of type: ReLU\n",
              "        (7): Object of type: Conv2d\n",
              "        (8): Object of type: ReLU\n",
              "        (9): Object of type: MaxPool2d\n",
              "        (10): Object of type: Conv2d\n",
              "        (11): Object of type: ReLU\n",
              "        (12): Object of type: Conv2d\n",
              "        (13): Object of type: ReLU\n",
              "        (14): Object of type: Conv2d\n",
              "        (15): Object of type: ReLU\n",
              "        (16): Object of type: Conv2d\n",
              "        (17): Object of type: ReLU\n",
              "        (18): Object of type: MaxPool2d\n",
              "        (19): Object of type: Conv2d\n",
              "        (20): Object of type: ReLU\n",
              "        (21): Object of type: Conv2d\n",
              "        (22): Object of type: ReLU\n",
              "        (23): Object of type: Conv2d\n",
              "        (24): Object of type: ReLU\n",
              "        (25): Object of type: Conv2d\n",
              "        (26): Object of type: ReLU\n",
              "        (27): Object of type: MaxPool2d\n",
              "        (28): Object of type: Conv2d\n",
              "        (29): Object of type: ReLU\n",
              "        (30): Object of type: Conv2d\n",
              "        (31): Object of type: ReLU\n",
              "        (32): Object of type: Conv2d\n",
              "        (33): Object of type: ReLU\n",
              "        (34): Object of type: Conv2d\n",
              "        (35): Object of type: ReLU\n",
              "        (36): Object of type: MaxPool2d\n",
              "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): ReLU(inplace=True)\n",
              "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (3): ReLU(inplace=True)\n",
              "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (6): ReLU(inplace=True)\n",
              "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (8): ReLU(inplace=True)\n",
              "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (11): ReLU(inplace=True)\n",
              "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (13): ReLU(inplace=True)\n",
              "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (15): ReLU(inplace=True)\n",
              "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (17): ReLU(inplace=True)\n",
              "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (20): ReLU(inplace=True)\n",
              "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (22): ReLU(inplace=True)\n",
              "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (24): ReLU(inplace=True)\n",
              "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (26): ReLU(inplace=True)\n",
              "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (29): ReLU(inplace=True)\n",
              "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (31): ReLU(inplace=True)\n",
              "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (33): ReLU(inplace=True)\n",
              "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (35): ReLU(inplace=True)\n",
              "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=1, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "loss_list = training_loop(model, learning_rate, od_dataloader, n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "kLJY1fmXWJtO",
        "outputId": "6df39519-54f4-4a3f-f3c7-8bed313ca6a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c4bbfb8cebbb>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mod_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-c4bbfb8cebbb>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, learning_rate, train_dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_bboxes_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_classes_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m               \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: GeneralizedRCNN.forward() takes from 2 to 3 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_FZM1aDWpIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "FF2pFCXWgHC4",
        "CVyQczF6VjNJ"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}