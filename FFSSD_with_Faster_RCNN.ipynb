{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/FFSSD_with_Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "b99e4733-1569-48e0-88b0-ec9692704a03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torchsummary\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# !pip install torch_intermediate_layer_getter\n",
        "# from torch_intermediate_layer_getter import IntermediateLayerGetter\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "clear_output()\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFv4RLO6zEAX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main Part"
      ],
      "metadata": {
        "id": "9zLoTK3ndnQI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1i6Zvz3aj_L9",
        "outputId": "ff4a353c-86a0-4f67-fe99-ab115938c722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size, limit=20):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes[:limit]))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes[:limit]))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths[:limit]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    # gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    gt_classes_expand = torch.zeros((B, tot_anc_boxes, N), device=device)\n",
        "    \n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "\n",
        "\n",
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    self.layers_id = [14, 16, 28, 30, 34]\n",
        "    super(FeatureExtractor, self).__init__()\n",
        "    self.model = torchvision.models.vgg19(weights='IMAGENET1K_V1')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    self.model = self.model.to(device)\n",
        "    for param in self.model.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "    for name in self.model.children():\n",
        "      self.block = torch.nn.ParameterList(name)\n",
        "      break\n",
        "\n",
        "  def forward(self, images):\n",
        "    return training(self.block, torch.nn.Sequential(*self.model.features), images, self.layers_id)\n",
        "\n",
        "\n",
        "\n",
        "def ftb(block, current_layer, former_layer=None, ):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  x = torch.nn.Conv2d(current_layer.shape[1], 256, kernel_size=3, stride=1, padding=1, device=device)(current_layer)\n",
        "  block.append(x)\n",
        "  x = torch.nn.ReLU(inplace=True)(x)\n",
        "  block.append(x)\n",
        "  x = torch.nn.Conv2d(256, 512, kernel_size=3, padding=1, device=device)(x)\n",
        "  block.append(x)\n",
        "  if former_layer is None:\n",
        "    return block, x\n",
        "  elif x.shape == former_layer.shape:\n",
        "    return block, torch.add(x, former_layer)\n",
        "\n",
        "  else:\n",
        "    d = torch.nn.ConvTranspose2d(former_layer.shape[1], 512, kernel_size=4, stride=2, padding=1, device=device)(former_layer)\n",
        "    out =  torch.add(x, d)\n",
        "    block.append(out)\n",
        "    return block, out\n",
        "\n",
        "\n",
        "def rfem(x, block):\n",
        "  channel = x.shape[1]\n",
        "  #branch1\n",
        "  \n",
        "  x1 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.ReLU()(x1)\n",
        "  block.append(x1)\n",
        "  x1 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x1)\n",
        "  block.append(x1)\n",
        "\n",
        "  #branch2\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.ReLU()(x2)\n",
        "  block.append(x2)\n",
        "  x2 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x2)\n",
        "  block.append(x2)\n",
        "\n",
        "  #branch3\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.ReLU()(x3)\n",
        "  block.append(x3)\n",
        "  x3 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x3)\n",
        "  block.append(x3)\n",
        "\n",
        "  #branch4\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel, out_channels=channel//4, kernel_size=(1, 1), padding='same', device=device)(x)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//3, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//3, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//3, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//3, out_channels=channel//4, kernel_size=(3, 1), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.Conv2d(in_channels=channel//4, out_channels=channel//4, kernel_size=(1, 3), padding='same', device=device)(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.ReLU()(x4)\n",
        "  block.append(x4)\n",
        "  x4 = torch.nn.BatchNorm2d(num_features=channel//4, device=device)(x4)\n",
        "  block.append(x4)\n",
        "\n",
        "  combined_x = torch.concat((x1, x2, x3, x4), dim=1)\n",
        "  out = torch.add(combined_x, x/0.5)\n",
        "  block.append(out)\n",
        "\n",
        "  return block, out\n",
        "\n",
        "def training(block, model, images, layers_id):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    feature_maps = []\n",
        "    x = images\n",
        "    for id, layer in enumerate(model):\n",
        "      x = layer(x)\n",
        "      x = x.to(device)\n",
        "      if id in layers_id:\n",
        "        feature_maps.append(x)\n",
        "\n",
        "    conv4_2=feature_maps[0]\n",
        "    conv4_2 = torch.nn.Conv2d(in_channels=feature_maps[0].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_2)\n",
        "    conv4_2 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_2)\n",
        "    conv4_3=feature_maps[1]\n",
        "    conv4_3 = torch.nn.Conv2d(in_channels=feature_maps[1].shape[1], out_channels=feature_maps[4].shape[1], kernel_size=2, stride=1, padding='same', device=device)(conv4_3)\n",
        "    conv4_3 = torch.nn.MaxPool2d(kernel_size=4, stride=4, padding=1)(conv4_3)\n",
        "    conv5_3=feature_maps[2]\n",
        "    conv6_2=feature_maps[3]\n",
        "    conv7_2=feature_maps[4]\n",
        "\n",
        "\n",
        "    block, rfem11_temp = rfem(conv4_2, block)\n",
        "    block, rfem12_temp = rfem(rfem11_temp, block)\n",
        "    block, rfem13_temp = rfem(rfem12_temp, block)\n",
        "    block, rfem14_temp = rfem(rfem13_temp, block)\n",
        "\n",
        "    rfem11 = torch.add(rfem11_temp, conv4_3)\n",
        "    rfem12 = torch.add(rfem12_temp, conv5_3)\n",
        "    rfem13 = torch.add(rfem13_temp, conv7_2)\n",
        "    rfem14 = torch.add(rfem14_temp, conv6_2)\n",
        "    \n",
        "    block, ftb_4 = ftb(block, rfem14)\n",
        "    block, ftb_3 = ftb(block, rfem13)\n",
        "    block, ftb_2 = ftb(block, rfem12)\n",
        "    block, ftb_1 = ftb(block, rfem11)\n",
        "\n",
        "    block, rfem21_temp = rfem(ftb_1, block)\n",
        "    block, rfem22_temp = rfem(ftb_2, block)\n",
        "    block, rfem23_temp = rfem(ftb_3, block)\n",
        "    block, rfem24_temp = rfem(ftb_4, block)\n",
        "\n",
        "    rfem21 = torch.add(rfem21_temp, conv4_3)\n",
        "    rfem22 = torch.add(rfem21_temp, conv5_3)\n",
        "    rfem23 = torch.add(rfem21_temp, conv7_2)\n",
        "    rfem24 = torch.add(rfem21_temp, conv6_2)\n",
        "\n",
        "    det_1 = torch.nn.Softmax(dim=1)(torch.cat((rfem21, rfem11)))\n",
        "    block.append(det_1)\n",
        "    det_2 = torch.nn.Softmax(dim=1)(torch.cat((rfem22, rfem12)))\n",
        "    block.append(det_2)\n",
        "    det_3 = torch.nn.Softmax(dim=1)(torch.cat((rfem23, rfem13)))\n",
        "    block.append(det_3)\n",
        "    det_4 = torch.nn.Softmax(dim=1)(torch.cat((rfem24, rfem14)))\n",
        "    block.append(det_4)\n",
        "\n",
        "    D = torch.cat((det_1, det_2, det_3, det_4), dim=1)\n",
        "    out=torch.nn.Conv2d(in_channels=D.shape[1], out_channels=D.shape[1], kernel_size=1, stride=1, device=device)(D)\n",
        "    block.append(out)\n",
        "\n",
        "    return block, out\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# x = torch.rand((5, 3, 320, 320))\n",
        "# x = x.to(device)\n",
        "# fe = FeatureExtractor()\n",
        "# block, res = fe(x)    \n",
        "# res.shape    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        block, feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            block, feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 1\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)"
      ],
      "metadata": {
        "id": "eVWm0UFC29gK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "f49e3381-b93a-46cb-d9da-692a2ddcada4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 1/20 [00:01<00:37,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [00:04<00:37,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [00:06<00:35,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [00:08<00:33,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n",
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 6/20 [00:13<00:33,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [00:15<00:30,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [00:17<00:27,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [00:20<00:24,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [00:22<00:21,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [00:24<00:21,  2.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [00:27<00:19,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [00:29<00:16,  2.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [00:31<00:13,  2.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [00:34<00:11,  2.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [00:36<00:09,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [00:41<00:09,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [00:43<00:05,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n",
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:49<00:00,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 30, 40, 9, 4])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "# clear_output()\n",
        "learning_rate = 1e-1\n",
        "n_epochs = 20\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fFRW4iC1uXf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a913d781-9381-4538-f968-d58e1344f4f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.6993937492370605,\n",
              " 4.721628189086914,\n",
              " 4.599406719207764,\n",
              " 4.653415679931641,\n",
              " 4.625076770782471,\n",
              " 4.706867218017578,\n",
              " 4.6336669921875,\n",
              " 4.672582626342773,\n",
              " 4.646522045135498,\n",
              " 4.5743727684021,\n",
              " 4.589663028717041,\n",
              " 4.668224811553955,\n",
              " 4.687575340270996,\n",
              " 4.634005069732666,\n",
              " 4.591204643249512,\n",
              " 4.638217449188232,\n",
              " 4.651782989501953,\n",
              " 4.58854866027832,\n",
              " 4.631416320800781,\n",
              " 4.61844539642334]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "27998004-cbd2-4c50-a12b-20600ba85eeb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0b4a5dc250>]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "d9ac132e-693d-4230-c846-917b9a848e5c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-3260ab0b1a5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-21e54a9e1038>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassificationModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-92987c4a48a2>\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, images, conf_thresh, nms_thresh)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffsets_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0manc_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manc_boxes_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manc_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0;31m# filter based on confidence threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0mconf_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf_scores\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-74910fceec3f>\u001b[0m in \u001b[0;36mgenerate_proposals\u001b[0;34m(anchors, offsets)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# apply offsets to anchors to create proposals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mproposals_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mproposals_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (21600) must match the size of tensor b (10800) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "# prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(600, 300))\n",
        "\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}