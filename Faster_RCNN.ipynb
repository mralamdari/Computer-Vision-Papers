{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dff215a2-657f-4f9d-9d98-c8c82c16dfb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19c0d8b-fbf4-4036-d399-e238d912faf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Paye0SiEAuJu",
        "outputId": "40553b91-0fd0-4de5-d6c6-64e496bf29cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b52f85ee23c9>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;31m# proposals_final.shape, conf_scores_final.shape, classes_final.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_data_all' is not defined"
          ]
        }
      ],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        # return proposals_final.to(device), conf_scores_final.to(device), feature_map.to(device), [out_c, out_h, out_w]\n",
        "        # return torch.Tensor(proposals_final), torch.Tensor(conf_scores_final), torch.Tensor(feature_map), [out_c, out_h, out_w]\n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        print('len(proposals_final)', len(proposals_final), proposals_final[0].shape)\n",
        "        print('len(conf_scores_final)', len(conf_scores_final), conf_scores_final[0].shape)\n",
        "        print('len(feature_map)', len(feature_map), feature_map[0].shape)\n",
        "        print(out_size)\n",
        "        # print(conf_scores_final)\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        print(classes_all)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "d = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "\n",
        "d.eval()\n",
        "proposals_final, conf_scores_final, classes_final = d.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "# proposals_final.shape, conf_scores_final.shape, classes_final.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "31185af0-a53b-4fc0-c3a2-c81de247bb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 238MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnKE3neESZ5",
        "outputId": "0d2960fd-232e-4597-b2c4-16847b6d8f35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [06:51<00:00, 137.07s/it]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 3\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2nsbro-Tse",
        "outputId": "48019e2b-a8f7-4a9e-a662-acbf33aab8d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.39511986262238, 4.781037269100066, 4.412893439767547]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "n_dOW6SREUn2",
        "outputId": "1a1effb0-a0bd-4b34-c2a6-d15b79fbe42e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fddf3601610>]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARwAAAESCAYAAAAv/mqQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApwElEQVR4nO3deVxU9f7H8deZGWBEWRRkU9TQABNxI3eudbMszdJKC0nqXrtq4VZZV/vVVfMWtqdmbi1el0LNMkvSzNJENJXFcFcUAQWXlF3Z5vz+8EaXRGRY5szA5/l4nD88fM+c93Ee8+acLzNzFFVVVYQQwgJ0WgcQQjQeUjhCCIuRwhFCWIwUjhDCYqRwhBAWI4UjhLAYKRwhhMUYtA5QHSaTibNnz+Lk5ISiKFrHEUL8D1VVycvLw8fHB52u6nMYmyics2fP4uvrq3UMIUQV0tPTad26dZVjbKJwnJycgGsH5OzsrHEaIcT/ys3NxdfXt/x1WhWbKJzfL6OcnZ2lcISwUtWZ7pBJYyGExUjhCCEsRgpHCGExZhXOzJkzURSlwhIYGFitbaOjo1EUhWHDhtUkpxCiATB70rhTp0788MMPfzyA4eYPkZqaytSpUwkNDTV3d0KIBsTswjEYDHh5eVV7fFlZGeHh4cyaNYsdO3aQnZ1t7i6FEA2E2XM4x48fx8fHBz8/P8LDw0lLS6ty/KuvvoqHhwdjxoyp9j6KiorIzc2tsFRHdmExy3elVns/QgjLMusMp1evXixbtoyAgAAyMzOZNWsWoaGhHDhwoNI3/cTGxvLxxx+TlJRkVqioqChmzZpl1jZXS8oY/mEcpy4WoNcphPdqa9b2Qoj6Z9YZzn333ceIESMIDg5m0KBBxMTEkJ2dzZo1a64bm5eXx+jRo1m6dCnu7u5mhZo+fTo5OTnlS3p6+k23MdrpGRFy7W3VMzccZG/qJbP2KYSof7V6p7Grqyv+/v6cOHHiup+lpKSQmprK0KFDy9eZTKZrOzUYOHr0KO3bt6/0cR0cHHBwcDA7z9MD2nPwTC4bkzN5emUC30zsh7dLE7MfRwhRP2r1Ppz8/HxSUlLw9va+7meBgYEkJyeTlJRUvjzwwAPceeedJCUl1cuHMRVF4a0RwQR6OXExv4jxK+K5WlJW5/sRQtSMWYUzdepUtm/fTmpqKnFxcQwfPhy9Xk9YWBgAERERTJ8+HQCj0UhQUFCFxdXVFScnJ4KCgrC3t6/7owEc7Q0sGR2Cq6Md+zNy+L+vDiB3whHCOphVOBkZGYSFhREQEMDIkSNxc3Nj9+7dtGzZEoC0tDQyMzPrJag52rg58kFYd3QKrEvIYFlcqtaRhBCAYgs3wsvNzcXFxYWcnByzPi3+0Y6T/HvjYfQ6hRVjetK3vXmT10KImzPn9dmgP0s1pv8tDOvqQ5lJJXJVAumXCrWOJESj1qALR1EU5jwcTFArZy4XljBuRTxXimUSWQitNOjCgWvvz1k8OgS3pvYcyszln+t+lUlkITTS4AsHoJVrExaEd0evU9iw/yxLd5zUOpIQjVKjKByA3n5u/Ov+2wCY890Rfj52QeNEQjQ+jaZwACL6tGVEj9aYVJj4eSKnfyvQOpIQjUqjKhxFUZg9LIguvq7kXClh7PJ4CopKtY4lRKPRqAoH/juJ/HgPWjo5cPRcHlPX7pdJZCEspNEVDoCXi5FFj3fHTq/w3YEsPtyWonUkIRqFRlk4AD3atuDVB4MAePv7o/x45JzGiYRo+Bpt4QCE9WxDeK82qCpM/jyJlAv5WkcSokFr1IUDMGNoJ0LaNievqJSxy/eRd7VE60hCNFiNvnDsDTo+fLw7Xs5GUi4U8Ozq/ZhMMoksRH1o9IUD4OFkZPHoHtgbdPxw+Bxztx7XOpIQDZIUzn918XXltWHXJpHnbj3OpgNZGicSouGRwvkfI0J8ebJvOwCeX5PEsXN52gYSooGRwvmT/xvSkd5+LSgoLmPs8n3kFMokshB1RQrnT+z0OhaM6k4r1yak/lbIpOhEymQSWYg6IYVTCbdmDiwe3QOjnY7txy7w9vdHtY4kRIMghXMDQa1ceOPhYAAWbkvh21/PapxICNsnhVOFB7u2Yuxf/AB4Ye2vHDpbvXucCyEqJ4VzEy8OCiD0VneulJQxdsU+LhcUax1JCJslhXMTBr2O+WHdaNPCkYzLV5jweQKlZSatYwlhk6RwqsHV0Z4lET1wtNez88RvzPnuiNaRhLBJUjjVFOjlzDsjugDwUewpvkrM0DiRELZHCscM93X2ZsKdHQCYti6Z5IwcjRMJYVukcMz07N3+/DXQg6JSE+NW7ONifpHWkYSwGVI4ZtLrFN57tCt+7k05m3OVZ1YlUCKTyEJUixRODbg0sWNJRA+aORjYc+oS//72kNaRhLAJUjg11MHDifce7QrAf3adZs3edG0DCWEDpHBq4e7bPHl2oD8AL68/QGLaZY0TCWHdpHBqaeJfO3DPbZ4Ul5kYvzKe87lXtY4khNUyq3BmzpyJoigVlsDAwBuOX7p0KaGhoTRv3pzmzZszcOBA9uzZU+vQ1kSnU3j30a7c6tGMc7lFjF8ZT1FpmdaxhLBKZp/hdOrUiczMzPIlNjb2hmO3bdtGWFgYP/30E7t27cLX15d77rmHM2fO1Cq0tWnmYGBJRAhORgMJadnM+Pqg3M1TiEoYzN7AYMDLy6taY1etWlXh3x999BHr1q1j69atRERE3HC7oqIiior+eH9Lbq71f0r7FvemzAvrxt+X7SV6bzpBrVx4vHdbrWMJYVXMPsM5fvw4Pj4++Pn5ER4eTlpaWrW3LSwspKSkhBYtWlQ5LioqChcXl/LF19fX3JiauDPAgxcGBQAwc8NB9py6pHEiIayLoppx7v/dd9+Rn59PQEAAmZmZzJo1izNnznDgwAGcnJxuuv0zzzzD5s2bOXjwIEaj8YbjKjvD8fX1JScnB2dn5+rG1YSqqkz4PJGNv2bi3syebyb2x9ulidaxhKg3ubm5uLi4VOv1aVbh/Fl2djZt27bl3XffZcyYMVWOnTNnDm+++Sbbtm0jODjYrP2Yc0DWoLC4lIc+jONIVh7BrV1YM64PRju91rGEqBfmvD5r9WdxV1dX/P39OXHiRJXj3n77bebMmcP3339vdtnYIkd7A0sjQnB1tOPXjBz+76sDMoksBLUsnPz8fFJSUvD29r7hmDfffJPZs2ezadMmQkJCarM7m+LbwpEFo7qjU2BdQgbL4lK1jiSE5swqnKlTp7J9+3ZSU1OJi4tj+PDh6PV6wsLCAIiIiGD69Onl49944w1eeeUVPvnkE9q1a0dWVhZZWVnk5+fX7VFYqX4d3HlpcEcA/r3xMHEpFzVOJIS2zCqcjIwMwsLCCAgIYOTIkbi5ubF7925atmwJQFpaGpmZmeXjFy5cSHFxMY888gje3t7ly9tvv123R2HFxvS/heHdWlFmUolclUD6pUKtIwmhmVpNGluKrU0a/9nVkjIeWRTHgTO53ObtzLqn+9LEXiaRRcNgsUljUT1GOz2LR4fg1tSeQ5m5/HPdrzKJLBolKRwLaeXahA/Du2PQKWzYf5alO05qHUkIi5PCsaBefm78a+htAMz57gg/H7ugcSIhLEsKx8JG927LyJDWmFSY+Hkip38r0DqSEBYjhWNhiqLw6oNBdPV1JedKCWOXx1NQVKp1LCEsQgpHA0Y7PYse70FLJweOnsvj+TX7ZRJZNApSOBrxcjGy6PHu2OkVNh3MYsFPVX88RIiGQApHQz3atuDVB4MAeGfLMbYePqdxIiHqlxSOxsJ6tiG8VxtUFaZEJ5FyoXF87EM0TlI4VmDG0E7c3q45eUWl/GP5PnKvlmgdSYh6IYVjBewNOj4M74GXs5GTFwp4bnUSJpNMIouGRwrHSrR0cmDx6B7YG3T8cPg87289rnUkIeqcFI4V6eLrStTwzgDM23qcTQeyNE4kRN2SwrEyD/dozd/6tQPg+TVJHDuXp20gIeqQFI4VemlwR/r4uVFQXMbY5fvIKZRJZNEwSOFYITu9jg9GdaOVaxNSfytkUnQiZTKJLBoAKRwr5dbs2iSy0U7H9mMXePv7o1pHEqLWpHCsWFArF954+NpdLhZuS+HbX89qnEiI2pHCsXIPdm3FuL/4AfDC2l85dNb6b3ssxI1I4diAF+8NJPRWd66UlDF2xT4uFxRrHUmIGpHCsQF6ncL8sG60aeFIxuUrTPg8gdIyk9axhDCbFI6NcHW0Z0lEDxzt9ew88RtR3x3ROpIQZpPCsSGBXs68M6ILAB/HnuLLhAyNEwlhHikcG3NfZ28m/rUDANO+TObXjGxtAwlhBikcG/TsQH/uCvSguNTEuBXxXMgr0jqSENUihWODdDqF9x7ril/LpmTmXCVyVQIlMoksbIAUjo1yNtqxZHQIzRwM7Em9xOxvD2kdSYibksKxYR08mvH+o10BWL7rNKv3pmkbSIibkMKxcQNv8+S5u/0BeGX9QRLSLmucSIgbk8JpACbc2YFBnTwpLjMxfkU853Ovah1JiEpJ4TQAOp3COyO7cqtHM87nFTF+ZTxFpWVaxxLiOmYVzsyZM1EUpcISGBhY5TZr164lMDAQo9FI586diYmJqVVgUblmDgaWRoTgbDSQkJbNjK8Pyt08hdUx+wynU6dOZGZmli+xsbE3HBsXF0dYWBhjxowhMTGRYcOGMWzYMA4cOFCr0KJy7dybMi+sG4oC0XvTWfWLTCIL62J24RgMBry8vMoXd3f3G46dO3cu9957Ly+88AIdO3Zk9uzZdO/enQ8++KBWocWN3RHgwYuDrp11ztxwkD2nLmmcSIg/mF04x48fx8fHBz8/P8LDw0lLu/Fv0V27djFw4MAK6wYNGsSuXbuq3EdRURG5ubkVFlF94wf4MSTYm1KTyjOr4snMuaJ1JCEAMwunV69eLFu2jE2bNrFw4UJOnTpFaGgoeXmV31kgKysLT0/PCus8PT3Jyqr69idRUVG4uLiUL76+vubEbPQUReGtR4IJ9HLiYn4x41bEc7VEJpGF9swqnPvuu48RI0YQHBzMoEGDiImJITs7mzVr1tRpqOnTp5OTk1O+pKen1+njNwaO9tcmkV0d7fg1I4f/++qATCILzdXqz+Kurq74+/tz4sSJSn/u5eXFuXPnKqw7d+4cXl5eVT6ug4MDzs7OFRZhPt8WjiwY1R29TmFdQgaf7kzVOpJo5GpVOPn5+aSkpODt7V3pz/v06cPWrVsrrNuyZQt9+vSpzW6FGfp1cOelwR0BeC3mMHEnLmqcSDRmZhXO1KlT2b59O6mpqcTFxTF8+HD0ej1hYWEAREREMH369PLxkydPZtOmTbzzzjscOXKEmTNnsm/fPiZMmFC3RyGq9Pd+7XioWyvKTCqRnyWQfqlQ60iikTKrcDIyMggLCyMgIICRI0fi5ubG7t27admyJQBpaWlkZmaWj+/bty+fffYZS5YsoUuXLnzxxResX7+eoKCguj0KUSVFUXj9oc50buXC5cISxq6I50qxTCILy1NUG5hJzM3NxcXFhZycHJnPqYWz2VcYOj+W3wqKGdrFh3mPdUVRFK1jCRtnzutTPkvViPi4NuHD8O4YdArf7D/Lkp9Pah1JNDJSOI1MLz83Zgy9DYA3Nh1h+7ELGicSjYkUTiP0eO+2PBrii0mFiZ8lkHqxQOtIopGQwmmEFEXh1WGd6NbGldyrpYxdsY+ColKtY4lGQAqnkXIw6Fn0eA9aOjlw7Fw+z6/ZL+9EFvVOCqcR83Q2sujxHtjpFTYdzGLBT5W/Y1yIuiKF08j1aNuc2Q9ee1/UO1uOsfXwuZtsIUTNSeEIHuvZhsd7t0FVYUp0EikX8rWOJBooKRwBwL/u78Tt7ZqTV1TKP5bvI/dqidaRRAMkhSMAsDfo+DC8B94uRk5eKOC51UmYTDKJLOqWFI4o19LJgcWje2Bv0PHD4fO8v/W41pFEAyOFIyoIbu1K1PDOAMzbepxNB6r+dkYhzCGFI67zcI/W/L3fLQA8vyaJY+cq/wpZIcwlhSMq9dLgQPq2d6OguIx/LN9HTqFMIovak8IRlTLodXwwqjutXJtw+rdCJkYnUiaTyKKWpHDEDbVoas+SiB4Y7XT8fOwCb20+qnUkYeOkcESVOvm48OYjXQBYtD2Fb/af1TiRsGVSOOKmHujiw7gBfgC88MV+Dp2VGxOKmpHCEdXy4qBAQm9152qJibEr9nGpoFjrSMIGSeGIatHrFOaHdaOtmyMZl68w4bMESstMWscSNkYKR1Sbq6M9S0aH4GivJy7lN6K+O6J1JGFjpHCEWQK8nHh35LVJ5I9jT/FlQobGiYQtkcIRZrs3yJuJf+0AwLQvk/k1I1vbQMJmSOGIGnl2oD93BXpQXGpi3Ip4LuQVaR1J2AApHFEjOp3Ce491xa9lUzJzrhK5KoESmUQWNyGFI2rM2WjH0ogQnBwM7Em9xOxvD2kdSVg5KRxRK+1bNuP9x7qiKLB812lW703TOpKwYlI4otbu6ujJcwP9AXhl/UES0i5rnEhYKykcUSci7+zAvZ28KC4zMX5FPOdyr2odSVghKRxRJ3Q6hbdHdsHfsxnn84oYvzKeotIyrWMJKyOFI+pMMwcDS0aH4Gw0kJiWzb/WH5S7eYoKpHBEnWrn3pT5o7qjU2D1vnRW/iKTyOIPtSqcOXPmoCgKU6ZMqXLc+++/T0BAAE2aNMHX15dnn32Wq1flGr+hGuDfkhfvDQRg1oaD7Dl1SeNEwlrUuHD27t3L4sWLCQ4OrnLcZ599xrRp05gxYwaHDx/m448/ZvXq1bz00ks13bWwAeP+4sf9wd6UmlSeWRXP2ewrWkcSVqBGhZOfn094eDhLly6lefPmVY6Ni4ujX79+jBo1inbt2nHPPfcQFhbGnj17ahRY2AZFUXjzkWA6ejtzMb+YcSviuVoik8iNXY0KJzIykiFDhjBw4MCbju3bty/x8fHlBXPy5EliYmIYPHjwDbcpKioiNze3wiJsj6O9gSWje9Dc0Y7kMzm89FWyTCI3cgZzN4iOjiYhIYG9e/dWa/yoUaO4ePEi/fv3R1VVSktLGT9+fJWXVFFRUcyaNcvcaMIK+bZw5INR3Yn4ZA9fJpwhyMeFv/e/RetYQiNmneGkp6czefJkVq1ahdForNY227Zt4/XXX+fDDz8kISGBL7/8ko0bNzJ79uwbbjN9+nRycnLKl/T0dHNiCivTr4M7Lw3uCMBrMYeJO3FR40RCK4pqxjnu+vXrGT58OHq9vnxdWVkZiqKg0+koKiqq8DOA0NBQevfuzVtvvVW+buXKlYwdO5b8/Hx0upt3Xm5uLi4uLuTk5ODs7FzduMKKqKrK82v282XiGZo72rFhQn98WzhqHUvUAXNen2ad4dx1110kJyeTlJRUvoSEhBAeHk5SUtJ1ZQNQWFh4Xan8Pk6u5xsPRVF4/aHOBLd24XJhCX9btpdTFwu0jiUszKzCcXJyIigoqMLStGlT3NzcCAoKAiAiIoLp06eXbzN06FAWLlxIdHQ0p06dYsuWLbzyyisMHTq00oISDZfRTs+ix3vg4eTAifP53D9vB18nndE6lrAgsyeNbyYtLa3CGc3LL7+Moii8/PLLnDlzhpYtWzJ06FBee+21ut61sAE+rk34ZmJ/JkcnsvvkJSZHJ7Er5TdmPtAJo538AmrozJrD0YrM4TQ8ZSaVuVuPM//H46gqBHg6sSC8Gx08nLSOJsxUb3M4QtQVvU7hubv9WTWmF+7NHDh6Lo+h83fyRbzcBaIhk8IRmurbwZ3vJofSv4M7V0rKmLp2P8+v2U9hcanW0UQ9kMIRmmvp5MB//t6Tqff4o1NgXUIGD3ywk6NZeVpHE3VMCkdYBb1OYcJfb+Xzf/TG0/naX7Ee+CCW6D1p8vaJBkQKR1iVXn5uxEwKZYB/S4pKTUz7Mpkpq5PIL5JLrIZACkdYHbdmDnz65O1Muy8QvU7h66SzDJ0fy8GzOVpHE7UkhSOskk6nMH5Ae9aM642Pi5FTFwsY/mEcK3aflkssGyaFI6xaj7YtiJkcysCO124r/Mr6A0z4LJHcqyVaRxM1IIUjrJ6roz1LI0J4eUhH7PQKG5MzuX9eLL9mZGsdTZhJCkfYBEVReCrUj7Xj+9K6eRPSLhXy8MI4Pok9JZdYNkQKR9iUrr6ubJwUyr2dvCgpU3n120OMWxFPTqFcYtkCKRxhc1ya2LHw8e7MeqAT9nod3x86x+B5O+QWwzZACkfYJEVReKJvO758pi9t3Rw5k32FkYt2seTnFEwmucSyVlI4wqYFtXLh24n9y29J83rMEZ5avo9LBcVaRxOVkMIRNs/JaMf8sG68Prwz9gYdPx45z5B5O9ibKjfgszZSOKJBUBSFUb3a8HVkP/xaNiUz5yqPLdnNgp9OyCWWFZHCEQ1KR29nvpnQn4e6taLMpPLW5qM88ekeLuYXaR1NIIUjGqCmDgbeGdmFNx8JxminY8fxiwyeu4NdKb9pHa3Rk8IRDZKiKIwM8WXDhP7c6tGM83lFhH+0m/d/OEaZXGJpRgpHNGj+nk5smNCfkSGtManw/g/HGf3xL5zPvap1tEZJCkc0eE3s9bz5SBfee7QLjvZ64lJ+Y/C8Hew4fkHraI2OFI5oNIZ3a803E/sT6OXExfxiIj7Zw9ubj1JaZtI6WqMhhSMalfYtm7E+sh+jerVBVeGDn04waukvZOZc0TpaoyCFIxodo52e14d3Zn5YN5o5GNiTeonBc3fw05HzWkdr8KRwRKM1tIsP307sT1Ar5/L7nUfFHKZELrHqjRSOaNTauTdl3dN9ebJvOwAW/3ySRxfv4ky2XGLVBykc0eg5GPTMfKATix7vjpPRQEJaNoPn7mDLoXNaR2twpHCE+K97g7yJmRRKF19Xcq6U8I/l+3j1m0MUl8olVl2RwhHif/i2cGTtuD481f8WAD7ZeYoRi+JIv1SocbKGQQpHiD+xN+h4+f7b+CgiBJcmduzPyGHwvB18l5ypdTSbJ4UjxA0MvM2TmMmh9GjbnLyrpTy9KoF/fX2AqyVlWkezWVI4QlShlWsTosf25uk72gOwfNdpHl4Yx6mLBRons021Kpw5c+agKApTpkypclx2djaRkZF4e3vj4OCAv78/MTExtdm1EBZjp9fxz3sDWfa322nR1J6DZ3MZOj+WDfvPah3N5tS4cPbu3cvixYsJDg6uclxxcTF33303qampfPHFFxw9epSlS5fSqlWrmu5aCE3cEeBBzKRQet7SgvyiUiZ9nsj0L5PlEssMNSqc/Px8wsPDWbp0Kc2bN69y7CeffMKlS5dYv349/fr1o127dgwYMIAuXbrUKLAQWvJyMfLZU72Y9NcOKAp8vieNYQt2cuJ8vtbRbEKNCicyMpIhQ4YwcODAm47dsGEDffr0ITIyEk9PT4KCgnj99dcpK7vxb4WioiJyc3MrLEJYC4Nex3P3BLDi771wb+bAkaw8hs6PZV18htbRrJ7ZhRMdHU1CQgJRUVHVGn/y5Em++OILysrKiImJ4ZVXXuGdd97h3//+9w23iYqKwsXFpXzx9fU1N6YQ9a7/re7ETO5P3/ZuXCkp4/m1+5m6dj+FxaVaR7NaimrGjZnT09MJCQlhy5Yt5XM3d9xxB127duX999+vdBt/f3+uXr3KqVOn0Ov1ALz77ru89dZbZGZW/r6GoqIiior++NLr3NxcfH19ycnJwdnZubpxhbCIMpPKgp9O8P4PxzCpcKtHMxaEd8ff00nraBaRm5uLi4tLtV6fZp3hxMfHc/78ebp3747BYMBgMLB9+3bmzZuHwWCo9DLJ29sbf3//8rIB6NixI1lZWRQXV36zMgcHB5ydnSssQlgrvU5h0l238tk/euPp7MDx8/k88EEsq/emYcbv80bBrMK56667SE5OJikpqXwJCQkhPDycpKSkCqXyu379+nHixAlMpj8+j3Ls2DG8vb2xt7ev/REIYSV6+7kRMymUv/i35GqJiX+uS+bZ1UnkF8kl1u/MKhwnJyeCgoIqLE2bNsXNzY2goCAAIiIimD59evk2Tz/9NJcuXWLy5MkcO3aMjRs38vrrrxMZGVm3RyKEFXBr5sCyJ2/nxXsD0OsU1ied5YH5sRw6K3/4gHp4p3FaWlqFuRlfX182b97M3r17CQ4OZtKkSUyePJlp06bV9a6FsAo6ncIzd3Rg9djeeLsYOXmxgGEf7mTl7tON/hLLrEljrZgzKSWENblcUMzUtfvZ+t+vLx0S7E3UQ51xNtppnKzu1NuksRDCPM2b2vPREyG8PKQjBp3Cxl8zGTo/luSMHK2jaUIKR4h6pigKT4X6sXZ8H1q5NuH0b4U8vDCOZTtPNbpLLCkcISykW5vmxEwKZVAnT4rLTMz85hDjV8aTU1iidTSLkcIRwoJcHO1Y9HgPZg69DXu9js0HzzF43g4S0y5rHc0ipHCEsDBFUXiy3y2se7ovbVo4cib7CiMW7WLpzycb/CWWFI4QGunc2oVvJ/VnSLA3pSaV12IO89R/9nG5oPJ34DcEUjhCaMjZaMcHYd14bXgQ9gYdW4+cZ/C8HexLvaR1tHohhSOExhRFIbxXW9Y/0w8/96Zk5lzl0SW7+XDbCUymhnWJJYUjhJW4zceZDRP7M6yrD2UmlTc3HeXJZXu5mF90841thBSOEFakmYOB9x7typsPB2O00/HzsQsMnruD3Sd/0zpanZDCEcLKKIrCyNt92TChPx08mnE+r4hRS3cz94fjlNn4JZYUjhBWyt/TiQ0T+jGiR2tMKrz3wzEiPvmF83lXtY5WY1I4QlgxR3sDb43owrsju+Bor2fnid8YPDeW2OMXtY5WI1I4QtiAh7q3ZsOE/gR6OXExv4jRn/zCO98fpbTMdPONrYgUjhA2ooNHM9ZH9iOsZxtUFeb/eIJRH/1CVo7tXGJJ4QhhQ4x2eqIe6sy8sG40tdez59QlBs/bwbaj57WOVi1SOELYoAe6+PDtpFA6+ThzqaCYJz/dy5zvjlBi5ZdYUjhC2Khb3Juy7um+RPRpC8Ci7Sk8tmQ3Z7KvaJzsxqRwhLBhRjs9rz4YxMLw7jgZDcSfvszguTv44dA5raNVSgpHiAbgvs7ebJwYSpfWLuRcKeGp5fv497eHKC61rkssKRwhGog2bo6sHd+XMf1vAeCj2FOMWLyL9EuFGif7gxSOEA2IvUHHK/ffxtKIEFya2LE/PZvB83aw6UDlt9W2NCkcIRqgu2/zZOOk/nRv40re1VLGr0xgxtcHKCq9/nbcliSFI0QD1bq5I6vH9WHcAD8A/rPrNA8vjCP1YoFmmaRwhGjA7PQ6pt/XkU+fvJ3mjnYcOJPL/fNj+Wb/WU3ySOEI0QjcGehBzORQerZrQX5RKRM/T+Slr5K5WmLZSywpHCEaCW+XJnz2j15M/GsHFAU++yWNYQt2knIh32IZpHCEaEQMeh3P3xPA8r/3xL2ZPUey8hg6P5avEjMssn8pHCEaodBbWxIzKZQ+fm4UFpfx7Or9vLB2P4XFpfW6XykcIRopD2cjK5/qxbMD/dEpsDY+gwc/2Mmxc3n1tk8pHCEaMb1OYfLAW1n1VG88nBw4fj6fBz6IZc2+9Hq5C6gUjhCCPu3diJkcSuit7lwtMfHiF7/y3Jr9FBTV7SVWrQpnzpw5KIrClClTqjU+OjoaRVEYNmxYbXYrhKgH7s0c+M/fevLCoAD0OoWvEs/w8MK4Ov0a0xoXzt69e1m8eDHBwcHVGp+amsrUqVMJDQ2t6S6FEPVMp1OIvLMD0WN74+VsJLxXGwz6ursQqtEj5efnEx4eztKlS2nevPlNx5eVlREeHs6sWbPw8/OryS6FEBZ0e7sWfP/cX3i8d9s6fdwaFU5kZCRDhgxh4MCB1Rr/6quv4uHhwZgxY6o1vqioiNzc3AqLEMKynI12KIpSp49pMHeD6OhoEhIS2Lt3b7XGx8bG8vHHH5OUlFTtfURFRTFr1ixzowkhrJxZZzjp6elMnjyZVatWYTQabzo+Ly+P0aNHs3TpUtzd3au9n+nTp5OTk1O+pKenmxNTCGGlFNWMP7avX7+e4cOHo9fry9eVlZWhKAo6nY6ioqIKP0tKSqJbt24V1plM12a8dTodR48epX379jfdb25uLi4uLuTk5ODs7FzduEIICzDn9WnWJdVdd91FcnJyhXV/+9vfCAwM5J///GeFYgEIDAy8bvzLL79MXl4ec+fOxdfX15zdCyFsnFmF4+TkRFBQUIV1TZs2xc3NrXx9REQErVq1IioqCqPReN14V1dXgOvWCyEaPrMnjW8mLS0NnU7ewCyEuJ5ZczhaycnJwdXVlfT0dJnDEcLK5Obm4uvrS3Z2Ni4uLlWOrfMznPqQl3ft06sy5yOE9crLy7tp4djEGY7JZOLs2bM4OTlV+Uak35u2IZ0JyTHZhsZ8TKqqkpeXh4+Pz02nU2ziDEen09G6detqj3d2dm4wT/rv5JhsQ2M9ppud2fxOZneFEBYjhSOEsJgGVTgODg7MmDEDBwcHraPUGTkm2yDHVD02MWkshGgYGtQZjhDCuknhCCEsRgpHCGExUjhCCIuRwhFCWIzNFc6CBQto164dRqORXr16sWfPnirHr127lsDAQIxGI507dyYmJsZCSavPnGNatmwZiqJUWKrz7YuW9PPPPzN06FB8fHxQFIX169ffdJtt27bRvXt3HBwc6NChA8uWLav3nNVl7vFs27btuudIURSysrIsE7gaoqKiuP3223FycsLDw4Nhw4Zx9OjRm25X29eTTRXO6tWree6555gxYwYJCQl06dKFQYMGcf78+UrHx8XFERYWxpgxY0hMTGTYsGEMGzaMAwcOWDj5jZl7THDtreaZmZnly+nTpy2Y+OYKCgro0qULCxYsqNb4U6dOMWTIEO68806SkpKYMmUKTz31FJs3b67npNVj7vH87ujRoxWeJw8Pj3pKaL7t27cTGRnJ7t272bJlCyUlJdxzzz0UFBTccJs6eT2pNqRnz55qZGRk+b/LyspUHx8fNSoqqtLxI0eOVIcMGVJhXa9evdRx48bVa05zmHtMn376qeri4mKhdLUHqF999VWVY1588UW1U6dOFdY9+uij6qBBg+oxWc1U53h++uknFVAvX75skUx14fz58yqgbt++/YZj6uL1ZDNnOMXFxcTHx1e4NY1Op2PgwIHs2rWr0m127dp13a1sBg0adMPxllaTY4Jr9wVr27Ytvr6+PPjggxw8eNASceuNtT9PNdW1a1e8vb25++672blzp9ZxqpSTkwNAixYtbjimLp4nmymcixcvUlZWhqenZ4X1np6eN7w2zsrKMmu8pdXkmAICAvjkk0/4+uuvWblyJSaTib59+5KRkWGJyPXiRs9Tbm4uV65c0ShVzXl7e7No0SLWrVvHunXr8PX15Y477iAhIUHraJUymUxMmTKFfv36VfnVv3XxerKJr6cQf+jTpw99+vQp/3ffvn3p2LEjixcvZvbs2RomE78LCAggICCg/N99+/YlJSWF9957jxUrVmiYrHKRkZEcOHCA2NjYet+XzZzhuLu7o9frOXfuXIX1586dw8vLq9JtvLy8zBpvaTU5pj+zs7OjW7dunDhxoj4iWsSNnidnZ2eaNGmiUaq61bNnT6t8jiZMmMC3337LTz/9dNPvnKqL15PNFI69vT09evRg69at5etMJhNbt26t8Bv/f/Xp06fCeIAtW7bccLyl1eSY/qysrIzk5GS8vb3rK2a9s/bnqS4kJSVZ1XOkqioTJkzgq6++4scff+SWW2656TZ18jzVdFZbC9HR0aqDg4O6bNky9dChQ+rYsWNVV1dXNSsrS1VVVR09erQ6bdq08vE7d+5UDQaD+vbbb6uHDx9WZ8yYodrZ2anJyclaHcJ1zD2mWbNmqZs3b1ZTUlLU+Ph49bHHHlONRqN68OBBrQ7hOnl5eWpiYqKamJioAuq7776rJiYmqqdPn1ZVVVWnTZumjh49unz8yZMnVUdHR/WFF15QDx8+rC5YsEDV6/Xqpk2btDqECsw9nvfee09dv369evz4cTU5OVmdPHmyqtPp1B9++EGrQ7jO008/rbq4uKjbtm1TMzMzy5fCwsLyMfXxerKpwlFVVZ0/f77apk0b1d7eXu3Zs6e6e/fu8p8NGDBAfeKJJyqMX7Nmjerv76/a29urnTp1Ujdu3GjhxDdnzjFNmTKlfKynp6c6ePBgNSEhQYPUN/b7n4X/vPx+HE888YQ6YMCA67bp2rWram9vr/r5+amffvqpxXPfiLnH88Ybb6jt27dXjUaj2qJFC/WOO+5Qf/zxR23C30BlxwNU+H+vj9eTfB+OEMJibGYORwhh+6RwhBAWI4UjhLAYKRwhhMVI4QghLEYKRwhhMVI4QgiLkcIRQliMFI4QwmKkcIQQFiOFI4SwmP8Hh5DWFhXPcAkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3fcbd5-f093-41b2-b4a1-8ab60b3f8036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([1, 4])\n",
            "len(conf_scores_final) 2 torch.Size([1])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n",
            "tensor([0], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4MFONZWvSY",
        "outputId": "ef141267-edd7-4bff-894e-e9aec0520c14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([[  1.6569, -14.6462,   7.2067,  10.3496]], device='cuda:0'),\n",
              "  tensor([], device='cuda:0', size=(0, 4))],\n",
              " [tensor([0.9939], device='cuda:0'), tensor([], device='cuda:0')],\n",
              " [tensor([0], device='cuda:0'),\n",
              "  tensor([], device='cuda:0', dtype=torch.int64)])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "l_f0DiBDEq1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "3b830468-f98d-4cb8-abfb-fcc200a6acec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2367\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2231\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2233\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    455\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    459\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Acquire a lock on the shared font cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;31m# Early return in the simple, non-deprecated case (much faster than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# calling bind()).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_varargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mreuse_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreuse_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 285791x5478 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAH99PhbTjI"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all.cpu(), fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0].cpu(), fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1].cpu(), fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivNrzM_PbViM"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "filters_data =[filters[0].cpu().detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}