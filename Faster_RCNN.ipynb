{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f7b7872-8e01-46ed-e2d6-b2eb0fcd5503"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19580bf-07b1-49a8-a205-8e17352802be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paye0SiEAuJu",
        "outputId": "ee70c8db-d230-47b5-964b-9131a2165f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([643, 4])\n",
            "len(conf_scores_final) 2 torch.Size([643])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([[-2.0668e+01, -3.4030e+01, -2.0613e+01, -3.3996e+01],\n",
              "          [-4.1951e+09, -9.8031e+04,  4.1951e+09,  9.8034e+04],\n",
              "          [-4.6926e+01,  8.6073e+00,  1.1034e+02,  8.6087e+00],\n",
              "          ...,\n",
              "          [ 4.7423e+01, -3.2506e+05,  4.7505e+01,  3.2511e+05],\n",
              "          [-2.0250e+00,  8.1543e+00, -2.0121e+00,  8.1543e+00],\n",
              "          [ 4.1813e+01, -1.1538e+04,  4.3409e+01,  1.1598e+04]], device='cuda:0'),\n",
              "  tensor([[ 1.7667e+01,  3.0368e+01,  1.7782e+01,  3.0976e+01],\n",
              "          [-7.2950e+02, -5.4807e+01,  7.2328e+02,  2.4694e+01],\n",
              "          [ 1.2647e-01,  7.8142e+00,  3.1565e-01,  1.1400e+01],\n",
              "          [-2.9028e+00,  1.5035e+01, -1.6415e+00,  1.5246e+01],\n",
              "          [-2.8460e+01,  8.9033e+00, -1.9654e+01,  9.0732e+00],\n",
              "          [-1.1415e+01,  3.8098e+01,  9.1787e+00,  4.2667e+01],\n",
              "          [ 3.7426e+00, -3.3385e+01,  3.7857e+00,  6.9326e+01],\n",
              "          [-3.0673e+01, -6.7821e+01, -3.0673e+01, -6.7821e+01],\n",
              "          [-4.4646e+13, -1.3104e+08,  4.4646e+13,  1.3104e+08],\n",
              "          [ 1.1632e+01, -7.7050e+00,  1.6900e+01, -7.6009e+00],\n",
              "          [ 1.1311e+01,  3.5595e+00,  1.1312e+01,  3.6628e+00],\n",
              "          [ 1.4796e+01, -6.3720e-01,  1.5060e+01,  3.1169e+00],\n",
              "          [ 1.1769e+01,  3.3335e+00,  1.2117e+01,  4.5897e+00],\n",
              "          [ 1.2750e+01, -9.9974e+00,  1.2798e+01,  2.9004e+01],\n",
              "          [-6.0894e+00,  4.1332e+00, -6.0445e+00,  5.8068e+00],\n",
              "          [ 9.1152e+00, -6.4323e+00,  9.8669e+00,  1.6803e+01],\n",
              "          [ 2.9149e+01,  2.1211e+01,  2.9149e+01,  2.1211e+01],\n",
              "          [ 2.9241e+01, -5.2837e+02,  3.5728e+01,  5.3381e+02],\n",
              "          [-2.4802e+01, -4.2681e+00,  5.5428e+00, -4.1429e+00],\n",
              "          [ 2.9961e+00,  6.9818e+00,  3.3834e+00,  1.7929e+01],\n",
              "          [-1.5808e+03, -1.1249e+03,  1.5913e+03,  1.1875e+03],\n",
              "          [-5.4888e+00,  1.5064e+01,  6.0927e+00,  1.7737e+01],\n",
              "          [-5.1762e+02,  8.9261e+00,  5.2977e+02,  9.6018e+00],\n",
              "          [ 5.0466e-01,  1.3981e+01,  3.2348e+00,  1.3984e+01],\n",
              "          [ 1.1833e+01,  1.0295e+01,  1.2263e+01,  1.0295e+01],\n",
              "          [ 1.2955e+01, -2.2226e+01,  1.8564e+01,  6.9850e+01],\n",
              "          [ 3.8675e+00,  4.3515e+00,  8.9589e+00,  4.3894e+00],\n",
              "          [ 4.3802e+00,  6.7682e+00,  1.8588e+01,  7.2488e+00],\n",
              "          [ 2.0854e+01, -9.0795e+03,  2.1179e+01,  9.1397e+03],\n",
              "          [-1.0400e+02, -6.3584e+01, -1.0393e+02, -6.3561e+01],\n",
              "          [ 2.9688e-01, -1.0808e+02,  2.0235e+00,  1.2545e+02],\n",
              "          [ 9.1811e+00,  2.2531e+01,  1.3288e+01,  2.6024e+01],\n",
              "          [-2.5732e+04, -2.2099e+01,  2.5838e+04,  1.0205e+01],\n",
              "          [ 8.2432e+00,  6.4182e+00,  9.1248e+00,  1.2583e+01],\n",
              "          [ 3.1628e+01,  2.5201e+01,  3.5751e+01,  2.8543e+01],\n",
              "          [ 1.1382e+01,  1.0046e+01,  1.2444e+01,  1.0235e+01],\n",
              "          [-2.9848e+01, -1.9403e+01, -2.9847e+01, -1.9392e+01],\n",
              "          [-2.2371e+02,  1.7471e+01,  2.1081e+02,  2.2182e+01],\n",
              "          [-7.0861e+07, -8.3058e+05,  7.0861e+07,  8.3065e+05],\n",
              "          [ 1.3601e+01,  1.5716e+01,  1.4137e+01,  1.5936e+01],\n",
              "          [-1.4506e+00,  5.3128e+00,  6.8015e+00,  6.3786e+00],\n",
              "          [ 6.3091e+01,  3.4393e+00,  6.8016e+01,  7.3544e+00],\n",
              "          [-7.5213e+04, -7.4523e+06,  7.5195e+04,  7.4523e+06],\n",
              "          [ 2.1360e+01,  9.5335e+00,  2.1723e+01,  9.6820e+00],\n",
              "          [ 1.5825e+00,  6.7618e+00,  1.7733e+00,  7.0780e+00],\n",
              "          [-3.9806e+00,  1.3576e+01, -2.5170e+00,  1.6009e+01],\n",
              "          [-6.2947e+00, -2.5511e+04, -1.3290e+00,  2.5562e+04],\n",
              "          [-4.8984e+08, -2.5386e+03,  4.8984e+08,  2.6446e+03],\n",
              "          [-4.1642e+01, -3.2786e+01, -4.1641e+01, -2.9801e+01],\n",
              "          [ 1.1396e+01,  1.2346e+01,  1.2130e+01,  1.3147e+01],\n",
              "          [ 1.3474e+01,  1.3019e+01,  1.4073e+01,  1.4534e+01],\n",
              "          [-1.2026e+00, -4.8078e+03,  1.0589e+01,  4.8455e+03],\n",
              "          [ 1.4504e+00,  8.4582e+00,  1.4574e+00,  8.4857e+00],\n",
              "          [-2.9266e+02, -7.9362e+00,  3.3112e+02, -7.7626e+00],\n",
              "          [-5.3734e+00, -2.4660e+00, -4.8798e+00,  8.2710e+00],\n",
              "          [ 1.6106e+01,  1.6206e+00,  1.6278e+01,  7.9318e+00],\n",
              "          [ 1.5804e+01,  1.1431e+01,  1.7885e+01,  2.1993e+01],\n",
              "          [ 3.8319e+00, -6.3704e+00,  4.4625e+00,  2.1464e+01],\n",
              "          [ 3.0544e+00,  1.9627e+01,  3.0642e+00,  2.6837e+01],\n",
              "          [ 1.5234e+01,  7.2946e+00,  1.5380e+01,  7.6490e+00],\n",
              "          [-1.2963e+01, -6.1972e+00,  1.5015e+01,  3.8826e+00],\n",
              "          [ 1.7595e+01,  8.1478e+00,  2.1517e+01,  8.5370e+00],\n",
              "          [ 5.7262e-01, -1.3475e+01,  2.0233e+00, -1.3101e+01],\n",
              "          [-8.5480e+00, -7.9741e+00, -8.1894e+00, -7.5344e+00],\n",
              "          [ 7.8845e+00,  2.1747e+00,  8.4747e+00,  8.0599e+00],\n",
              "          [ 4.4426e+00, -6.5983e+01,  9.6319e+00, -9.4349e+00],\n",
              "          [ 1.0297e+01, -2.0236e+04,  2.0065e+01,  2.0227e+04],\n",
              "          [-1.1212e+01, -1.2291e+04, -1.1212e+01,  1.2346e+04],\n",
              "          [ 5.6474e+00,  3.2735e+01,  6.3191e+00,  3.2989e+01],\n",
              "          [-5.9081e+05, -1.2892e+05,  5.9104e+05,  1.2908e+05],\n",
              "          [-4.4551e+03,  8.5678e+00,  4.4786e+03,  8.6759e+00],\n",
              "          [ 2.2897e+01,  2.0919e+01,  2.3458e+01,  2.2282e+01],\n",
              "          [ 3.0192e+00,  1.2879e+01,  3.2515e+00,  1.3326e+01],\n",
              "          [ 9.5184e+01,  3.3590e+01,  9.7838e+01,  5.7398e+01],\n",
              "          [ 3.1566e+01, -1.0958e+01,  3.7946e+01,  3.3064e+01],\n",
              "          [ 1.5099e+01,  1.1674e+01,  1.6002e+01,  1.3451e+01],\n",
              "          [-9.2339e-01,  2.4427e+00, -6.6391e-01,  2.6058e+00],\n",
              "          [-8.8662e+00,  6.3563e+00, -8.4167e+00,  6.3719e+00],\n",
              "          [ 1.1888e+01,  5.2528e+00,  1.2482e+01,  5.3372e+00],\n",
              "          [ 2.9834e+00, -2.4611e+01,  2.9892e+00, -2.4606e+01],\n",
              "          [-4.0051e+00, -4.3501e+05,  7.4283e+01,  4.3507e+05],\n",
              "          [ 6.1211e+00, -7.5305e+01,  1.2635e+01, -6.2244e+01],\n",
              "          [-2.4075e+00,  1.2803e+01, -1.8738e+00,  1.3564e+01],\n",
              "          [-3.2435e+01, -1.7651e+01, -3.2022e+01, -1.7646e+01],\n",
              "          [-8.7477e+02,  2.5008e+01,  9.1730e+02,  2.8357e+01],\n",
              "          [ 4.6480e+00, -9.1007e+03,  4.6529e+00,  9.1500e+03],\n",
              "          [ 1.6325e+01, -4.1144e+02,  1.6514e+01,  4.2381e+02],\n",
              "          [-6.5883e+01, -3.4739e+01,  8.6496e+01, -3.4206e+01],\n",
              "          [-3.6826e+01, -4.0435e+01, -3.6313e+01, -3.2418e+01],\n",
              "          [ 1.1465e+01,  1.7515e+01,  1.3738e+01,  2.1789e+01],\n",
              "          [ 3.9121e+00,  3.1228e+00,  2.2325e+01,  3.5548e+00],\n",
              "          [ 9.8252e+00,  2.0451e+01,  3.8704e+01,  2.0964e+01],\n",
              "          [ 4.3140e+00, -9.6546e+00,  4.3141e+00, -9.6064e+00],\n",
              "          [ 7.4078e+00,  1.6642e+01,  7.6381e+00,  1.7103e+01],\n",
              "          [ 1.5635e+01,  2.3466e+01,  1.6877e+01,  2.4082e+01],\n",
              "          [ 1.3338e+01,  1.0100e+01,  1.3367e+01,  1.0591e+01],\n",
              "          [-1.0896e+01,  3.5899e+00,  3.9553e-01,  3.5987e+00],\n",
              "          [-6.0038e+01, -4.8662e+01, -5.9676e+01, -4.8662e+01],\n",
              "          [-6.7372e+01,  1.4598e+01,  6.7681e+01,  1.5043e+01],\n",
              "          [ 5.8043e+00,  9.7045e+00,  6.1056e+00,  1.4151e+01],\n",
              "          [-7.8377e+00, -1.2974e+01, -6.6006e+00, -4.9677e+00],\n",
              "          [-2.9547e+01,  2.5651e+01,  1.1091e+01,  3.4275e+01],\n",
              "          [-4.1009e+00, -1.7731e+11, -4.0953e+00,  1.7731e+11],\n",
              "          [ 2.2094e+00, -1.6050e+00,  2.2137e+00, -1.5941e+00],\n",
              "          [ 1.6906e+00, -6.4793e+00,  2.1003e+00,  1.9046e+00],\n",
              "          [ 4.4783e+00, -4.5246e+00,  5.5643e+00,  2.3631e+01],\n",
              "          [-9.6763e+00, -1.2578e+01, -9.6651e+00,  9.5823e+00],\n",
              "          [ 1.2386e+01,  1.4878e+01,  1.2402e+01,  1.5173e+01],\n",
              "          [ 2.1069e+01, -4.7804e+00,  2.9738e+01, -4.6506e+00],\n",
              "          [-6.4301e+00,  2.4406e+01, -6.3230e+00,  2.4406e+01],\n",
              "          [ 1.3237e+01,  1.7819e+00,  1.4723e+01,  3.2485e+00],\n",
              "          [ 9.0829e+00, -4.7074e+00,  9.0837e+00, -4.2747e+00],\n",
              "          [-8.8765e+01, -3.9072e+01, -8.8765e+01, -3.9072e+01],\n",
              "          [-7.6206e+05, -9.8365e+02,  7.6211e+05,  1.0586e+03],\n",
              "          [-5.3833e+00, -2.9255e+03, -3.2611e+00,  3.0209e+03],\n",
              "          [ 1.3924e+01, -3.1277e+00,  1.3926e+01, -2.3031e+00],\n",
              "          [-6.1520e-01,  4.7703e+00, -5.8669e-01,  5.4966e+00],\n",
              "          [-4.6445e+01, -9.7914e+00, -4.6436e+01, -9.7860e+00],\n",
              "          [ 4.5125e+00, -1.2386e+00,  4.7920e+00, -1.1722e+00],\n",
              "          [-8.0745e+01,  2.2197e+01,  7.2124e+01,  2.2199e+01],\n",
              "          [-2.5162e+02,  3.2550e+01,  3.1327e+02,  3.2635e+01],\n",
              "          [ 4.1523e+01,  2.7465e+01,  4.3016e+01,  4.0246e+01],\n",
              "          [ 3.2229e+00,  2.3003e-01,  5.2593e+00,  1.1620e+00],\n",
              "          [ 1.4596e+01, -8.1767e+00,  1.8097e+01, -5.8439e+00],\n",
              "          [ 8.1248e+00,  2.9296e+01,  1.0059e+01,  3.4948e+01],\n",
              "          [-5.3699e+04,  2.9386e+00,  5.3740e+04,  2.9401e+00],\n",
              "          [ 1.1337e+01,  2.2957e+01,  1.1548e+01,  2.3885e+01],\n",
              "          [-6.0529e+00, -1.5478e+01, -5.7139e+00, -8.6880e+00],\n",
              "          [ 3.3241e+00, -7.5671e+00,  3.7698e+00, -5.9900e+00],\n",
              "          [ 7.6305e+00,  1.4218e+01,  7.8003e+00,  1.4296e+01],\n",
              "          [-1.3104e+01, -1.2450e+01, -1.0644e+01, -5.5157e+00],\n",
              "          [ 6.4853e+00, -2.1120e+01,  6.6359e+00,  1.3809e+01],\n",
              "          [ 1.3340e+01, -1.1432e+02,  1.3589e+01, -2.2184e-01]], device='cuda:0')],\n",
              " [tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
              "          0.9996, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9994, 0.9994, 0.9994,\n",
              "          0.9994, 0.9994, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9993, 0.9993,\n",
              "          0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9991, 0.9991, 0.9991, 0.9991,\n",
              "          0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9990, 0.9990, 0.9989,\n",
              "          0.9989, 0.9989, 0.9989, 0.9988, 0.9987, 0.9987, 0.9987, 0.9986, 0.9986,\n",
              "          0.9986, 0.9985, 0.9985, 0.9985, 0.9984, 0.9984, 0.9984, 0.9984, 0.9983,\n",
              "          0.9983, 0.9983, 0.9983, 0.9983, 0.9982, 0.9981, 0.9981, 0.9981, 0.9981,\n",
              "          0.9980, 0.9979, 0.9978, 0.9977, 0.9977, 0.9975, 0.9975, 0.9974, 0.9973,\n",
              "          0.9973, 0.9973, 0.9972, 0.9971, 0.9970, 0.9970, 0.9969, 0.9969, 0.9969,\n",
              "          0.9969, 0.9968, 0.9968, 0.9968, 0.9967, 0.9967, 0.9966, 0.9965, 0.9964,\n",
              "          0.9961, 0.9960, 0.9959, 0.9958, 0.9958, 0.9957, 0.9954, 0.9953, 0.9952,\n",
              "          0.9950, 0.9950, 0.9950, 0.9949, 0.9949, 0.9948, 0.9948, 0.9947, 0.9946,\n",
              "          0.9942, 0.9941, 0.9940, 0.9937, 0.9934, 0.9933, 0.9933, 0.9932, 0.9932,\n",
              "          0.9931, 0.9928, 0.9927, 0.9923, 0.9922, 0.9921, 0.9920, 0.9918, 0.9918,\n",
              "          0.9916, 0.9914, 0.9914, 0.9912, 0.9911, 0.9910, 0.9907, 0.9905, 0.9904,\n",
              "          0.9904, 0.9903, 0.9902, 0.9901], device='cuda:0'),\n",
              "  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9997, 0.9996, 0.9996, 0.9996, 0.9994, 0.9993,\n",
              "          0.9992, 0.9991, 0.9991, 0.9990, 0.9990, 0.9989, 0.9989, 0.9989, 0.9987,\n",
              "          0.9987, 0.9985, 0.9984, 0.9984, 0.9984, 0.9983, 0.9983, 0.9981, 0.9981,\n",
              "          0.9980, 0.9980, 0.9980, 0.9979, 0.9978, 0.9978, 0.9978, 0.9976, 0.9975,\n",
              "          0.9975, 0.9975, 0.9971, 0.9970, 0.9970, 0.9967, 0.9967, 0.9966, 0.9965,\n",
              "          0.9964, 0.9964, 0.9960, 0.9948, 0.9947, 0.9946, 0.9944, 0.9941, 0.9939,\n",
              "          0.9935, 0.9933, 0.9932, 0.9927, 0.9927, 0.9925, 0.9923, 0.9920, 0.9911,\n",
              "          0.9910, 0.9910, 0.9909, 0.9909, 0.9907, 0.9906, 0.9903],\n",
              "         device='cuda:0')],\n",
              " [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         device='cuda:0'),\n",
              "  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        # return proposals_final.to(device), conf_scores_final.to(device), feature_map.to(device), [out_c, out_h, out_w]\n",
        "        # return torch.Tensor(proposals_final), torch.Tensor(conf_scores_final), torch.Tensor(feature_map), [out_c, out_h, out_w]\n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        print('len(proposals_final)', len(proposals_final), proposals_final[0].shape)\n",
        "        print('len(conf_scores_final)', len(conf_scores_final), conf_scores_final[0].shape)\n",
        "        print('len(feature_map)', len(feature_map), feature_map[0].shape)\n",
        "        print(out_size)\n",
        "        # print(conf_scores_final)\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        # print(classes_all)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "d = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "\n",
        "d.eval()\n",
        "proposals_final, conf_scores_final, classes_final = d.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "# proposals_final.shape, conf_scores_final.shape, classes_final.shape\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "7fca39a7-b019-4c94-f3ed-874106b8a5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFnKE3neESZ5"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2nsbro-Tse",
        "outputId": "ec61a50f-5fe5-4c46-9be7-fc1905d822f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.438581808371478, 6.690241173115743]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "n_dOW6SREUn2",
        "outputId": "4cd58ac0-5672-44ee-aff9-c700675cceea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd9ce514850>]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ccc72d-f27e-4028-f13d-0be9413fc917"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([100, 4])\n",
            "len(conf_scores_final) 2 torch.Size([100])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n"
          ]
        }
      ],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "l_f0DiBDEq1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "a7400cf8-7872-4098-e441-601b470f2c3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2367\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2231\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2233\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    455\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    459\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Acquire a lock on the shared font cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;31m# Early return in the simple, non-deprecated case (much faster than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# calling bind()).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_varargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mreuse_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreuse_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 849124642x0 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAH99PhbTjI"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all.cpu(), fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0].cpu(), fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1].cpu(), fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivNrzM_PbViM"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "filters_data =[filters[0].cpu().detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}