{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dff215a2-657f-4f9d-9d98-c8c82c16dfb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19c0d8b-fbf4-4036-d399-e238d912faf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "Paye0SiEAuJu",
        "outputId": "40553b91-0fd0-4de5-d6c6-64e496bf29cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-b52f85ee23c9>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mproposals_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_scores_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;31m# proposals_final.shape, conf_scores_final.shape, classes_final.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_data_all' is not defined"
          ]
        }
      ],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        # return proposals_final.to(device), conf_scores_final.to(device), feature_map.to(device), [out_c, out_h, out_w]\n",
        "        # return torch.Tensor(proposals_final), torch.Tensor(conf_scores_final), torch.Tensor(feature_map), [out_c, out_h, out_w]\n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        print('len(proposals_final)', len(proposals_final), proposals_final[0].shape)\n",
        "        print('len(conf_scores_final)', len(conf_scores_final), conf_scores_final[0].shape)\n",
        "        print('len(feature_map)', len(feature_map), feature_map[0].shape)\n",
        "        print(out_size)\n",
        "        # print(conf_scores_final)\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        print(classes_all)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "d = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "\n",
        "d.eval()\n",
        "proposals_final, conf_scores_final, classes_final = d.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "# proposals_final.shape, conf_scores_final.shape, classes_final.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "31185af0-a53b-4fc0-c3a2-c81de247bb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 238MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            print(total_loss, loss.item())\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnKE3neESZ5",
        "outputId": "48c4bdc3-16dc-46c7-ee48-822c676513df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.5908944606781006\n",
            "1.5908944606781006 4.010169982910156\n",
            "5.601064443588257 2.661823034286499\n",
            "8.262887477874756 3.3783044815063477\n",
            "11.641191959381104 1.623331069946289\n",
            "13.264523029327393 0.7983145713806152\n",
            "14.062837600708008 1.0751638412475586\n",
            "15.138001441955566 4.604476451873779\n",
            "19.742477893829346 1.4907557964324951\n",
            "21.23323369026184 2.542330741882324\n",
            "23.775564432144165 3.8937060832977295\n",
            "27.669270515441895 2.0793116092681885\n",
            "29.748582124710083 2.64054536819458\n",
            "32.38912749290466 1.7206275463104248\n",
            "34.10975503921509 2.0246775150299072\n",
            "36.134432554244995 1.7908580303192139\n",
            "37.92529058456421 2.0264363288879395\n",
            "39.95172691345215 1.7226260900497437\n",
            "41.67435300350189 64.1946792602539\n",
            "105.8690322637558 2.1890029907226562\n",
            "108.05803525447845 1.6450510025024414\n",
            "109.7030862569809 6.985363960266113\n",
            "116.68845021724701 3.3131580352783203\n",
            "120.00160825252533 1.5795773267745972\n",
            "121.58118557929993 17.543630599975586\n",
            "139.1248161792755 3.1184914112091064\n",
            "142.24330759048462 2.303082227706909\n",
            "144.54638981819153 1.9613149166107178\n",
            "146.50770473480225 2.037424087524414\n",
            "148.54512882232666 1.5202782154083252\n",
            "150.06540703773499 11.81496810913086\n",
            "161.88037514686584 7.899240493774414\n",
            "169.77961564064026 0.8298619389533997\n",
            "170.60947757959366 1.7195539474487305\n",
            "172.3290315270424 2.6974189281463623\n",
            "175.02645045518875 0.7528499364852905\n",
            "175.77930039167404 2.2346811294555664\n",
            "178.0139815211296 2.3445069789886475\n",
            "180.35848850011826 2.0225369930267334\n",
            "182.381025493145 4.442741394042969\n",
            "186.82376688718796 1.6395337581634521\n",
            "188.4633006453514 5.188990592956543\n",
            "193.65229123830795 0.795928418636322\n",
            "194.44821965694427 1.823728084564209\n",
            "196.27194774150848 34.914161682128906\n",
            "231.1861094236374 3.085446834564209\n",
            "234.2715562582016 1.495361089706421\n",
            "235.76691734790802 2.1976399421691895\n",
            "237.9645572900772 28.36465835571289\n",
            "266.3292156457901 1.6429492235183716\n",
            "267.9721648693085 3.3928821086883545\n",
            "271.3650469779968 1.4704324007034302\n",
            "272.83547937870026 1.5595102310180664\n",
            "274.3949896097183 2.648141384124756\n",
            "277.0431309938431 1.6234769821166992\n",
            "278.6666079759598 2.2052555084228516\n",
            "280.87186348438263 7.608623027801514\n",
            "288.48048651218414 0.7660501599311829\n",
            "289.2465366721153 3.9582104682922363\n",
            "293.20474714040756 2.7717442512512207\n",
            "295.9764913916588 2.658884286880493\n",
            "298.6353756785393 2.229860305786133\n",
            "300.8652359843254 88.61370849609375\n",
            "389.47894448041916 3.291210651397705\n",
            "392.77015513181686 1.1664034128189087\n",
            "393.9365585446358 1.911698579788208\n",
            "395.848257124424 1.6288199424743652\n",
            "397.47707706689835 2.0612902641296387\n",
            "399.538367331028 2.250129222869873\n",
            "401.78849655389786 2.821521043777466\n",
            "404.6100175976753 45.25921630859375\n",
            "449.8692339062691 2.187248706817627\n",
            "452.0564826130867 2.24573016166687\n",
            "454.30221277475357 1.6223745346069336\n",
            "455.9245873093605 2.9458417892456055\n",
            "458.8704290986061 2.453187942504883\n",
            "461.323617041111 1.557410478591919\n",
            "462.8810275197029 1.9890482425689697\n",
            "464.8700757622719 1.892866611480713\n",
            "466.7629423737526 3.346679210662842\n",
            "470.10962158441544 3.5468521118164062\n",
            "473.65647369623184 1.741986870765686\n",
            "475.3984605669975 1.5899358987808228\n",
            "476.98839646577835 1.5667039155960083\n",
            "478.55510038137436 4.485116481781006\n",
            "483.04021686315536 1.7025806903839111\n",
            "484.7427975535393 1.4507237672805786\n",
            "486.19352132081985 2.9616284370422363\n",
            "489.1551497578621 1.8725775480270386\n",
            "491.02772730588913 0.7673464417457581\n",
            "491.7950737476349 1.8503224849700928\n",
            "493.645396232605 1.1214553117752075\n",
            "494.7668515443802 1.9280133247375488\n",
            "496.69486486911774 1.8790717124938965\n",
            "498.57393658161163 22.60344696044922\n",
            "521.1773835420609 3.9850940704345703\n",
            "525.1624776124954 12.799448013305664\n",
            "537.9619256258011 0.7800548076629639\n",
            "538.741980433464 2.8926732540130615\n",
            "541.6346536874771 1.8240342140197754\n",
            "543.4586879014969 26.26193618774414\n",
            "569.720624089241 5.468601226806641\n",
            "575.1892253160477 3.5968475341796875\n",
            "578.7860728502274 2.2920119762420654\n",
            "581.0780848264694 13.876730918884277\n",
            "594.9548157453537 2.342899799346924\n",
            "597.2977155447006 2.0840377807617188\n",
            "599.3817533254623 8.495670318603516\n",
            "607.8774236440659 0.746560275554657\n",
            "608.6239839196205 8.035188674926758\n",
            "616.6591725945473 1.7077714204788208\n",
            "618.3669440150261 0.7880604267120361\n",
            "619.1550044417381 2.4345204830169678\n",
            "621.5895249247551 1.6515692472457886\n",
            "623.2410941720009 2.725668430328369\n",
            "625.9667626023293 5.14985466003418\n",
            "631.1166172623634 2.3665854930877686\n",
            "633.4832027554512 2.42899489402771\n",
            "635.9121976494789 1.5542300939559937\n",
            "637.4664277434349 2.3692359924316406\n",
            "639.8356637358665 0.8318752646446228\n",
            "640.6675390005112 1.5783898830413818\n",
            "642.2459288835526 1.508821725845337\n",
            "643.7547506093979 2.355229616165161\n",
            "646.109980225563 0.7605929374694824\n",
            "646.8705731630325 2.2565839290618896\n",
            "649.1271570920944 2.625424385070801\n",
            "651.7525814771652 2.6221518516540527\n",
            "654.3747333288193 4.980400562286377\n",
            "659.3551338911057 2.2838845252990723\n",
            "661.6390184164047 1.7572333812713623\n",
            "663.3962517976761 2.417149066925049\n",
            "665.8134008646011 1.7396643161773682\n",
            "667.5530651807785 4.703248023986816\n",
            "672.2563132047653 5.714507102966309\n",
            "677.9708203077316 2.543118476867676\n",
            "680.5139387845993 1.5578800439834595\n",
            "682.0718188285828 3.0934109687805176\n",
            "685.1652297973633 1.7293436527252197\n",
            "686.8945734500885 1.8121685981750488\n",
            "688.7067420482635 1.72182297706604\n",
            "690.4285650253296 0.9940129518508911\n",
            "691.4225779771805 3.6340982913970947\n",
            "695.0566762685776 3.4844276905059814\n",
            "698.5411039590836 1.9348970651626587\n",
            "700.4760010242462 0.8600403666496277\n",
            "701.3360413908958 2.4224109649658203\n",
            "703.7584523558617 1.4102016687393188\n",
            "705.168654024601 2.1426260471343994\n",
            "707.3112800717354 1.6542037725448608\n",
            "708.9654838442802 1.725400447845459\n",
            "710.6908842921257 2.036914825439453\n",
            "712.7277991175652 2.7066688537597656\n",
            "715.4344679713249 0.8085840344429016\n",
            "716.2430520057678 8.313962936401367\n",
            "724.5570149421692 1.6265902519226074\n",
            "726.1836051940918 8.654291152954102\n",
            "734.8378963470459 0.759223997592926\n",
            "735.5971203446388 4.794961929321289\n",
            "740.3920822739601 20.173725128173828\n",
            "760.5658074021339 2.0869622230529785\n",
            "762.6527696251869 0.7368175387382507\n",
            "763.3895871639252 0.8157219886779785\n",
            "764.2053091526031 nan\n",
            "764.2053091526031 1.9723222255706787\n",
            "766.1776313781738 2.4081296920776367\n",
            "768.5857610702515 1.7874637842178345\n",
            "770.3732248544693 2.333674430847168\n",
            "772.7068992853165 8.271724700927734\n",
            "780.9786239862442 2.8108150959014893\n",
            "783.7894390821457 1.7335057258605957\n",
            "785.5229448080063 3.8774099349975586\n",
            "789.4003547430038 1.7614948749542236\n",
            "791.1618496179581 9.66274642944336\n",
            "800.8245960474014 1.8537179231643677\n",
            "802.6783139705658 2.8430569171905518\n",
            "805.5213708877563 2.8950319290161133\n",
            "808.4164028167725 1.8761847019195557\n",
            "810.292587518692 20.7863826751709\n",
            "831.0789701938629 4.237160682678223\n",
            "835.3161308765411 0.9223505854606628\n",
            "836.2384814620018 1.6504924297332764\n",
            "837.8889738917351 0.8157824277877808\n",
            "838.7047563195229 2.9920284748077393\n",
            "841.6967847943306 0.8934677839279175\n",
            "842.5902525782585 4.298707008361816\n",
            "846.8889595866203 1.2726047039031982\n",
            "848.1615642905235 2.7725086212158203\n",
            "850.9340729117393 1.6435964107513428\n",
            "852.5776693224907 2.595989942550659\n",
            "855.1736592650414 1.6797467470169067\n",
            "856.8534060120583 3.143824815750122\n",
            "859.9972308278084 1.570199966430664\n",
            "861.567430794239 2.3842155933380127\n",
            "863.9516463875771 nan\n",
            "863.9516463875771 3.790428400039673\n",
            "867.7420747876167 1.766746163368225\n",
            "869.508820950985 2.0028209686279297\n",
            "871.5116419196129 1.6609175205230713\n",
            "873.172559440136 4.917987823486328\n",
            "878.0905472636223 2.3712964057922363\n",
            "880.4618436694145 1.5629276037216187\n",
            "882.0247712731361 1.4844590425491333\n",
            "883.5092303156853 1.691633701324463\n",
            "885.2008640170097 0.7140474915504456\n",
            "885.9149115085602 2.34036922454834\n",
            "888.2552807331085 5.5413594245910645\n",
            "893.7966401576996 1.5358307361602783\n",
            "895.3324708938599 4.541988372802734\n",
            "899.8744592666626 3.3101918697357178\n",
            "903.1846511363983 4.897026062011719\n",
            "908.08167719841 2.3625354766845703\n",
            "910.4442126750946 1.5746381282806396\n",
            "912.0188508033752 0.7679856419563293\n",
            "912.7868364453316 1.7503913640975952\n",
            "914.5372278094292 2.009852409362793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 1/2 [02:19<02:19, 139.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "916.547080218792 2.53153133392334\n",
            "0 1.58711576461792\n",
            "1.58711576461792 3.8079543113708496\n",
            "5.3950700759887695 2.4873626232147217\n",
            "7.882432699203491 3.171142816543579\n",
            "11.05357551574707 1.5517115592956543\n",
            "12.605287075042725 0.6923950910568237\n",
            "13.297682166099548 1.0610480308532715\n",
            "14.35873019695282 4.531999588012695\n",
            "18.890729784965515 1.645462989807129\n",
            "20.536192774772644 2.6592135429382324\n",
            "23.195406317710876 3.995210647583008\n",
            "27.190616965293884 1.8822128772735596\n",
            "29.072829842567444 2.455465316772461\n",
            "31.528295159339905 1.8824933767318726\n",
            "33.41078853607178 1.923832893371582\n",
            "35.33462142944336 1.527077317237854\n",
            "36.86169874668121 1.8088724613189697\n",
            "38.67057120800018 1.764542818069458\n",
            "40.43511402606964 61.50928497314453\n",
            "101.94439899921417 2.037142753601074\n",
            "103.98154175281525 1.6058945655822754\n",
            "105.58743631839752 6.618464469909668\n",
            "112.20590078830719 3.379141092300415\n",
            "115.5850418806076 1.6275869607925415\n",
            "117.21262884140015 18.089092254638672\n",
            "135.30172109603882 2.9658570289611816\n",
            "138.267578125 2.7896945476531982\n",
            "141.0572726726532 1.8519339561462402\n",
            "142.90920662879944 2.4458236694335938\n",
            "145.35503029823303 1.616173267364502\n",
            "146.97120356559753 11.424924850463867\n",
            "158.3961284160614 7.123771667480469\n",
            "165.51990008354187 0.8594381809234619\n",
            "166.37933826446533 1.7510826587677002\n",
            "168.13042092323303 2.7652511596679688\n",
            "170.895672082901 0.8704288601875305\n",
            "171.76610094308853 2.0211777687072754\n",
            "173.7872787117958 2.331139326095581\n",
            "176.1184180378914 2.299882650375366\n",
            "178.41830068826675 4.032008171081543\n",
            "182.4503088593483 1.61993408203125\n",
            "184.07024294137955 5.5946879386901855\n",
            "189.66493088006973 0.8256751894950867\n",
            "190.49060606956482 1.6581676006317139\n",
            "192.14877367019653 36.28125\n",
            "228.43002367019653 2.983821392059326\n",
            "231.41384506225586 1.4733637571334839\n",
            "232.88720881938934 2.2595622539520264\n",
            "235.14677107334137 28.289697647094727\n",
            "263.4364687204361 1.8470690250396729\n",
            "265.28353774547577 3.6835293769836426\n",
            "268.9670671224594 1.525996446609497\n",
            "270.4930635690689 1.529629111289978\n",
            "272.0226926803589 2.625096321105957\n",
            "274.64778900146484 1.7751952409744263\n",
            "276.42298424243927 2.323213577270508\n",
            "278.7461978197098 7.342845916748047\n",
            "286.0890437364578 0.7673063278198242\n",
            "286.85635006427765 4.275936126708984\n",
            "291.13228619098663 2.9712295532226562\n",
            "294.1035157442093 2.7301368713378906\n",
            "296.8336526155472 2.26029896736145\n",
            "299.09395158290863 81.46971893310547\n",
            "380.5636705160141 4.110836982727051\n",
            "384.67450749874115 0.9775840044021606\n",
            "385.6520915031433 1.8015344142913818\n",
            "387.4536259174347 1.7674524784088135\n",
            "389.2210783958435 1.8249129056930542\n",
            "391.04599130153656 2.3895092010498047\n",
            "393.43550050258636 2.7319071292877197\n",
            "396.1674076318741 42.98643493652344\n",
            "439.1538425683975 2.0260119438171387\n",
            "441.17985451221466 2.3947174549102783\n",
            "443.57457196712494 1.68604576587677\n",
            "445.2606177330017 2.9999094009399414\n",
            "448.26052713394165 2.9270079135894775\n",
            "451.1875350475311 1.618146300315857\n",
            "452.805681347847 1.9637469053268433\n",
            "454.7694282531738 1.9040292501449585\n",
            "456.6734575033188 3.173828125\n",
            "459.8472856283188 3.84200382232666\n",
            "463.68928945064545 1.5579819679260254\n",
            "465.2472714185715 1.5810526609420776\n",
            "466.82832407951355 1.6533455848693848\n",
            "468.48166966438293 4.29905891418457\n",
            "472.7807285785675 2.0109591484069824\n",
            "474.7916877269745 1.4613460302352905\n",
            "476.2530337572098 2.7615482807159424\n",
            "479.0145820379257 1.724053978919983\n",
            "480.7386360168457 0.702637255191803\n",
            "481.4412732720375 1.8543593883514404\n",
            "483.29563266038895 0.9427177906036377\n",
            "484.2383504509926 1.9401354789733887\n",
            "486.178485929966 1.650720477104187\n",
            "487.82920640707016 21.67528533935547\n",
            "509.50449174642563 3.7819340229034424\n",
            "513.2864257693291 11.980253219604492\n",
            "525.2666789889336 0.7830640077590942\n",
            "526.0497429966927 2.955681324005127\n",
            "529.0054243206978 1.8449070453643799\n",
            "530.8503313660622 28.777843475341797\n",
            "559.628174841404 5.632197856903076\n",
            "565.260372698307 3.338186740875244\n",
            "568.5985594391823 2.3706130981445312\n",
            "570.9691725373268 13.257966995239258\n",
            "584.2271395325661 2.7124974727630615\n",
            "586.9396370053291 2.271819591522217\n",
            "589.2114565968513 8.188909530639648\n",
            "597.400366127491 0.8026310205459595\n",
            "598.202997148037 8.887434005737305\n",
            "607.0904311537743 1.6239224672317505\n",
            "608.714353621006 0.8599832057952881\n",
            "609.5743368268013 2.416475296020508\n",
            "611.9908121228218 1.7727128267288208\n",
            "613.7635249495506 2.6545891761779785\n",
            "616.4181141257286 5.018528461456299\n",
            "621.4366425871849 1.7409493923187256\n",
            "623.1775919795036 2.6042771339416504\n",
            "625.7818691134453 1.579803466796875\n",
            "627.3616725802422 1.6268057823181152\n",
            "628.9884783625603 0.8959650993347168\n",
            "629.884443461895 1.6393685340881348\n",
            "631.5238119959831 1.6235246658325195\n",
            "633.1473366618156 2.525578260421753\n",
            "635.6729149222374 0.8363932371139526\n",
            "636.5093081593513 1.9252755641937256\n",
            "638.4345837235451 2.424042224884033\n",
            "640.8586259484291 3.1273653507232666\n",
            "643.9859912991524 5.101203918457031\n",
            "649.0871952176094 1.9492114782333374\n",
            "651.0364066958427 1.7408251762390137\n",
            "652.7772318720818 2.2317140102386475\n",
            "655.0089458823204 1.6545099020004272\n",
            "656.6634557843208 4.816730976104736\n",
            "661.4801867604256 5.378589153289795\n",
            "666.8587759137154 2.2699577808380127\n",
            "669.1287336945534 1.74605393409729\n",
            "670.8747876286507 3.2590599060058594\n",
            "674.1338475346565 1.8135287761688232\n",
            "675.9473763108253 1.6455998420715332\n",
            "677.5929761528969 1.6765323877334595\n",
            "679.2695085406303 0.8621525764465332\n",
            "680.1316611170769 3.9644055366516113\n",
            "684.0960666537285 3.1088967323303223\n",
            "687.2049633860588 2.284961700439453\n",
            "689.4899250864983 0.8850445747375488\n",
            "690.3749696612358 2.276872158050537\n",
            "692.6518418192863 1.7281181812286377\n",
            "694.379960000515 2.1063923835754395\n",
            "696.4863523840904 1.5934998989105225\n",
            "698.079852283001 1.7483696937561035\n",
            "699.828221976757 1.9295865297317505\n",
            "701.7578085064888 2.6566123962402344\n",
            "704.414420902729 0.80192631483078\n",
            "705.2163472175598 7.935215950012207\n",
            "713.151563167572 1.744212031364441\n",
            "714.8957751989365 8.019377708435059\n",
            "722.9151529073715 0.8863507509231567\n",
            "723.8015036582947 4.578361511230469\n",
            "728.3798651695251 19.440603256225586\n",
            "747.8204684257507 2.0962843894958496\n",
            "749.9167528152466 0.8116063475608826\n",
            "750.7283591628075 0.744743824005127\n",
            "751.4731029868126 nan\n",
            "751.4731029868126 1.8324894905090332\n",
            "753.3055924773216 2.5829014778137207\n",
            "755.8884939551353 1.9035624265670776\n",
            "757.7920563817024 2.048661947250366\n",
            "759.8407183289528 8.273436546325684\n",
            "768.1141548752785 2.5561790466308594\n",
            "770.6703339219093 1.8347673416137695\n",
            "772.5051012635231 3.849844217300415\n",
            "776.3549454808235 1.429081678390503\n",
            "777.784027159214 9.867246627807617\n",
            "787.6512737870216 1.9495463371276855\n",
            "789.6008201241493 3.0019683837890625\n",
            "792.6027885079384 2.7731337547302246\n",
            "795.3759222626686 1.9260038137435913\n",
            "797.3019260764122 19.478614807128906\n",
            "816.7805408835411 3.831460475921631\n",
            "820.6120013594627 0.8546249866485596\n",
            "821.4666263461113 1.6078112125396729\n",
            "823.074437558651 0.8288848996162415\n",
            "823.9033224582672 2.9217677116394043\n",
            "826.8250901699066 0.8433976173400879\n",
            "827.6684877872467 4.409377098083496\n",
            "832.0778648853302 1.3141365051269531\n",
            "833.3920013904572 2.9043350219726562\n",
            "836.2963364124298 1.510769248008728\n",
            "837.8071056604385 2.589104413986206\n",
            "840.3962100744247 1.6842988729476929\n",
            "842.0805089473724 3.1302709579467773\n",
            "845.2107799053192 1.6573619842529297\n",
            "846.8681418895721 2.495889902114868\n",
            "849.364031791687 nan\n",
            "849.364031791687 3.58315372467041\n",
            "852.9471855163574 1.7680037021636963\n",
            "854.7151892185211 1.4746172428131104\n",
            "856.1898064613342 1.613439679145813\n",
            "857.80324614048 3.910505771636963\n",
            "861.713751912117 2.2527012825012207\n",
            "863.9664531946182 1.5340416431427002\n",
            "865.5004948377609 1.6318657398223877\n",
            "867.1323605775833 1.5691382884979248\n",
            "868.7014988660812 0.7326395511627197\n",
            "869.434138417244 2.2439000606536865\n",
            "871.6780384778976 6.03495979309082\n",
            "877.7129982709885 1.5578848123550415\n",
            "879.2708830833435 4.214034080505371\n",
            "883.4849171638489 3.555018186569214\n",
            "887.0399353504181 4.629350662231445\n",
            "891.6692860126495 2.320816993713379\n",
            "893.9901030063629 1.6217695474624634\n",
            "895.6118725538254 0.7711737751960754\n",
            "896.3830463290215 1.746497392654419\n",
            "898.1295437216759 1.9592117071151733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [04:38<00:00, 139.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "900.088755428791 1.879906177520752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2nsbro-Tse",
        "outputId": "fab2d1f1-6d0b-4140-b9e7-36660b01ce82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.235385306694541, 4.156537611088995]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "n_dOW6SREUn2",
        "outputId": "e5922afe-fb75-40dd-e3e3-a79516f5319f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fddd526c9a0>]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASkAAAESCAYAAAChJCPsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAnBUlEQVR4nO3deVxU9f7H8feZGTYRRkRlURT3DVAQXDMruWqR2WKGoqi5dItS6lZopf5KC1Qyr16vC+4lUN3rdls0NbFbscliCO6aIIuUJQOSgPD9/TE6VxKQGQZmez8fj3kkZ75n+ByGeXUYxlESQggQERkpmaEHICJqCCNFREaNkSIio8ZIEZFRY6SIyKgxUkRk1BgpIjJqCkMPoC81NTUoKCiAg4MDJEky9DhEdBchBEpLS+Hu7g6ZTLtzI7OJVEFBATw8PAw9BhE1IC8vD506ddJqH7OJlIODAwD1F8HR0dHA0xDR3VQqFTw8PDSPU22YTaTu/Ijn6OjISBEZKV2eiuET50Rk1BgpIjJqjBQRGTVGioiMGiNFREaNkSIio2Zxkbr06w3sP1Fg6DGIqJHM5nVSjVFcehPBmxJRXFqBiqpqPOvPV6gTGTuLOpNq39oGY/q5QgjgzX//hM+O5xl6JCK6D4uKlCRJeG9Cf4QO6wIhgIh//4RPU3MNPRYRNcCiIgWoQ/XuE/0xY7jn7VBlIT6FoSIyVhYXKUAdqiXj+2HmCE8AwILdWYhNZqiIjJFFRgpQh2rx4/3w/IiuAIC39mRhV/JlA09FRH9msZEC1KFa9HhfzHpAHaq395zEx0kMFZExsehIAepQvRPUF3NGqkO1aO9JfJz4s2GHIiINi48UoA7VW4/1xQsPdgMALNqXjR0//mzYoYgIACOlIUkSFjzaBy+MUodqyf5sbP/hkoGnIiJG6i6SJGHBuD548aHuAID/+08Otn7PUBEZEiP1J5Ik4c2xvRH2sDpU732Rg83/vWjgqYgsFyNVB0mS8PqY3nj54R4AgGVfnmKoiAyEkaqHJEn425hemPfI/0IV8x1DRdTSGKkGSJKE18b0xvzRPQEA7391ChuPXTDwVESWhZFqhFf/0gvhgepQRX59GusTGCqilsJINVJ4YC+8GtgLALD8wGn8M+G8gScisgyMlBbmB/bEa39Rh2rFgTNYd5ShImpujJSW5o3uidfHqEO18uAZrD1yzsATEZk3RkoHLz/SE2+M7Q0A+PDQWaxhqIiaDSOlo7CHe+DNcepQrTp0FqsPnzXwRETmiZFqgpce6oEFj/YBAKw+fA4fHWKoiPSNkWqiv47qjrceU4fq70fOYdWhsxBCGHgqIvPBSOnB3Ae74+3H+gIA1jBURHrFSOnJnAe74Z0gdajWfnse0d+cYaiI9ICR0qPZI7th0eP9AADrjl7AyoMMFVFTMVJ6NuuBrlgyXh2qfyZcwPIDDBVRUzBSzWDmiK5494n+AIANxy4g6uvTDBWRjhipZjJ9uCfem6AO1cbvLiKSoSLSCSPVjEKHeWLp7VBt+u4i3v/yFENFpCVGqplNG+aJZU96AQA2f38JS79gqIi0wUi1gKlDu+CDp7wBAFt/uIT3vshhqIgaiZFqIVOGdEbk0+pQbfvhZ7z7H4aKqDEYqRY0eXBnLH/GG5IEbP/xZ/zf/myGiug+GKkW9lxAZyx/2geSBOxIvIzF+xgqooYwUgYwKcADy59Rh+rjpMtYtO8kamoYKqK6MFIGMsnfAysnDoAkAZ8k5eIdhoqoToyUAU0c1AnRt0MVm5yLt/cyVER/1qRIRUVFQZIkhIeH17smJiYGI0eOhJOTE5ycnBAYGIiUlBTN9VVVVYiIiIC3tzfs7e3h7u6O0NBQFBQUNGU0k/HMoE5YNWkAZBIQl5KLt/ZkMVREd9E5Uqmpqdi4cSN8fHwaXJeQkIDJkyfj6NGjSExMhIeHB8aMGYP8/HwAQHl5OdLT07Fo0SKkp6dj9+7dOHPmDJ544gldRzM5T/l2wqpJAyGTgPjUPCzczVARaQgdlJaWip49e4pDhw6JUaNGifnz5zd631u3bgkHBwexY8eOetekpKQIAOLy5cuNvt2SkhIBQJSUlDR6H2OzN+OK6LrgC9El4gvx+meZorq6xtAjEelFUx6fOp1JhYWFISgoCIGBgVrvW15ejqqqKrRt27beNSUlJZAkCW3atKl3TUVFBVQqVa2LqZswsCNWB/tCJgGfp13Bm//+CdU8oyILp9B2h/j4eKSnpyM1NVWnTxgREQF3d/d6A3fz5k1ERERg8uTJcHR0rPd2IiMj8e677+o0gzF7YoA7JADhn2biX2lXUCMEVk4cALlMMvRoRAah1ZlUXl4e5s+fj127dsHW1lbrTxYVFYX4+Hjs2bOnzv2rqqowadIkCCGwfv36Bm9r4cKFKCkp0Vzy8vK0nsdYjR/gjjXBvpDLJOxOz8cbn5/gGRVZLEmIxr/cee/evXjqqacgl8s126qrqyFJEmQyGSoqKmpdd7fo6GgsW7YMhw8fhr+//z3X3wnUxYsX8e2338LZ2VmrA1GpVFAqlSgpKWnwDMyUfJVViFfiMlBdI/DkQHd8OGkgz6jIJDXl8anVj3ujR49GVlZWrW0zZ85Enz59EBERUW+gVqxYgffffx8HDx5sMFDnzp3D0aNHtQ6UuXrM2w0SgFfiMrA3swACwIfPDoBCzpe3keXQKlIODg7w8vKqtc3e3h7Ozs6a7aGhoejYsSMiIyMBAMuXL8fixYsRGxsLT09PFBUVAQBat26N1q1bo6qqChMnTkR6ejq++OILVFdXa9a0bdsW1tbWTT5IU/aotxv+IUl4OTYd+zILIASwahJDRZZD79/pubm5KCws1Hy8fv16VFZWYuLEiXBzc9NcoqOjAQD5+fnYv38/rly5goEDB9Za8+OPP+p7PJM0zssV60L8oJBJ2H+iAOGfZuJWdY2hxyJqEVo9J2XMzPE5qT/7JrsIYbHpqKoWCPJxw9+fG8gzKjIJTXl88jvchIzp74r1IYNgJZfw5U+FmB+fiSqeUZGZY6RMTGA/F2yYOgjWchm+zCrEvLgMhorMGiNlgkb3dcGGaX6wlsvw9ckivBLLUJH5YqRM1CN9XLBxmvqM6kB2EV6OTUflLYaKzA8jZcIe7tMBm0IHwVohw8HsqwhjqMgMMVIm7qHeHRAT6g9rhQyHcq7ipV0MFZkXRsoMjOrVHptD/WGjkOHwqat4aVcaKm5VG3osIr1gpMzEg73aY/P0O6EqxoufpDNUZBYYKTMysmd7bJ0RAFsrGb49XYy/fpyGm1UMFZk2RsrMjOjRDlunq0N19Mwv+OsnDBWZNkbKDA3v0U5zRpVw5hfM5RkVmTBGykwN794O22YMhp2VHN+d/QVzdh5nqMgkMVJmbFh3Z2ybGQA7Kzn+e+5XhopMEiNl5oZ2c8b2mQFoZa0O1ewdx/FHJUNFpoORsgBDujljx/ODYW8tx/fnf8WsHakMFZkMRspCBHi21YTqxwvX8Pz2VJRX3jL0WET3xUhZEH/Pttg5azBa2yiQeJGhItPASFmYQV3UZ1StbRRIuvgbZm5jqMi4MVIWaFAXJ+ycNRgONgokX/oNM7al4kYFQ0XGiZGyUH6d/xeqlEu/Yca2FJQxVGSEGCkL5tvZCR/PHgIHWwVSf/4dM7YyVGR8GCkLN9CjDT6ZpQ7V8cu/Y/rWFJTerDL0WEQajBRhgEcb7Jo9BI62CqQxVGRkGCkCAPh0aoPYOUOhtLNCeu51hG5NgYqhIiPASJGGV0clds0eAqWdFTJyryN0C0NFhsdIUS13QtWmlRUy865j2pYUlPzBUJHhMFJ0D6+OSsTOHgqnVlY4kXcdoVuSGSoyGEaK6tTP3RG77oTqSgmmbUlGSTlDRS2PkaJ69XN3ROycoWhrb42frpRgKkNFBsBIUYP6ujkibs5QONtbIyu/BCFbknC9vNLQY5EFYaTovnq7OiD2dqhO5qswJSYZv99gqKhlMFLUKL1dHRA3dyjatbZGTqEKIZsZKmoZjBQ1Wi8XB8TNGYp2rW2QU6jClM3J+I2hombGSJFWero4IH7uELRrbYNThSpMiUnCtbIKQ49FZoyRIq316OCA+LlD0d7BBqeLShGyOZmhombDSJFOenRojfi5Q9HhdqimxCTjV4aKmgEjRTrr3l4dKhdHG5y5WorJm5LwSylDRfrFSFGTdGvfGvFzh8HV0RbnisswJYahIv1ipKjJurazR/zcoXBTqkM1OSYJxaU3DT0WmQlGivTC865QnS8uw+RNSShWMVTUdIwU6U0XZ3Wo3JW2uPDLDQTHMFTUdIwU6ZU6VMPQsY0dLv5yA8GbknCVoaImaFKkoqKiIEkSwsPD610TExODkSNHwsnJCU5OTggMDERKSkqtNUIILF68GG5ubrCzs0NgYCDOnTvXlNHIgDo7t0L83KHqUP2qDlVRCUNFutE5Uqmpqdi4cSN8fHwaXJeQkIDJkyfj6NGjSExMhIeHB8aMGYP8/HzNmhUrVmDNmjXYsGEDkpOTYW9vj7Fjx+LmTX5jmyqPtv8L1aVfbyB4UyIKS/4w9FhkioQOSktLRc+ePcWhQ4fEqFGjxPz58xu9761bt4SDg4PYsWOHEEKImpoa4erqKlauXKlZc/36dWFjYyPi4uIafbslJSUCgCgpKWn0PtT88n67IUZEHRFdIr4QD674VuT/Xm7okcgAmvL41OlMKiwsDEFBQQgMDNR63/LyclRVVaFt27YAgEuXLqGoqKjWbSmVSgwZMgSJiYn13k5FRQVUKlWtCxmfTk6t8OkLw+DR1g6Xr5UjeFMS8q/zjIoaT+tIxcfHIz09HZGRkTp9woiICLi7u2uiVFRUBABwcXGptc7FxUVzXV0iIyOhVCo1Fw8PD53moebXsY0d4ucOQ+e2rZD7WzmCNyUyVNRoWkUqLy8P8+fPx65du2Bra6v1J4uKikJ8fDz27Nmj0/53W7hwIUpKSjSXvLy8Jt0eNS91qIaii3Mr5P32B4I3JeLK7+WGHotMgFaRSktLQ3FxMfz8/KBQKKBQKHDs2DGsWbMGCoUC1dXV9e4bHR2NqKgofPPNN7WebHd1dQUAXL16tdb6q1evaq6ri42NDRwdHWtdyLi53w6VpyZUScj7jaGihmkVqdGjRyMrKwuZmZmai7+/P0JCQpCZmQm5XF7nfitWrMDSpUtx4MAB+Pv717qua9eucHV1xZEjRzTbVCoVkpOTMWzYMB0OiYyZm1L9o1/Xdva48jtDRfenVaQcHBzg5eVV62Jvbw9nZ2d4eXkBAEJDQ7Fw4ULNPsuXL8eiRYuwdetWeHp6oqioCEVFRSgrKwMAzeusli1bhv379yMrKwuhoaFwd3fHk08+qb8jJaPhqrRF3Jyh6NbOHvnXGSpqmN5fcZ6bm4vCwkLNx+vXr0dlZSUmTpwINzc3zSU6Olqz5s0338Qrr7yCuXPnIiAgAGVlZThw4ECTn7ci4+WqtEXc3P+F6rmNici9xlDRvSQhhDD0EPqgUqmgVCpRUlLC56dMSLHqJoJjknDxlxtwU9refnLd3tBjkZ415fHJv7tHBtXBUR2m7u3tUVhyE8GbkvDzrzcMPRYZEUaKDK6Dgy3i5w5Djw6tNaG6xFDRbYwUGYX2DjaImzMUPTu0RpHqJoI3JeLiL2WGHouMACNFRqO9gw3i5g5FL5fWuKqqQPCmJFxgqCweI0VGpV1r9RlVbxcHFJdWYPKmJJwvZqgsGSNFRse5tQ1i5wxBH9fboYphqCwZI0VGSR2qoejj6oBfStU/+p0vLjX0WGQAjBQZrbb21oibMxR93Rzxa5k6VOeuMlSWhpEio+Zkb43Y2UPQz80Rv5ZVInhTEs4yVBaFkSKj52Rvjdg5Q9Df3RHXblRi8qYknCliqCwFI0UmoU0ra+yaPQReHW+HKiYJp4v4bqyWgJEik9GmlTV2zRoK745K/Hb7jOpUIUNl7hgpMinKVlb4ZPYQDOikxO/lVZgSk4ScAobKnDFSZHKUdlbYOWsIBni0UYdqcxKyC0oMPRY1E0aKTJLSzgofzxqMgR5tcL28ClNiknEyn6EyR4wUmSxHWyvsnDUYvp3boOSPKoRsZqjMESNFJs3R1go7nx8Mv9uhmhKThKwrDJU5YaTI5DnYWmHH84MxqIsTVDdvIWRzEn66ct3QY5GeMFJkFu6Eyl8TqmScyLtu6LFIDxgpMhutbRTY/vxgBHg6ofTmLUzdnIyM3N8NPRY1ESNFZqW1jQLbZw7GYM+2KK24hdAtKUhnqEwaI0Vmx95GgW0zAzCk6/9ClXaZoTJVjBSZpTuhGtqtLcoqbmH61hSkXf7N0GORDhgpMlutrBXYOiMAw7o5o+z2GdXxnxkqU8NIkVm7E6rh3Z1xo7Ia07emIJWhMimMFJk9O2s5tkwPwAM92mlClXKJoTIVjBRZBDtrOTZP98fInu1QXlmNGdtSkHzxmqHHokZgpMhi2FrJERN6d6hSkcRQGT1GiizKnVA92Ks9/qiqxsxtqfjxwq+GHosawEiRxbG1kmPTtEEYdTtUz29PxY/nGSpjxUiRRbK1kmPjtEF4uHd73KyqwfM7UvEDQ2WUGCmyWLZWcmyYNgiP9OmgDtX2VHx/jqEyNowUWTQbhRzrp/phdJ8OqLhVg1k7UvHd2V8MPRbdhZEii2ejkOOfU/0Q2NcFFbdqMHvncRxjqIwGI0WE26EK8cNf+rmg8lYN5uw8joQzxYYei8BIEWlYK2RYN8UPY26Hau7ONBxlqAyOkSK6i7VChnUhfhjX3xWV1TV4YWcajp5mqAyJkSL6Eyu5DGun+OJRr9uh+jgNR05dNfRYFouRIqqDlVyGNZN98Zi3OlR//SQNh3MYKkNgpIjqYSWX4e/BvgjycUNVtcCLu9JwiKFqcYwUUQOs5DL8/bmBePx2qF7alYaD2UWGHsuiMFJE96GQy7D6uYEYP8AdVdUCYbvSceAkQ9VSGCmiRlDIZfho0gBMGOiOWzUCL8em48DJQkOPZRGaFKmoqChIkoTw8PB612RnZ+OZZ56Bp6cnJEnC6tWr71lTXV2NRYsWoWvXrrCzs0P37t2xdOlSCCGaMh6RXinkMqyaNBBP3g5VWGwGvspiqJqbQtcdU1NTsXHjRvj4+DS4rry8HN26dcOzzz6LV199tc41y5cvx/r167Fjxw70798fx48fx8yZM6FUKjFv3jxdRyTSO7lMwoeTBkImSdidkY9X4jIgBBDk42bo0cyWTpEqKytDSEgIYmJisGzZsgbXBgQEICAgAACwYMGCOtf8+OOPmDBhAoKCggAAnp6eiIuLQ0pKSr23W1FRgYqKCs3HKpVK28Mg0olcJmHlswMACdidno958RkQEHjcx93Qo5klnX7cCwsLQ1BQEAIDA/UyxPDhw3HkyBGcPXsWAHDixAl8//33ePTRR+vdJzIyEkqlUnPx8PDQyyxEjSGXSVg5cQAmDuqE6hqB+fGZ2H+iwNBjmSWtz6Ti4+ORnp6O1NRUvQ2xYMECqFQq9OnTB3K5HNXV1Xj//fcREhJS7z4LFy7Ea6+9pvlYpVIxVNSi5DIJy5/xgQTg87QrCI/PgBACEwZ2NPRoZkWrSOXl5WH+/Pk4dOgQbG1t9TbEZ599hl27diE2Nhb9+/dHZmYmwsPD4e7ujunTp9e5j42NDWxsbPQ2A5Eu7oRKJkn49HgeXv00EwAYKj3SKlJpaWkoLi6Gn5+fZlt1dTW+++47/OMf/0BFRQXkcrnWQ7zxxhtYsGABgoODAQDe3t64fPkyIiMj640UkbGQySREPu0NSQLiU9WhqhECT/l2MvRoZkGrSI0ePRpZWVm1ts2cORN9+vRBRESEToEC1L8BlMlqPz0ml8tRU1Oj0+0RtTSZTMIHT6lDFZeSh9c+OwEhgKf9GKqm0ipSDg4O8PLyqrXN3t4ezs7Omu2hoaHo2LEjIiMjAQCVlZXIycnR/Dk/Px+ZmZlo3bo1evToAQAYP3483n//fXTu3Bn9+/dHRkYGVq1aheeff77JB0jUUmQyCe8/6Q1JkhCbnIu/fX4CNQKYOIihagqdXydVn9zc3FpnRQUFBfD19dV8HB0djejoaIwaNQoJCQkAgLVr12LRokV46aWXUFxcDHd3d7zwwgtYvHixvscjalYymYRlE7wgk4BPknLxxr9OQAiBZ/35Sx1dScJMXtatUqmgVCpRUlICR0dHQ49DFk4IgcX7svFx0mVIErD8aR9MCrDcUDXl8cm/u0fUDCRJwnsT+mP6sC4QAojY/RM+Tc019FgmiZEiaiaSJOH/nuiPGcM91aH6dxbiUhgqbTFSRM1IkiQsGd8PM0d4AgAW7s5CbDJDpQ1GiqiZSZKExY/3w/MjugIA3tqThV3Jlw08lelgpIhagCRJWPR4X8x+QB2qt/ecxMdJDFVjMFJELUSSJLwd1BdzRqpDtWjvSexM/NmwQ5kARoqoBUmShLce64sXHuwGAFi8Lxs7fvzZsEMZOUaKqIVJkoQFj/bBX0d1BwAs2Z+NbT9cMvBUxouRIjIASZIQMa43XnxIHap3/5ODrd8zVHVhpIgMRJIkvDm2N8IeVofqvS9ysPm/Fw08lfFhpIgMSJIkvD6mN155RP2X7Zd9eYqh+hNGisjAJEnCa3/phXl3hWrTdxcMPJXxYKSIjIAkSXhtTG/MH90TAPDBV6ex8RhDBTBSREbl1b/0QnigOlSRX5/G+gSGipEiMjLhgb3wamAvAMDyA6ex7uh5A09kWIwUkRGaH9gTf/uLOlQrD56x6FAxUkRG6pXRPfHG2N4A1KFae+ScgScyDEaKyIiFPdxDE6oPD53F3w9bXqgYKSIjF/ZwD0SM6wMA+OjwWaw+fNbAE7UsRorIBLz4UHcseFQdqtWHz2HVobMwk3+e4L4YKSIT8ddR3fHWY+pQrTlyDh9ZSKgYKSITMvfB7ngnqC8AYM235y3ijIqRIjIxs0d204Rq7bfnEf3NGbMOFSNFZIJmj+yGxY/3AwCsO3oBKw6ab6gYKSIT9fwDXbFkvDpU6xMuYPkB8wwVI0VkwmaO6Ip3n+gPANhw7AKivj5tdqFipIhM3PThnnhvgjpUG7+7iA++OmVWoWKkiMxA6DBPLH3SCwAQ899LeP9L8wkVI0VkJqYN7YJlt0O1+ftLWPqFeYSKkSIyI1OHdsEHT3kDALb+cAnvfZFj8qFipIjMzJQhnRH5tDpU2374Ge/+x7RDxUgRmaHJgztj+TPekCRg+48/Y8n+bJMNFSNFZKaeC+iM5U/7QJKAnYmXsXifaYaKkSIyY5MCPLDiGXWoPk66jEX7TqKmxrRCxUgRmbln/T2wcuIASBLwSVIu3jGxUDFSRBZg4qBO+PBZdahik3Px9t4skwkVI0VkIZ7264RVkwZAJgFxKXl4a49phIqRIrIgT/l2wqpJAyGTgPjUPCzY/ZPRh4qRIrIwT/p2xEfPqUP12fEriPi3cYeKkSKyQBMGdsTqYF/IJODztCt4898/odpIQ6Uw9ABEZBhPDHCHBCD800z8K+0KaoTAyokDIJdJhh6tFkaKyIKNH+AOmSRhXnwGdqfnAwJY+axxhapJP+5FRUVBkiSEh4fXuyY7OxvPPPMMPD09IUkSVq9eXee6/Px8TJ06Fc7OzrCzs4O3tzeOHz/elPGIqBGCfNywdrIv5DIJuzPy8bfPMo3qRz+dI5WamoqNGzfCx8enwXXl5eXo1q0boqKi4OrqWuea33//HSNGjICVlRW+/vpr5OTk4MMPP4STk5Ou4xGRFh7zdsM/JvtCIZOwN7MAr32WiVvVNYYeC4COP+6VlZUhJCQEMTExWLZsWYNrAwICEBAQAABYsGBBnWuWL18ODw8PbNu2TbOta9euuoxGRDp61NsN/5AkvBybjn2ZBagRwEeTBkAhN+zv13T67GFhYQgKCkJgYKBehti/fz/8/f3x7LPPokOHDvD19UVMTEyD+1RUVEClUtW6EFHTjPNyxboQPyhkEv5zogDhnxr+jErrSMXHxyM9PR2RkZF6G+LixYtYv349evbsiYMHD+LFF1/EvHnzsGPHjnr3iYyMhFKp1Fw8PDz0Ng+RJRvb3xX/DPGDlVzCFz8VYn58JqoMGCqtIpWXl4f58+dj165dsLW11dsQNTU18PPzwwcffABfX1/MnTsXc+bMwYYNG+rdZ+HChSgpKdFc8vLy9DYPkaUb098V60MGwUou4cusQsyPzzBYqLSKVFpaGoqLi+Hn5weFQgGFQoFjx45hzZo1UCgUqK6u1mkINzc39OvXr9a2vn37Ijc3t959bGxs4OjoWOtCRPoT2M8FG6YOgrVchq+yijAvzjCh0ipSo0ePRlZWFjIzMzUXf39/hISEIDMzE3K5XKchRowYgTNnztTadvbsWXTp0kWn2yMi/Rjd1wUbpvnBWi7D1yeL8HJsOipvtWyotIqUg4MDvLy8al3s7e3h7OwMLy/1v1IRGhqKhQsXavaprKzUBK2yshL5+fnIzMzE+fPnNWteffVVJCUl4YMPPsD58+cRGxuLTZs2ISwsTE+HSUS6eqSPCzaGDoK1QoaD2VdbPFR6/91ibm4uCgsLNR8XFBTA19cXvr6+KCwsRHR0NHx9fTF79mzNmoCAAOzZswdxcXHw8vLC0qVLsXr1aoSEhOh7PCLSwcO9O2DTNHWovsm5irAWDJUkTPFNj+ugUqmgVCpRUlLC56eImsmxs79gzs7jqLxVg8C+HbAuxA82ivs/zdOUxyffBYGIGm1Ur/bYHOoPG4UMh08V46VP0lFxS7dfmDUWI0VEWnmwV3tsmR4AG4UMR04XY15cRrP+KzSMFBFp7YGe7bB1RgAcbBWYOMgDktR875rAt2ohIp2M6NEO30c8AqWdVbN+Hp5JEZHOmjtQACNFREaOkSIio8ZIEZFRY6SIyKgxUkRk1BgpIjJqZvM6qTuveOXbCBMZnzuPS11emW42kSotLQUAvo0wkRErLS2FUqnUah+zeReEmpoaFBQUwMHBocGX6KtUKnh4eCAvL89s3i2Bx2QaLPmYhBAoLS2Fu7s7ZDLtnmUymzMpmUyGTp06NXq9Ob7lMI/JNFjqMWl7BnUHnzgnIqPGSBGRUbO4SNnY2GDJkiWwsbEx9Ch6w2MyDTwm3ZjNE+dEZJ4s7kyKiEwLI0VERo2RIiKjxkgRkVFjpIjIqJlFpNatWwdPT0/Y2tpiyJAhSElJaXD9559/jj59+sDW1hbe3t746quval0vhMDixYvh5uYGOzs7BAYG4ty5c815CPfQ5phiYmIwcuRIODk5wcnJCYGBgfesnzFjBiRJqnUZN25ccx+GhjbHs3379ntmtbW1rbXG1O6jhx566J5jkiQJQUFBmjWGvo++++47jB8/Hu7u7pAkCXv37r3vPgkJCfDz84ONjQ169OiB7du337NG28fnPYSJi4+PF9bW1mLr1q0iOztbzJkzR7Rp00ZcvXq1zvU//PCDkMvlYsWKFSInJ0e88847wsrKSmRlZWnWREVFCaVSKfbu3StOnDghnnjiCdG1a1fxxx9/GOUxTZkyRaxbt05kZGSIU6dOiRkzZgilUimuXLmiWTN9+nQxbtw4UVhYqLn89ttvRnk827ZtE46OjrVmLSoqqrXG1O6ja9eu1TqekydPCrlcLrZt26ZZY8j7SAghvvrqK/H222+L3bt3CwBiz549Da6/ePGiaNWqlXjttddETk6OWLt2rZDL5eLAgQOaNdp+nepi8pEaPHiwCAsL03xcXV0t3N3dRWRkZJ3rJ02aJIKCgmptGzJkiHjhhReEEELU1NQIV1dXsXLlSs31169fFzY2NiIuLq4ZjuBe2h7Tn926dUs4ODiIHTt2aLZNnz5dTJgwQd+jNoq2x7Nt2zahVCrrvT1zuI8++ugj4eDgIMrKyjTbDHkf/VljIvXmm2+K/v3719r23HPPibFjx2o+burXSQghTPrHvcrKSqSlpSEwMFCzTSaTITAwEImJiXXuk5iYWGs9AIwdO1az/tKlSygqKqq1RqlUYsiQIfXepj7pckx/Vl5ejqqqKrRt27bW9oSEBHTo0AG9e/fGiy++iGvXrul19rroejxlZWXo0qULPDw8MGHCBGRnZ2uuM4f7aMuWLQgODoa9vX2t7Ya4j3R1v8eSPr5OgIk/J/Xrr7+iuroaLi4utba7uLigqKiozn2KiooaXH/nv9rcpj7pckx/FhERAXd391rfHOPGjcPOnTtx5MgRLF++HMeOHcOjjz6K6upqvc7/Z7ocT+/evbF161bs27cPn3zyCWpqajB8+HBcuXIFgOnfRykpKTh58iRmz55da7uh7iNd1fdYUqlU+OOPP/TyvQyY0Vu1kFpUVBTi4+ORkJBQ68nm4OBgzZ+9vb3h4+OD7t27IyEhAaNHjzbEqPUaNmwYhg0bpvl4+PDh6Nu3LzZu3IilS5cacDL92LJlC7y9vTF48OBa203pPmpJJn0m1a5dO8jlcly9erXW9qtXr8LV1bXOfVxdXRtcf+e/2tymPulyTHdER0cjKioK33zzDXx8fBpc261bN7Rr1w7nz59v8swNacrx3GFlZQVfX1/NrKZ8H924cQPx8fGYNWvWfT9PS91HuqrvseTo6Ag7Ozu93PeAiUfK2toagwYNwpEjRzTbampqcOTIkVr/J77bsGHDaq0HgEOHDmnWd+3aFa6urrXWqFQqJCcn13ub+qTLMQHAihUrsHTpUhw4cAD+/v73/TxXrlzBtWvX4Obmppe566Pr8dyturoaWVlZmllN9T4C1C9/qaiowNSpU+/7eVrqPtLV/R5L+rjvAZjHSxBsbGzE9u3bRU5Ojpg7d65o06aN5lfW06ZNEwsWLNCs/+GHH4RCoRDR0dHi1KlTYsmSJXW+BKFNmzZi37594qeffhITJkxo8V9va3NMUVFRwtraWvzrX/+q9evr0tJSIYQQpaWl4vXXXxeJiYni0qVL4vDhw8LPz0/07NlT3Lx50+iO59133xUHDx4UFy5cEGlpaSI4OFjY2tqK7OzsWsdsSvfRHQ888IB47rnn7tlu6PvozgwZGRkiIyNDABCrVq0SGRkZ4vLly0IIIRYsWCCmTZumWX/nJQhvvPGGOHXqlFi3bl2dL0Fo6OvUGCYfKSGEWLt2rejcubOwtrYWgwcPFklJSZrrRo0aJaZPn15r/WeffSZ69eolrK2tRf/+/cWXX35Z6/qamhqxaNEi4eLiImxsbMTo0aPFmTNnWuJQNLQ5pi5duggA91yWLFkihBCivLxcjBkzRrRv315YWVmJLl26iDlz5mj1jdKSxxMeHq5Z6+LiIh577DGRnp5e6/ZM7T4SQojTp08LAOKbb76557aM4T46evRond9Hd45j+vTpYtSoUffsM3DgQGFtbS26detW63VfdzT0dWoMvp8UERk1k35OiojMHyNFREaNkSIio8ZIEZFRY6SIyKgxUkRk1BgpIjJqjBQRGTVGioiMGiNFREaNkSIio/b/08Bav6cFnTEAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "1wJXbPO5Eny_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e26a017-edae-4511-b461-f761e5f33f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([0, 4])\n",
            "len(conf_scores_final) 2 torch.Size([0])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n",
            "tensor([], device='cuda:0', dtype=torch.int64)\n"
          ]
        }
      ],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4MFONZWvSY",
        "outputId": "d3f38d6a-b2df-4a2f-9ce0-f07ecca9d59b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([], device='cuda:0', size=(0, 4)),\n",
              "  tensor([], device='cuda:0', size=(0, 4))],\n",
              " [tensor([], device='cuda:0'), tensor([], device='cuda:0')],\n",
              " [tensor([], device='cuda:0', dtype=torch.int64),\n",
              "  tensor([], device='cuda:0', dtype=torch.int64)])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JXCX9h1QEnwj"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "l_f0DiBDEq1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "3b830468-f98d-4cb8-abfb-fcc200a6acec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2366\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2367\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2368\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2230\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2231\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2232\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2233\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    455\u001b[0m         *pil_kwargs* and *metadata* are forwarded).\n\u001b[1;32m    456\u001b[0m         \"\"\"\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         mpl.image.imsave(\n\u001b[1;32m    459\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Acquire a lock on the shared font cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/_api/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;31m# Early return in the simple, non-deprecated case (much faster than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# calling bind()).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_varargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mget_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mreuse_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreuse_renderer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lastKey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcleared\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_RendererAgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_renderers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Image size of 285791x5478 pixels is too large. It must be less than 2^16 in each direction."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 2 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAH99PhbTjI"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all.cpu(), fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0].cpu(), fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1].cpu(), fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivNrzM_PbViM"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "filters_data =[filters[0].cpu().detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}