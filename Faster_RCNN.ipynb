{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f7b7872-8e01-46ed-e2d6-b2eb0fcd5503"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19580bf-07b1-49a8-a205-8e17352802be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paye0SiEAuJu",
        "outputId": "314567c8-52c3-434f-bef9-c7fd904182e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([617, 4])\n",
            "len(conf_scores_final) 2 torch.Size([617])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([[ 2.7523e+01, -1.2768e+33,  3.4788e+01,  1.2768e+33],\n",
              "          [-1.2810e+03, -2.1982e+00,  1.2887e+03, -2.1982e+00],\n",
              "          [ 1.0487e+01,  8.0398e+00,  1.0981e+01,  8.0765e+00],\n",
              "          ...,\n",
              "          [-2.7764e+09,  2.0452e+02,  2.7764e+09,  2.0645e+02],\n",
              "          [ 9.7297e+01,        -inf,  9.7297e+01,         inf],\n",
              "          [ 1.3595e+02, -4.7402e+02,  1.3597e+02,  6.6181e+02]], device='cuda:0'),\n",
              "  tensor([[-3.7841e+00, -4.7343e+01, -2.3287e+00,  2.6757e+01],\n",
              "          [ 2.6228e+01,  1.4971e+01,  2.6425e+01,  1.5013e+01],\n",
              "          [-1.1224e+01,  3.2999e+01, -1.1222e+01,  3.3731e+01],\n",
              "          [-2.2406e+01,  2.0002e+01,  4.3875e+01,  2.4138e+01],\n",
              "          [ 3.3445e+00, -3.5999e+02,  3.9590e+00,  3.7258e+02],\n",
              "          [ 9.4133e+00,  6.7393e+00,  1.0401e+01,  8.6134e+00],\n",
              "          [-6.3774e+00, -1.9484e+01, -6.3773e+00,  4.1537e+01],\n",
              "          [ 1.3643e+00, -7.6618e-01,  2.8131e+00, -7.6387e-01],\n",
              "          [-1.6585e+02, -1.5765e+02, -1.6585e+02, -1.4485e+02],\n",
              "          [ 2.0551e+00,  2.2615e+01,  1.0683e+01,  2.6020e+01],\n",
              "          [-4.6141e+00,  2.2891e+01, -4.2236e+00,  3.8141e+01],\n",
              "          [-3.5218e+01, -2.3577e+03, -3.5191e+01,  2.4064e+03],\n",
              "          [ 1.1182e+01, -3.2051e+01,  1.1652e+01,  2.9925e+01],\n",
              "          [-1.3501e+03, -4.8356e-01,  1.4664e+03, -4.7680e-01],\n",
              "          [ 4.0212e+00, -4.6853e+01,  4.2001e+00,  1.6238e+01],\n",
              "          [ 1.0192e+02,  2.4776e+01,  1.0813e+02,  3.4048e+01],\n",
              "          [ 5.7556e+00, -1.2039e+02,  6.6793e+00,  1.2620e+02],\n",
              "          [ 1.2719e+01, -2.6153e+06,  1.2740e+01,  2.6154e+06],\n",
              "          [-2.3620e+04,  3.5187e+01,  2.3703e+04,  3.5187e+01],\n",
              "          [-7.0428e+00, -9.4472e+01,  2.1020e+01,  1.0638e+02],\n",
              "          [-3.9595e+03,  1.2319e+01,  3.9830e+03,  1.6597e+01],\n",
              "          [ 1.2589e+01, -9.7690e+00,  1.2736e+01, -9.3709e+00],\n",
              "          [ 9.3088e+00,  6.3777e+00,  9.4487e+00,  6.3778e+00],\n",
              "          [-2.6532e+01, -2.8080e+01, -1.5360e+01, -2.7961e+01],\n",
              "          [ 6.4102e+01, -8.1981e+01,  6.4108e+01,  1.4117e+02],\n",
              "          [ 3.7996e+00,  3.3312e-01,  7.7003e+00,  1.6055e+00],\n",
              "          [ 8.4974e+00, -8.6539e+00,  1.1109e+01,  1.1183e+01],\n",
              "          [ 9.1032e+00,  5.8691e+00,  9.3417e+00,  6.0371e+00],\n",
              "          [ 2.5386e+01, -1.3496e+02,  2.8513e+01,  2.2044e+02],\n",
              "          [-9.5447e+00, -1.0371e+01, -9.1921e+00, -1.0363e+01],\n",
              "          [ 2.9016e+00,  2.7740e-01,  3.1820e+00,  7.8517e-01],\n",
              "          [-2.9869e+01,  1.0733e+01,  2.9129e+01,  1.0779e+01],\n",
              "          [-7.8438e-01,  3.8673e+00,  1.2172e-01,  3.8903e+00],\n",
              "          [ 1.2230e+01,  6.5228e+00,  1.2239e+01,  6.5228e+00],\n",
              "          [ 1.9961e+00,  1.5396e+01,  2.7412e+00,  1.6590e+01],\n",
              "          [ 7.1632e+00,  1.3000e+01,  9.3428e+00,  1.3162e+01],\n",
              "          [ 1.1373e+01,  1.0884e+01,  1.1373e+01,  1.0886e+01],\n",
              "          [ 1.2629e+01,  9.0067e+00,  1.4294e+01,  9.0149e+00],\n",
              "          [-9.2719e-01,  1.2022e+01, -5.4651e-01,  1.2133e+01],\n",
              "          [-5.3196e+07,  2.3483e+01,  5.3196e+07,  2.3548e+01],\n",
              "          [-8.2018e-02,  9.4478e+00,  3.6117e+00,  1.0513e+01],\n",
              "          [ 3.6271e+01,  8.6159e+00,  3.9966e+01,  3.5458e+01],\n",
              "          [-4.8431e+01, -3.2875e+06,  1.0029e+02,  3.2876e+06],\n",
              "          [ 2.9253e+01,  1.5423e+01,  2.9802e+01,  3.2251e+01],\n",
              "          [-1.0360e+02, -1.0827e+02, -1.0360e+02, -1.0827e+02],\n",
              "          [-5.6387e-01, -5.6161e+00,  1.0186e+00, -5.5277e+00],\n",
              "          [-7.1720e+04, -4.5920e+02,  7.1942e+04,  5.7220e+02],\n",
              "          [-1.8350e+01,  8.7801e+00, -1.7753e+01,  8.9272e+00],\n",
              "          [-8.8517e-01, -3.5445e+00, -8.8517e-01, -3.5430e+00],\n",
              "          [ 1.1540e+01,  1.2037e+01,  2.3040e+01,  1.2275e+01],\n",
              "          [ 1.0695e+01,  2.9092e+01,  1.7236e+01,  3.2241e+01],\n",
              "          [ 3.5330e+00, -1.8535e+05,  3.5338e+00,  1.8539e+05],\n",
              "          [-1.3284e+01, -2.6736e+01, -1.2781e+01, -2.2038e+01],\n",
              "          [-1.8855e+00,  1.7480e+01,  1.9187e+01,  1.8968e+01],\n",
              "          [-1.2192e+04,  6.9664e+00,  1.2238e+04,  7.4012e+00],\n",
              "          [-3.4826e+01, -3.7557e+01, -3.4801e+01, -3.6146e+01],\n",
              "          [ 1.2511e+01,  8.7974e+00,  1.2755e+01,  9.7672e+00],\n",
              "          [-1.4987e-01,  6.7830e+00, -7.8547e-02,  7.1711e+00],\n",
              "          [ 2.7881e+00,  5.9370e+00,  2.7918e+00,  5.9768e+00],\n",
              "          [ 1.0359e+01,  1.1553e+01,  1.0359e+01,  1.1568e+01],\n",
              "          [ 1.1058e+01, -1.5808e+01,  1.1639e+01, -1.5533e+01],\n",
              "          [-3.4656e+01,  1.8977e+00, -8.6810e+00,  2.1949e+00],\n",
              "          [ 9.2526e+00,  4.1634e+00,  9.7768e+00,  5.9093e+00],\n",
              "          [ 1.6937e-01,  2.5982e+01,  1.3240e+00,  2.6246e+01],\n",
              "          [ 6.4465e+00, -7.0413e+04,  6.4465e+00,  7.0489e+04],\n",
              "          [-4.5122e+01, -2.4224e+01, -4.4925e+01, -2.4215e+01],\n",
              "          [-1.1268e+02, -9.7990e+01, -1.1268e+02, -9.6315e+01],\n",
              "          [ 1.8748e+01,  1.7294e+01,  1.9630e+01,  1.9951e+01],\n",
              "          [ 8.9598e-01, -1.5525e+01,  1.9976e+00,  5.1136e+00],\n",
              "          [-3.5665e+00,  3.0303e+00,  2.2974e-01,  3.6511e+00],\n",
              "          [-3.0220e+01,  4.1978e+00, -3.0063e+01,  4.2326e+00],\n",
              "          [ 2.5434e+01,  2.2916e+01,  2.5455e+01,  2.7973e+01],\n",
              "          [-9.3364e+00,  1.1105e+00, -8.7628e+00,  1.6274e+00],\n",
              "          [-3.9250e+01, -5.2091e+01, -3.9250e+01, -5.2091e+01],\n",
              "          [ 1.7114e+01,  3.8544e+01,  5.0823e+01,  3.9270e+01],\n",
              "          [-1.8581e+01, -2.2321e+01, -1.8580e+01, -2.2321e+01],\n",
              "          [ 1.1457e+01,  1.2931e+00,  1.3125e+01,  9.9141e+00],\n",
              "          [-3.6239e+01, -5.5226e-01, -3.6239e+01, -5.5226e-01],\n",
              "          [ 1.5278e+01, -1.7928e+03,  3.0459e+01,  1.8314e+03],\n",
              "          [ 3.8557e+00, -1.3201e+00,  3.8644e+00, -1.3200e+00],\n",
              "          [-1.2738e+05, -9.1771e+05,  1.2742e+05,  9.1770e+05],\n",
              "          [ 2.4318e+00,  5.6234e+00,  2.7994e+00,  7.4255e+00],\n",
              "          [-1.5906e+01,  2.8244e+00, -9.9015e+00,  3.8644e+00],\n",
              "          [ 5.0198e+00,  1.5309e+01,  5.5880e+00,  1.5309e+01],\n",
              "          [-2.5982e+03, -2.4965e+03,  2.6177e+03,  2.5094e+03],\n",
              "          [ 1.3493e+01, -1.7545e+00,  1.3918e+01,  1.2905e+00],\n",
              "          [-2.4673e+01,  7.5950e+00,  3.7645e+01,  8.6589e+00],\n",
              "          [ 5.7273e+00,  1.8961e+01,  2.3836e+01,  2.0150e+01],\n",
              "          [-1.7747e+01, -4.8550e+01, -1.7747e+01, -4.8550e+01],\n",
              "          [-1.2033e+01, -1.4716e+01, -1.2032e+01, -1.4716e+01],\n",
              "          [-3.3384e-01,  4.7947e+00, -3.3217e-01,  5.0847e+00],\n",
              "          [ 1.6110e+01,  3.1228e+00,  1.6112e+01,  7.2259e+00],\n",
              "          [ 2.3258e+00, -1.9074e+01,  2.5340e+00, -1.7778e+01],\n",
              "          [ 8.5591e+00, -1.0333e+01,  1.1621e+01, -1.0333e+01],\n",
              "          [ 2.6287e+00, -2.8287e+01,  2.6291e+00, -2.8286e+01],\n",
              "          [ 1.1375e+01, -3.4864e+00,  4.5127e+01, -2.2844e+00],\n",
              "          [-2.0809e+03,  1.7240e+01,  2.1412e+03,  2.0474e+01],\n",
              "          [-1.5551e+02, -1.0714e+02, -1.5437e+02, -1.0714e+02],\n",
              "          [ 1.5110e+01,  1.7954e+01,  1.5357e+01,  2.1879e+01],\n",
              "          [-2.1527e+02, -1.9373e+02,  2.6233e+02,  2.3877e+02],\n",
              "          [ 7.1263e+00,  1.0128e+01,  9.7937e+00,  1.0362e+01],\n",
              "          [-2.5561e+00,  9.3905e+00,  1.4423e+01,  1.7793e+01],\n",
              "          [ 1.1279e+01,  2.2822e+00,  1.3991e+01,  2.6388e+00],\n",
              "          [-1.7807e-01, -2.1596e+00,  6.0994e-02, -2.1588e+00],\n",
              "          [-7.3116e+00, -6.9654e+01, -7.2099e+00, -6.9650e+01],\n",
              "          [-5.9863e+00, -1.7906e+00, -5.1402e+00, -1.7178e+00],\n",
              "          [ 1.5235e+00,  2.0578e+00,  1.5292e+00,  2.0630e+00],\n",
              "          [ 2.1324e+01,  1.7878e+01,  2.1340e+01,  1.8060e+01],\n",
              "          [ 2.3725e+01,  1.1131e+01,  2.3950e+01,  1.2456e+01],\n",
              "          [ 2.3559e+00,  2.0932e+01,  2.4623e+00,  2.3691e+01],\n",
              "          [-1.2073e+00, -8.3622e+00, -1.0700e+00, -7.1407e+00],\n",
              "          [-3.9197e-01, -5.1087e+02, -2.9816e-01,  5.1909e+02],\n",
              "          [-1.2451e+01, -2.5457e+01, -1.2451e+01, -2.5415e+01],\n",
              "          [ 5.9902e+00, -2.5544e+01,  5.9911e+00, -2.5544e+01],\n",
              "          [ 8.2815e+00, -1.3438e+01,  8.6514e+00, -1.3369e+01],\n",
              "          [-1.2084e+01, -8.4346e+00, -7.7735e+00, -7.1161e+00],\n",
              "          [-1.2081e+02,  4.3683e+01,  1.8018e+02,  4.4816e+01],\n",
              "          [ 2.7630e+00,  6.3248e+00,  2.7685e+00,  6.4340e+00],\n",
              "          [-2.6487e+01, -4.1074e+00, -2.3625e+01, -1.2704e+00],\n",
              "          [ 7.7226e+00,  8.5460e-01,  8.5380e+00,  8.6222e-01],\n",
              "          [-1.6327e+02, -9.7404e+00,  9.9025e+01, -9.7382e+00],\n",
              "          [-3.0577e+01, -2.2698e+04,  5.6256e+01,  2.2702e+04],\n",
              "          [ 4.8529e+01, -1.5990e+05,  4.8580e+01,  1.5989e+05],\n",
              "          [ 1.2130e+01, -7.8261e+01,  1.3439e+01,  8.4917e+01],\n",
              "          [ 1.3701e+01,  4.8465e+01,  1.9368e+01,  5.0540e+01],\n",
              "          [-2.6638e+04,  5.2707e+00,  2.6610e+04,  5.8563e+00],\n",
              "          [-3.7874e+00,  3.1203e+00, -3.5685e+00,  3.2105e+00],\n",
              "          [-5.1206e+00,  1.2826e+01, -5.1188e+00,  1.2926e+01],\n",
              "          [-7.7239e+00,  6.9132e+00, -7.6060e+00,  6.9192e+00],\n",
              "          [ 1.6635e+01, -1.2602e+01,  1.6951e+01, -1.1508e+01],\n",
              "          [-2.8941e+00,  1.0836e+01, -2.6059e+00,  1.1182e+01],\n",
              "          [-7.2164e+00,  1.1823e+00, -6.9987e+00,  4.6830e+00],\n",
              "          [ 8.3202e+00, -1.0041e+01,  8.3262e+00, -9.9912e+00],\n",
              "          [-1.1134e-01, -9.4178e+03, -8.7676e-02,  9.4756e+03],\n",
              "          [-8.4944e+00, -2.4820e+01, -8.4198e+00, -2.4693e+01],\n",
              "          [ 7.3383e+00,  2.7205e+01,  3.0463e+01,  2.8885e+01],\n",
              "          [ 4.2397e+00,  9.9852e+00,  4.4713e+00,  1.2621e+01],\n",
              "          [-2.8858e+07, -1.0247e+02,  2.8858e+07,  2.3540e+02],\n",
              "          [ 3.3724e+00, -5.2843e+00,  3.3745e+00, -5.2798e+00],\n",
              "          [-2.5053e+01, -1.9585e+01, -2.5053e+01, -1.9492e+01],\n",
              "          [ 9.2935e-01,  4.1321e+00,  1.0282e+00,  4.1399e+00],\n",
              "          [ 1.0646e+01, -1.0821e+01,  1.0928e+01, -1.0493e+01],\n",
              "          [-4.1625e+00, -5.2564e+01, -4.1625e+00, -5.2480e+01],\n",
              "          [-1.3609e+03,  2.1361e+01,  1.3862e+03,  2.2214e+01],\n",
              "          [ 4.0572e-01, -1.5855e+00,  4.1482e-01, -1.5854e+00],\n",
              "          [ 1.2450e+01,  1.8205e+01,  1.2990e+01,  1.8207e+01],\n",
              "          [-4.9328e+00,  1.0257e+01, -4.9327e+00,  1.0257e+01],\n",
              "          [ 2.2977e+01, -4.4762e+02,  2.3360e+01,  4.7126e+02],\n",
              "          [ 9.0412e+00, -1.4446e+01,  9.0435e+00, -1.4446e+01],\n",
              "          [-1.7113e+01, -7.0073e+00, -1.7103e+01, -6.5937e+00],\n",
              "          [-2.2106e+08, -6.4774e+06,  2.2106e+08,  6.4775e+06],\n",
              "          [-9.3976e-01,  3.3968e+01,  3.8562e+01,  4.4135e+01],\n",
              "          [ 1.5837e+01,  9.5375e+00,  1.6918e+01,  1.8802e+01],\n",
              "          [ 1.9235e+01,  5.6731e+00,  2.1568e+01,  8.0043e+00],\n",
              "          [-8.8886e+00,  5.7015e+00, -8.8529e+00,  8.8603e+00],\n",
              "          [-5.5096e+00, -7.0025e+01, -5.4637e+00, -6.5709e+01],\n",
              "          [-2.4708e+00, -4.3795e+01, -2.4707e+00, -4.3795e+01],\n",
              "          [-1.5512e+01, -3.7171e+00, -1.5204e+01, -2.2616e+00],\n",
              "          [ 6.9726e+00, -7.4076e+01,  7.7958e+00,  1.1611e+02],\n",
              "          [ 1.5861e+00, -8.5826e+00,  1.6177e+00, -7.6273e+00],\n",
              "          [-1.1484e+01, -8.3929e+01, -1.1484e+01, -1.0386e+01],\n",
              "          [ 4.0911e+00, -9.0435e+00,  6.2792e+00, -2.0209e+00],\n",
              "          [-1.0294e+07, -5.9156e+04,  1.0294e+07,  5.9208e+04],\n",
              "          [ 1.1209e+01,  1.5185e+01,  1.1679e+01,  1.7050e+01],\n",
              "          [ 7.1473e+00, -3.0073e+00,  7.1481e+00,  1.9209e+01],\n",
              "          [ 1.1936e+01,  1.2026e+01,  1.1977e+01,  1.4228e+01],\n",
              "          [ 2.7077e+00, -1.0710e+00,  2.7344e+00, -1.0501e+00],\n",
              "          [-2.0063e+01,  3.2830e+01,  7.2744e+01,  3.3424e+01],\n",
              "          [ 6.0056e+01,  1.5276e+01,  6.0547e+01,  1.6012e+01],\n",
              "          [ 2.2412e+00,  9.8349e+00,  2.5646e+00,  1.2261e+01],\n",
              "          [-1.9678e+01, -5.7165e+00, -1.9380e+01, -5.3786e+00],\n",
              "          [-2.9900e+01, -2.7120e+01, -2.9900e+01, -2.7120e+01],\n",
              "          [ 5.1811e+00,  9.5619e+00,  5.8733e+00,  9.8542e+00],\n",
              "          [-2.6283e+01, -1.6192e+00, -2.6235e+01, -1.5990e+00],\n",
              "          [ 9.4274e+00,  4.1808e+01,  9.4378e+00,  4.9682e+01],\n",
              "          [-1.7079e+01, -2.4118e+00, -1.7070e+01, -2.0479e+00],\n",
              "          [ 2.8652e+01, -3.8333e+02,  2.9176e+01,  4.5761e+02],\n",
              "          [ 1.5837e+02, -1.3307e+02,  1.5907e+02,  2.3760e+02],\n",
              "          [ 5.7446e+00,  1.2418e+01,  5.9239e+00,  1.2488e+01],\n",
              "          [-7.7552e+00,  1.4731e+01, -5.1655e+00,  1.5234e+01],\n",
              "          [ 1.2190e+01, -6.7004e+00,  1.2194e+01, -6.6947e+00],\n",
              "          [-2.6092e+01, -4.4032e+01, -2.6092e+01, -4.4032e+01],\n",
              "          [ 6.5992e+00, -4.7593e+01,  6.6784e+00,  2.9264e+01],\n",
              "          [-1.3795e+01, -2.2373e+01, -1.3795e+01, -2.2373e+01],\n",
              "          [ 1.1406e+01, -4.1244e+00,  1.1728e+01, -4.1151e+00],\n",
              "          [-2.7401e-01,  1.4402e+01, -2.2079e-01,  1.4420e+01],\n",
              "          [-1.5289e+00, -2.3514e+02, -1.3708e+00,  2.7202e+02],\n",
              "          [ 1.4650e+01,  3.3210e+01,  1.5963e+01,  1.0181e+02],\n",
              "          [-9.4885e+00,  1.0698e+01, -8.6159e+00,  1.1024e+01],\n",
              "          [ 6.4783e+00, -8.6675e+00,  7.1844e+00, -8.2991e+00],\n",
              "          [ 9.7240e+00,  7.0395e+00,  9.7321e+00,  7.0398e+00],\n",
              "          [ 8.8217e+00,  1.4291e+01,  9.1932e+00,  1.4373e+01],\n",
              "          [-2.5215e+00, -8.2535e+00, -1.7878e+00,  8.7041e-01],\n",
              "          [ 2.2489e+01,  1.6714e+01,  2.2648e+01,  1.7288e+01],\n",
              "          [-4.1421e+01, -4.8407e+01,  1.3850e+00, -4.8407e+01],\n",
              "          [-1.8845e+01, -3.5485e+01, -1.8715e+01, -3.5221e+01]], device='cuda:0')],\n",
              " [tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
              "          0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
              "          0.9996, 0.9996, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9994,\n",
              "          0.9994, 0.9994, 0.9994, 0.9994, 0.9994, 0.9994, 0.9994, 0.9994, 0.9994,\n",
              "          0.9994, 0.9994, 0.9994, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9993,\n",
              "          0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9992,\n",
              "          0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9990, 0.9990, 0.9990, 0.9990,\n",
              "          0.9990, 0.9989, 0.9989, 0.9989, 0.9989, 0.9988, 0.9988, 0.9988, 0.9988,\n",
              "          0.9988, 0.9988, 0.9987, 0.9986, 0.9986, 0.9986, 0.9986, 0.9985, 0.9985,\n",
              "          0.9984, 0.9984, 0.9984, 0.9984, 0.9984, 0.9984, 0.9983, 0.9983, 0.9983,\n",
              "          0.9983, 0.9982, 0.9981, 0.9980, 0.9979, 0.9979, 0.9979, 0.9978, 0.9976,\n",
              "          0.9976, 0.9974, 0.9974, 0.9972, 0.9971, 0.9971, 0.9970, 0.9968, 0.9967,\n",
              "          0.9966, 0.9965, 0.9965, 0.9964, 0.9962, 0.9960, 0.9959, 0.9959, 0.9958,\n",
              "          0.9954, 0.9953, 0.9952, 0.9951, 0.9951, 0.9951, 0.9945, 0.9945, 0.9942,\n",
              "          0.9942, 0.9941, 0.9941, 0.9938, 0.9937, 0.9936, 0.9935, 0.9935, 0.9935,\n",
              "          0.9933, 0.9932, 0.9932, 0.9930, 0.9929, 0.9927, 0.9925, 0.9922, 0.9921,\n",
              "          0.9920, 0.9919, 0.9917, 0.9916, 0.9913, 0.9911, 0.9910, 0.9910, 0.9909,\n",
              "          0.9906, 0.9906, 0.9905, 0.9904, 0.9902], device='cuda:0'),\n",
              "  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9995, 0.9995, 0.9995, 0.9995, 0.9994, 0.9994, 0.9994,\n",
              "          0.9993, 0.9993, 0.9992, 0.9991, 0.9991, 0.9990, 0.9990, 0.9990, 0.9990,\n",
              "          0.9990, 0.9990, 0.9990, 0.9989, 0.9989, 0.9989, 0.9988, 0.9988, 0.9987,\n",
              "          0.9983, 0.9981, 0.9981, 0.9980, 0.9978, 0.9977, 0.9977, 0.9976, 0.9975,\n",
              "          0.9975, 0.9974, 0.9973, 0.9972, 0.9971, 0.9970, 0.9970, 0.9969, 0.9968,\n",
              "          0.9967, 0.9967, 0.9966, 0.9965, 0.9965, 0.9965, 0.9964, 0.9962, 0.9960,\n",
              "          0.9959, 0.9958, 0.9951, 0.9946, 0.9943, 0.9943, 0.9942, 0.9941, 0.9941,\n",
              "          0.9940, 0.9940, 0.9939, 0.9939, 0.9938, 0.9937, 0.9935, 0.9934, 0.9934,\n",
              "          0.9934, 0.9934, 0.9933, 0.9931, 0.9930, 0.9929, 0.9926, 0.9925, 0.9925,\n",
              "          0.9924, 0.9922, 0.9919, 0.9918, 0.9915, 0.9915, 0.9915, 0.9914, 0.9914,\n",
              "          0.9912, 0.9912, 0.9909, 0.9905, 0.9904, 0.9903, 0.9903],\n",
              "         device='cuda:0')],\n",
              " [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'),\n",
              "  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0], device='cuda:0')])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        # return proposals_final.to(device), conf_scores_final.to(device), feature_map.to(device), [out_c, out_h, out_w]\n",
        "        # return torch.Tensor(proposals_final), torch.Tensor(conf_scores_final), torch.Tensor(feature_map), [out_c, out_h, out_w]\n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        print('len(proposals_final)', len(proposals_final), proposals_final[0].shape)\n",
        "        print('len(conf_scores_final)', len(conf_scores_final), conf_scores_final[0].shape)\n",
        "        print('len(feature_map)', len(feature_map), feature_map[0].shape)\n",
        "        print(out_size)\n",
        "        # print(conf_scores_final)\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        # print(classes_all)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "d = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "\n",
        "d.eval()\n",
        "proposals_final, conf_scores_final, classes_final = d.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "# proposals_final.shape, conf_scores_final.shape, classes_final.shape\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "3370b7e8-0656-415b-de80-0489964f6b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|| 97.8M/97.8M [00:01<00:00, 74.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            print(total_loss, loss.item())\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFnKE3neESZ5",
        "outputId": "727581b6-16ac-4d25-e390-e86bed5e9c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.6184005737304688\n",
            "2.6184005737304688 23.65468978881836\n",
            "26.273090362548828 2.2658908367156982\n",
            "28.538981199264526 6.901319980621338\n",
            "35.440301179885864 8.789301872253418\n",
            "44.22960305213928 10.907424926757812\n",
            "55.137027978897095 0.9401548504829407\n",
            "56.077182829380035 9.99724006652832\n",
            "66.07442289590836 1.4310818910598755\n",
            "67.50550478696823 2.5466811656951904\n",
            "70.05218595266342 4.264460563659668\n",
            "74.31664651632309 1.7338368892669678\n",
            "76.05048340559006 2.4935812950134277\n",
            "78.54406470060349 21.535198211669922\n",
            "100.0792629122734 1.5896161794662476\n",
            "101.66887909173965 1.5121989250183105\n",
            "103.18107801675797 1.7422114610671997\n",
            "104.92328947782516 1.7301586866378784\n",
            "106.65344816446304 94.78871154785156\n",
            "201.4421597123146 1.7585232257843018\n",
            "203.2006829380989 1.60812509059906\n",
            "204.80880802869797 15.770227432250977\n",
            "220.57903546094894 3.201777935028076\n",
            "223.78081339597702 1.402453899383545\n",
            "225.18326729536057 73.49263000488281\n",
            "298.6758973002434 2.563847303390503\n",
            "301.2397446036339 1.9615131616592407\n",
            "303.2012577652931 13.314797401428223\n",
            "316.51605516672134 10.090576171875\n",
            "326.60663133859634 1.4074969291687012\n",
            "328.01412826776505 11.844290733337402\n",
            "339.85841900110245 17.485347747802734\n",
            "357.3437667489052 0.828875720500946\n",
            "358.1726424694061 1.8532952070236206\n",
            "360.02593767642975 50.73966598510742\n",
            "410.76560366153717 0.7748245000839233\n",
            "411.5404281616211 2.166525363922119\n",
            "413.7069535255432 2.3149542808532715\n",
            "416.0219078063965 1.804006576538086\n",
            "417.82591438293457 3.9635510444641113\n",
            "421.7894654273987 1.7400001287460327\n",
            "423.5294655561447 5.1835246086120605\n",
            "428.7129901647568 0.8063112497329712\n",
            "429.51930141448975 2.273155689239502\n",
            "431.79245710372925 77.36848449707031\n",
            "509.16094160079956 2.755962371826172\n",
            "511.91690397262573 1.4669392108917236\n",
            "513.3838431835175 2.2461977005004883\n",
            "515.630040884018 67.05140686035156\n",
            "582.6814477443695 1.5813058614730835\n",
            "584.2627536058426 5.613164901733398\n",
            "589.875918507576 2.5694313049316406\n",
            "592.4453498125076 1.5592775344848633\n",
            "594.0046273469925 2.2819669246673584\n",
            "596.2865942716599 1.5241774320602417\n",
            "597.8107717037201 2.605531692504883\n",
            "600.416303396225 20.772594451904297\n",
            "621.1888978481293 0.7558737993240356\n",
            "621.9447716474533 16.421367645263672\n",
            "638.366139292717 2.908682107925415\n",
            "641.2748214006424 15.28121566772461\n",
            "656.556037068367 2.0938048362731934\n",
            "658.6498419046402 85.78266143798828\n",
            "744.4325033426285 3.6852211952209473\n",
            "748.1177245378494 1.070618748664856\n",
            "749.1883432865143 33.7508430480957\n",
            "782.93918633461 3.8680648803710938\n",
            "786.8072512149811 2.058297634124756\n",
            "788.8655488491058 2.284295082092285\n",
            "791.1498439311981 2.7931864261627197\n",
            "793.9430303573608 47.36445999145508\n",
            "841.3074903488159 2.121253728866577\n",
            "843.4287440776825 2.042429208755493\n",
            "845.471173286438 1.225441336631775\n",
            "846.6966146230698 2.7833175659179688\n",
            "849.4799321889877 2.545590400695801\n",
            "852.0255225896835 1.336349606513977\n",
            "853.3618721961975 2.7914230823516846\n",
            "856.1532952785492 1.8173080682754517\n",
            "857.9706033468246 11.20767879486084\n",
            "869.1782821416855 4.079339981079102\n",
            "873.2576221227646 1.7360912561416626\n",
            "874.9937133789062 1.5940607786178589\n",
            "876.5877741575241 1.5551141500473022\n",
            "878.1428883075714 4.297748565673828\n",
            "882.4406368732452 1.7382569313049316\n",
            "884.1788938045502 1.4197323322296143\n",
            "885.5986261367798 27.47757339477539\n",
            "913.0761995315552 1.6724601984024048\n",
            "914.7486597299576 0.7103071212768555\n",
            "915.4589668512344 1.7106974124908447\n",
            "917.1696642637253 1.1171917915344238\n",
            "918.2868560552597 1.8144316673278809\n",
            "920.1012877225876 1.5595134496688843\n",
            "921.6608011722565 23.726465225219727\n",
            "945.3872663974762 3.6025843620300293\n",
            "948.9898507595062 13.054248809814453\n",
            "962.0440995693207 0.7811622023582458\n",
            "962.8252617716789 3.8051204681396484\n",
            "966.6303822398186 4.326003074645996\n",
            "970.9563853144646 31.540424346923828\n",
            "1002.4968096613884 5.699407577514648\n",
            "1008.196217238903 4.88056755065918\n",
            "1013.0767847895622 2.290017604827881\n",
            "1015.3668023943901 13.689665794372559\n",
            "1029.0564681887627 2.013765335083008\n",
            "1031.0702335238457 2.1362624168395996\n",
            "1033.2064959406853 11.545254707336426\n",
            "1044.7517506480217 0.7546623349189758\n",
            "1045.5064129829407 9.26434326171875\n",
            "1054.7707562446594 1.6782056093215942\n",
            "1056.448961853981 0.8837389349937439\n",
            "1057.3327007889748 3.048661708831787\n",
            "1060.3813624978065 1.756687045097351\n",
            "1062.138049542904 2.555298328399658\n",
            "1064.6933478713036 5.032584190368652\n",
            "1069.7259320616722 1.770198106765747\n",
            "1071.496130168438 3.240187168121338\n",
            "1074.7363173365593 3.409985065460205\n",
            "1078.1463024020195 39.05699920654297\n",
            "1117.2033016085625 0.9577039480209351\n",
            "1118.1610055565834 1.5804792642593384\n",
            "1119.7414848208427 1.6372406482696533\n",
            "1121.3787254691124 2.4076056480407715\n",
            "1123.7863311171532 0.8147848844528198\n",
            "1124.601116001606 2.015535831451416\n",
            "1126.6166518330574 2.705233335494995\n",
            "1129.3218851685524 3.698904275894165\n",
            "1133.0207894444466 5.336291790008545\n",
            "1138.357081234455 3.8636441230773926\n",
            "1142.2207253575325 4.782341003417969\n",
            "1147.0030663609505 2.007744312286377\n",
            "1149.0108106732368 2.543891668319702\n",
            "1151.5547023415565 6.655951499938965\n",
            "1158.2106538414955 5.122986793518066\n",
            "1163.3336406350136 2.221249580383301\n",
            "1165.5548902153969 1.5759756565093994\n",
            "1167.1308658719063 3.1973876953125\n",
            "1170.3282535672188 1.9088969230651855\n",
            "1172.237150490284 1.713670253753662\n",
            "1173.9508207440376 1.4494240283966064\n",
            "1175.4002447724342 0.8612456321716309\n",
            "1176.2614904046059 4.1510725021362305\n",
            "1180.412562906742 4.335943222045898\n",
            "1184.748506128788 1.854738473892212\n",
            "1186.6032446026802 2.664591073989868\n",
            "1189.26783567667 2.0009825229644775\n",
            "1191.2688181996346 1.7796934843063354\n",
            "1193.048511683941 2.1744794845581055\n",
            "1195.222991168499 1.558527946472168\n",
            "1196.7815191149712 7.787595748901367\n",
            "1204.5691148638725 7.117952346801758\n",
            "1211.6870672106743 11.084912300109863\n",
            "1222.7719795107841 0.7905785441398621\n",
            "1223.562558054924 23.838905334472656\n",
            "1247.4014633893967 1.590875267982483\n",
            "1248.9923386573792 8.75576114654541\n",
            "1257.7480998039246 0.8618998527526855\n",
            "1258.6099996566772 4.462152481079102\n",
            "1263.0721521377563 79.22766876220703\n",
            "1342.2998208999634 1.7832005023956299\n",
            "1344.083021402359 0.43068641424179077\n",
            "1344.5137078166008 0.8501147031784058\n",
            "1345.3638225197792 nan\n",
            "1345.3638225197792 1.885488510131836\n",
            "1347.249311029911 18.91708755493164\n",
            "1366.1663985848427 1.8260154724121094\n",
            "1367.9924140572548 1.7376259565353394\n",
            "1369.7300400137901 15.896918296813965\n",
            "1385.626958310604 2.5174546241760254\n",
            "1388.1444129347801 16.019105911254883\n",
            "1404.163518846035 4.102347373962402\n",
            "1408.2658662199974 1.469274878501892\n",
            "1409.7351410984993 9.90977668762207\n",
            "1419.6449177861214 1.4856151342391968\n",
            "1421.1305329203606 3.527552604675293\n",
            "1424.6580855250359 2.853349447250366\n",
            "1427.5114349722862 1.847766399383545\n",
            "1429.3592013716698 20.052967071533203\n",
            "1449.412168443203 4.120203971862793\n",
            "1453.5323724150658 0.826356053352356\n",
            "1454.3587284684181 2.4168083667755127\n",
            "1456.7755368351936 0.866884171962738\n",
            "1457.6424210071564 2.943258762359619\n",
            "1460.585679769516 0.7844362258911133\n",
            "1461.370115995407 16.00653839111328\n",
            "1477.3766543865204 1.2983702421188354\n",
            "1478.6750246286392 3.2322139739990234\n",
            "1481.9072386026382 1.9443092346191406\n",
            "1483.8515478372574 8.159903526306152\n",
            "1492.0114513635635 1.7035225629806519\n",
            "1493.7149739265442 17.42537498474121\n",
            "1511.1403489112854 1.6798150539398193\n",
            "1512.8201639652252 2.447172164916992\n",
            "1515.2673361301422 nan\n",
            "1515.2673361301422 3.673617124557495\n",
            "1518.9409532546997 1.9079627990722656\n",
            "1520.848916053772 1.6439663171768188\n",
            "1522.4928823709488 1.5816587209701538\n",
            "1524.074541091919 16.137866973876953\n",
            "1540.212408065796 2.2671563625335693\n",
            "1542.4795644283295 3.122429370880127\n",
            "1545.6019937992096 1.581801414489746\n",
            "1547.1837952136993 7.454376220703125\n",
            "1554.6381714344025 0.7480700016021729\n",
            "1555.3862414360046 7.555171966552734\n",
            "1562.9414134025574 7.417031288146973\n",
            "1570.3584446907043 1.1876370906829834\n",
            "1571.5460817813873 4.000438213348389\n",
            "1575.5465199947357 3.5796756744384766\n",
            "1579.1261956691742 6.55754280090332\n",
            "1585.6837384700775 2.387820243835449\n",
            "1588.071558713913 6.057025909423828\n",
            "1594.1285846233368 0.7547668218612671\n",
            "1594.883351445198 1.7222867012023926\n",
            "1596.6056381464005 2.239945888519287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 1/2 [02:23<02:23, 143.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1598.8455840349197 2.056919574737549\n",
            "0 1.3812742233276367\n",
            "1.3812742233276367 3.47056245803833\n",
            "4.851836681365967 2.2633981704711914\n",
            "7.115234851837158 4.393244743347168\n",
            "11.508479595184326 1.7293665409088135\n",
            "13.23784613609314 0.7374294400215149\n",
            "13.975275576114655 1.0127100944519043\n",
            "14.987985670566559 4.368485450744629\n",
            "19.356471121311188 1.460405707359314\n",
            "20.8168768286705 18.058034896850586\n",
            "38.87491172552109 4.6087751388549805\n",
            "43.48368686437607 1.9340283870697021\n",
            "45.41771525144577 3.311811685562134\n",
            "48.729526937007904 2.002375602722168\n",
            "50.73190253973007 4.437216281890869\n",
            "55.16911882162094 1.5520678758621216\n",
            "56.72118669748306 2.1004838943481445\n",
            "58.82167059183121 1.8191028833389282\n",
            "60.640773475170135 109.179443359375\n",
            "169.82021683454514 2.0296530723571777\n",
            "171.8498699069023 8.902215957641602\n",
            "180.75208586454391 6.748117446899414\n",
            "187.50020331144333 17.020845413208008\n",
            "204.52104872465134 2.047391891479492\n",
            "206.56844061613083 18.37682342529297\n",
            "224.9452640414238 17.168046951293945\n",
            "242.11331099271774 2.338721513748169\n",
            "244.4520325064659 1.7452750205993652\n",
            "246.19730752706528 2.405040740966797\n",
            "248.60234826803207 1.5207092761993408\n",
            "250.12305754423141 11.495702743530273\n",
            "261.6187602877617 37.68951416015625\n",
            "299.30827444791794 0.8838735818862915\n",
            "300.19214802980423 3.739734172821045\n",
            "303.9318822026253 6.847094535827637\n",
            "310.7789767384529 0.7291108965873718\n",
            "311.5080876350403 1.9899754524230957\n",
            "313.4980630874634 2.2463276386260986\n",
            "315.7443907260895 4.518764019012451\n",
            "320.26315474510193 3.8087587356567383\n",
            "324.07191348075867 1.5525970458984375\n",
            "325.6245105266571 28.904888153076172\n",
            "354.5293986797333 24.037839889526367\n",
            "378.56723856925964 4.865521430969238\n",
            "383.4327600002289 58.94535446166992\n",
            "442.3781144618988 10.858240127563477\n",
            "453.2363545894623 9.482806205749512\n",
            "462.7191607952118 3.458064556121826\n",
            "466.1772253513336 29.544185638427734\n",
            "495.72141098976135 4.747491359710693\n",
            "500.46890234947205 3.5854451656341553\n",
            "504.0543475151062 1.3842781782150269\n",
            "505.4386256933212 1.7280750274658203\n",
            "507.16670072078705 3.0808863639831543\n",
            "510.2475870847702 1.5878329277038574\n",
            "511.83542001247406 2.221661329269409\n",
            "514.0570813417435 8.769760131835938\n",
            "522.8268414735794 0.7278710007667542\n",
            "523.5547124743462 4.517182350158691\n",
            "528.0718948245049 4.5976762771606445\n",
            "532.6695711016655 12.16917610168457\n",
            "544.8387472033501 2.2505459785461426\n",
            "547.0892931818962 85.54405212402344\n",
            "632.6333453059196 3.562005043029785\n",
            "636.1953503489494 1.1347954273223877\n",
            "637.3301457762718 1.6408023834228516\n",
            "638.9709481596947 1.7134852409362793\n",
            "640.684433400631 2.2284069061279297\n",
            "642.9128403067589 2.0798451900482178\n",
            "644.9926854968071 2.7243685722351074\n",
            "647.7170540690422 43.639068603515625\n",
            "691.3561226725578 2.0991568565368652\n",
            "693.4552795290947 2.1788296699523926\n",
            "695.6341091990471 2.843735456466675\n",
            "698.4778446555138 2.792808771133423\n",
            "701.2706534266472 3.223968505859375\n",
            "704.4946219325066 1.6410353183746338\n",
            "706.1356572508812 1.5031161308288574\n",
            "707.63877338171 1.8499531745910645\n",
            "709.4887265563011 3.120965003967285\n",
            "712.6096915602684 4.217034339904785\n",
            "716.8267259001732 1.690263032913208\n",
            "718.5169889330864 1.6447741985321045\n",
            "720.1617631316185 1.3995810747146606\n",
            "721.5613442063332 3.9514667987823486\n",
            "725.5128110051155 1.7458524703979492\n",
            "727.2586634755135 1.4809136390686035\n",
            "728.7395771145821 9.423643112182617\n",
            "738.1632202267647 1.807145357131958\n",
            "739.9703655838966 0.7468631863594055\n",
            "740.717228770256 1.7011898756027222\n",
            "742.4184186458588 1.066788911819458\n",
            "743.4852075576782 1.799078345298767\n",
            "745.284285902977 1.6301229000091553\n",
            "746.9144088029861 22.971521377563477\n",
            "769.8859301805496 4.37156343460083\n",
            "774.2574936151505 13.514444351196289\n",
            "787.7719379663467 0.4559198319911957\n",
            "788.2278577983379 3.101182222366333\n",
            "791.3290400207043 1.7766990661621094\n",
            "793.1057390868664 26.800533294677734\n",
            "819.9062723815441 5.495378494262695\n",
            "825.4016508758068 4.664788722991943\n",
            "830.0664395987988 2.3554797172546387\n",
            "832.4219193160534 14.069497108459473\n",
            "846.4914164245129 1.6464455127716064\n",
            "848.1378619372845 2.200043201446533\n",
            "850.337905138731 8.328897476196289\n",
            "858.6668026149273 0.8787539005279541\n",
            "859.5455565154552 31.78587532043457\n",
            "891.3314318358898 1.6188191175460815\n",
            "892.9502509534359 0.8890088200569153\n",
            "893.8392597734928 2.293846845626831\n",
            "896.1331066191196 2.483522415161133\n",
            "898.6166290342808 2.3182244300842285\n",
            "900.934853464365 5.218792915344238\n",
            "906.1536463797092 1.8300526142120361\n",
            "907.9836989939213 2.6261792182922363\n",
            "910.6098782122135 7.734257698059082\n",
            "918.3441359102726 4.09574031829834\n",
            "922.4398762285709 3.3053157329559326\n",
            "925.7451919615269 5.102560043334961\n",
            "930.8477520048618 1.508800983428955\n",
            "932.3565529882908 2.100611925125122\n",
            "934.4571649134159 0.7783679366111755\n",
            "935.2355328500271 1.9586822986602783\n",
            "937.1942151486874 3.0288352966308594\n",
            "940.2230504453182 2.3771770000457764\n",
            "942.600227445364 4.977286338806152\n",
            "947.5775137841702 2.0176005363464355\n",
            "949.5951143205166 1.9694404602050781\n",
            "951.5645547807217 2.3182249069213867\n",
            "953.882779687643 2.1632847785949707\n",
            "956.046064466238 6.04630184173584\n",
            "962.0923663079739 5.667715072631836\n",
            "967.7600813806057 2.4739279747009277\n",
            "970.2340093553066 1.5894994735717773\n",
            "971.8235088288784 3.353224277496338\n",
            "975.1767331063747 1.488070011138916\n",
            "976.6648031175137 1.5996861457824707\n",
            "978.2644892632961 2.8624484539031982\n",
            "981.1269377171993 0.8413761854171753\n",
            "981.9683139026165 3.8962135314941406\n",
            "985.8645274341106 3.5653181076049805\n",
            "989.4298455417156 1.9677358865737915\n",
            "991.3975814282894 0.7869136929512024\n",
            "992.1844951212406 2.550936698913574\n",
            "994.7354318201542 1.5758262872695923\n",
            "996.3112581074238 2.092090129852295\n",
            "998.4033482372761 1.5377562046051025\n",
            "999.9411044418812 16.175552368164062\n",
            "1016.1166568100452 37.109832763671875\n",
            "1053.2264895737171 2.355745792388916\n",
            "1055.582235366106 0.7639748454093933\n",
            "1056.3462102115154 10.366203308105469\n",
            "1066.712413519621 1.8753793239593506\n",
            "1068.5877928435802 8.262334823608398\n",
            "1076.8501276671886 0.8872849941253662\n",
            "1077.737412661314 4.636014461517334\n",
            "1082.3734271228313 27.775676727294922\n",
            "1110.1491038501263 3.3999977111816406\n",
            "1113.549101561308 0.5514440536499023\n",
            "1114.1005456149578 0.7481161952018738\n",
            "1114.8486618101597 nan\n",
            "1114.8486618101597 9.393099784851074\n",
            "1124.2417615950108 5.191524505615234\n",
            "1129.433286100626 3.759441375732422\n",
            "1133.1927274763584 2.354968786239624\n",
            "1135.547696262598 10.580636978149414\n",
            "1146.1283332407475 2.600538730621338\n",
            "1148.7288719713688 4.9704742431640625\n",
            "1153.6993462145329 3.997681140899658\n",
            "1157.6970273554325 1.5043354034423828\n",
            "1159.201362758875 10.102256774902344\n",
            "1169.3036195337772 2.6379613876342773\n",
            "1171.9415809214115 2.9324350357055664\n",
            "1174.874015957117 6.97416353225708\n",
            "1181.8481794893742 2.0699522495269775\n",
            "1183.9181317389011 19.837974548339844\n",
            "1203.756106287241 15.812267303466797\n",
            "1219.5683735907078 0.760778546333313\n",
            "1220.329152137041 2.4524831771850586\n",
            "1222.7816353142262 9.249520301818848\n",
            "1232.031155616045 8.375170707702637\n",
            "1240.4063263237476 0.7828385829925537\n",
            "1241.1891649067402 4.424595355987549\n",
            "1245.6137602627277 1.289027214050293\n",
            "1246.902787476778 2.595022678375244\n",
            "1249.4978101551533 1.846811056137085\n",
            "1251.3446212112904 3.0165371894836426\n",
            "1254.361158400774 1.6079158782958984\n",
            "1255.96907427907 7.807277679443359\n",
            "1263.7763519585133 1.6308362483978271\n",
            "1265.407188206911 2.6556997299194336\n",
            "1268.0628879368305 nan\n",
            "1268.0628879368305 3.427755355834961\n",
            "1271.4906432926655 2.790827989578247\n",
            "1274.2814712822437 1.6543869972229004\n",
            "1275.9358582794666 1.604642629623413\n",
            "1277.54050090909 4.823575973510742\n",
            "1282.3640768826008 1.9288866519927979\n",
            "1284.2929635345936 63.33763122558594\n",
            "1347.6305947601795 7.871524810791016\n",
            "1355.5021195709705 16.789466857910156\n",
            "1372.2915864288807 0.7060476541519165\n",
            "1372.9976340830326 3.82255220413208\n",
            "1376.8201862871647 5.573415279388428\n",
            "1382.3936015665531 1.6831423044204712\n",
            "1384.0767438709736 4.158509254455566\n",
            "1388.2352531254292 3.48061466217041\n",
            "1391.7158677875996 25.806358337402344\n",
            "1417.522226125002 2.491530179977417\n",
            "1420.0137563049793 4.913915634155273\n",
            "1424.9276719391346 0.7755967974662781\n",
            "1425.7032687366009 2.0793333053588867\n",
            "1427.7826020419598 2.057110548019409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 2/2 [04:40<00:00, 140.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1429.8397125899792 2.3766725063323975\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2nsbro-Tse",
        "outputId": "158f255f-28bb-4324-d0a8-3a5ef1271116"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.377430892210402, 6.600075507356275]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "n_dOW6SREUn2",
        "outputId": "fafd01d2-a635-4549-e5ee-c0bb0530f0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd9e9d062e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4MFONZWvSY",
        "outputId": "b5c94246-e63d-4053-c50a-63b031a4513d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([], device='cuda:0', size=(0, 4)),\n",
              "  tensor([], device='cuda:0', size=(0, 4))],\n",
              " [tensor([], device='cuda:0'), tensor([], device='cuda:0')],\n",
              " [tensor([], device='cuda:0', dtype=torch.int64),\n",
              "  tensor([], device='cuda:0', dtype=torch.int64)])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JXCX9h1QEnwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "dc6ba8b0-c554-416a-b51a-1ad221e2688e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8ac16178e34c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_width\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mheight_scale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_height\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprop_proj_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprop_proj_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-954ae02bde6f>\u001b[0m in \u001b[0;36mproject_bboxes\u001b[0;34m(bboxes, width_scale_factor, height_scale_factor, mode)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mproj_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minvalid_bbox_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproj_bboxes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# indicating padded bboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, -1, 4] because the unspecified dimension size -1 can be any value and is ambiguous"
          ]
        }
      ],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAH99PhbTjI"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all.cpu(), fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0].cpu(), fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1].cpu(), fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivNrzM_PbViM"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "filters_data =[filters[0].cpu().detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}