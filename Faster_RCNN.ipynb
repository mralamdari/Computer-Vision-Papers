{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NZ2XqKRSf4Ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8f7b7872-8e01-46ed-e2d6-b2eb0fcd5503"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1i6Zvz3aj_L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19580bf-07b1-49a8-a205-8e17352802be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_ratios)*len(anc_scales)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - (w / 2)\n",
        "                    ymin = y_center - (h / 2)\n",
        "                    xmax = x_center + (w / 2)\n",
        "                    ymax = y_center + (h / 2)\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "IoU Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n",
        "\n",
        "activation map <==> pixel image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Positive / Negative Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01oUPTFTxub2"
      },
      "source": [
        "Proposal Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            if img.get_device() == 0:\n",
        "              img = img.cpu()\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    if bboxes.get_device() == 0:\n",
        "      bboxes = bboxes.cpu()\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    if type(x_points) == torch.Tensor and x_points.get_device() == 0:\n",
        "      x_points = x_points.cpu()\n",
        "    if type(y_points) == torch.Tensor and y_points.get_device() == 0:\n",
        "      y_points = y_points.cpu()\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        if type(special_point) == torch.Tensor and special_point.get_device() == 0:\n",
        "          special_point = special_point.cpu()\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QUIzOPx0jA"
      },
      "source": [
        "Proposal Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "Stage 1 of the detector (RPN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th_wGvoxyIPK"
      },
      "source": [
        "Regression Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 1 of the detector (Classification Module)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "Last Stage of the detector (Wrap Up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Paye0SiEAuJu",
        "outputId": "ee70c8db-d230-47b5-964b-9131a2165f7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(proposals_final) 2 torch.Size([643, 4])\n",
            "len(conf_scores_final) 2 torch.Size([643])\n",
            "len(feature_map) 2 torch.Size([2048, 20, 15])\n",
            "[2048, 20, 15]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([[-2.0668e+01, -3.4030e+01, -2.0613e+01, -3.3996e+01],\n",
              "          [-4.1951e+09, -9.8031e+04,  4.1951e+09,  9.8034e+04],\n",
              "          [-4.6926e+01,  8.6073e+00,  1.1034e+02,  8.6087e+00],\n",
              "          ...,\n",
              "          [ 4.7423e+01, -3.2506e+05,  4.7505e+01,  3.2511e+05],\n",
              "          [-2.0250e+00,  8.1543e+00, -2.0121e+00,  8.1543e+00],\n",
              "          [ 4.1813e+01, -1.1538e+04,  4.3409e+01,  1.1598e+04]], device='cuda:0'),\n",
              "  tensor([[ 1.7667e+01,  3.0368e+01,  1.7782e+01,  3.0976e+01],\n",
              "          [-7.2950e+02, -5.4807e+01,  7.2328e+02,  2.4694e+01],\n",
              "          [ 1.2647e-01,  7.8142e+00,  3.1565e-01,  1.1400e+01],\n",
              "          [-2.9028e+00,  1.5035e+01, -1.6415e+00,  1.5246e+01],\n",
              "          [-2.8460e+01,  8.9033e+00, -1.9654e+01,  9.0732e+00],\n",
              "          [-1.1415e+01,  3.8098e+01,  9.1787e+00,  4.2667e+01],\n",
              "          [ 3.7426e+00, -3.3385e+01,  3.7857e+00,  6.9326e+01],\n",
              "          [-3.0673e+01, -6.7821e+01, -3.0673e+01, -6.7821e+01],\n",
              "          [-4.4646e+13, -1.3104e+08,  4.4646e+13,  1.3104e+08],\n",
              "          [ 1.1632e+01, -7.7050e+00,  1.6900e+01, -7.6009e+00],\n",
              "          [ 1.1311e+01,  3.5595e+00,  1.1312e+01,  3.6628e+00],\n",
              "          [ 1.4796e+01, -6.3720e-01,  1.5060e+01,  3.1169e+00],\n",
              "          [ 1.1769e+01,  3.3335e+00,  1.2117e+01,  4.5897e+00],\n",
              "          [ 1.2750e+01, -9.9974e+00,  1.2798e+01,  2.9004e+01],\n",
              "          [-6.0894e+00,  4.1332e+00, -6.0445e+00,  5.8068e+00],\n",
              "          [ 9.1152e+00, -6.4323e+00,  9.8669e+00,  1.6803e+01],\n",
              "          [ 2.9149e+01,  2.1211e+01,  2.9149e+01,  2.1211e+01],\n",
              "          [ 2.9241e+01, -5.2837e+02,  3.5728e+01,  5.3381e+02],\n",
              "          [-2.4802e+01, -4.2681e+00,  5.5428e+00, -4.1429e+00],\n",
              "          [ 2.9961e+00,  6.9818e+00,  3.3834e+00,  1.7929e+01],\n",
              "          [-1.5808e+03, -1.1249e+03,  1.5913e+03,  1.1875e+03],\n",
              "          [-5.4888e+00,  1.5064e+01,  6.0927e+00,  1.7737e+01],\n",
              "          [-5.1762e+02,  8.9261e+00,  5.2977e+02,  9.6018e+00],\n",
              "          [ 5.0466e-01,  1.3981e+01,  3.2348e+00,  1.3984e+01],\n",
              "          [ 1.1833e+01,  1.0295e+01,  1.2263e+01,  1.0295e+01],\n",
              "          [ 1.2955e+01, -2.2226e+01,  1.8564e+01,  6.9850e+01],\n",
              "          [ 3.8675e+00,  4.3515e+00,  8.9589e+00,  4.3894e+00],\n",
              "          [ 4.3802e+00,  6.7682e+00,  1.8588e+01,  7.2488e+00],\n",
              "          [ 2.0854e+01, -9.0795e+03,  2.1179e+01,  9.1397e+03],\n",
              "          [-1.0400e+02, -6.3584e+01, -1.0393e+02, -6.3561e+01],\n",
              "          [ 2.9688e-01, -1.0808e+02,  2.0235e+00,  1.2545e+02],\n",
              "          [ 9.1811e+00,  2.2531e+01,  1.3288e+01,  2.6024e+01],\n",
              "          [-2.5732e+04, -2.2099e+01,  2.5838e+04,  1.0205e+01],\n",
              "          [ 8.2432e+00,  6.4182e+00,  9.1248e+00,  1.2583e+01],\n",
              "          [ 3.1628e+01,  2.5201e+01,  3.5751e+01,  2.8543e+01],\n",
              "          [ 1.1382e+01,  1.0046e+01,  1.2444e+01,  1.0235e+01],\n",
              "          [-2.9848e+01, -1.9403e+01, -2.9847e+01, -1.9392e+01],\n",
              "          [-2.2371e+02,  1.7471e+01,  2.1081e+02,  2.2182e+01],\n",
              "          [-7.0861e+07, -8.3058e+05,  7.0861e+07,  8.3065e+05],\n",
              "          [ 1.3601e+01,  1.5716e+01,  1.4137e+01,  1.5936e+01],\n",
              "          [-1.4506e+00,  5.3128e+00,  6.8015e+00,  6.3786e+00],\n",
              "          [ 6.3091e+01,  3.4393e+00,  6.8016e+01,  7.3544e+00],\n",
              "          [-7.5213e+04, -7.4523e+06,  7.5195e+04,  7.4523e+06],\n",
              "          [ 2.1360e+01,  9.5335e+00,  2.1723e+01,  9.6820e+00],\n",
              "          [ 1.5825e+00,  6.7618e+00,  1.7733e+00,  7.0780e+00],\n",
              "          [-3.9806e+00,  1.3576e+01, -2.5170e+00,  1.6009e+01],\n",
              "          [-6.2947e+00, -2.5511e+04, -1.3290e+00,  2.5562e+04],\n",
              "          [-4.8984e+08, -2.5386e+03,  4.8984e+08,  2.6446e+03],\n",
              "          [-4.1642e+01, -3.2786e+01, -4.1641e+01, -2.9801e+01],\n",
              "          [ 1.1396e+01,  1.2346e+01,  1.2130e+01,  1.3147e+01],\n",
              "          [ 1.3474e+01,  1.3019e+01,  1.4073e+01,  1.4534e+01],\n",
              "          [-1.2026e+00, -4.8078e+03,  1.0589e+01,  4.8455e+03],\n",
              "          [ 1.4504e+00,  8.4582e+00,  1.4574e+00,  8.4857e+00],\n",
              "          [-2.9266e+02, -7.9362e+00,  3.3112e+02, -7.7626e+00],\n",
              "          [-5.3734e+00, -2.4660e+00, -4.8798e+00,  8.2710e+00],\n",
              "          [ 1.6106e+01,  1.6206e+00,  1.6278e+01,  7.9318e+00],\n",
              "          [ 1.5804e+01,  1.1431e+01,  1.7885e+01,  2.1993e+01],\n",
              "          [ 3.8319e+00, -6.3704e+00,  4.4625e+00,  2.1464e+01],\n",
              "          [ 3.0544e+00,  1.9627e+01,  3.0642e+00,  2.6837e+01],\n",
              "          [ 1.5234e+01,  7.2946e+00,  1.5380e+01,  7.6490e+00],\n",
              "          [-1.2963e+01, -6.1972e+00,  1.5015e+01,  3.8826e+00],\n",
              "          [ 1.7595e+01,  8.1478e+00,  2.1517e+01,  8.5370e+00],\n",
              "          [ 5.7262e-01, -1.3475e+01,  2.0233e+00, -1.3101e+01],\n",
              "          [-8.5480e+00, -7.9741e+00, -8.1894e+00, -7.5344e+00],\n",
              "          [ 7.8845e+00,  2.1747e+00,  8.4747e+00,  8.0599e+00],\n",
              "          [ 4.4426e+00, -6.5983e+01,  9.6319e+00, -9.4349e+00],\n",
              "          [ 1.0297e+01, -2.0236e+04,  2.0065e+01,  2.0227e+04],\n",
              "          [-1.1212e+01, -1.2291e+04, -1.1212e+01,  1.2346e+04],\n",
              "          [ 5.6474e+00,  3.2735e+01,  6.3191e+00,  3.2989e+01],\n",
              "          [-5.9081e+05, -1.2892e+05,  5.9104e+05,  1.2908e+05],\n",
              "          [-4.4551e+03,  8.5678e+00,  4.4786e+03,  8.6759e+00],\n",
              "          [ 2.2897e+01,  2.0919e+01,  2.3458e+01,  2.2282e+01],\n",
              "          [ 3.0192e+00,  1.2879e+01,  3.2515e+00,  1.3326e+01],\n",
              "          [ 9.5184e+01,  3.3590e+01,  9.7838e+01,  5.7398e+01],\n",
              "          [ 3.1566e+01, -1.0958e+01,  3.7946e+01,  3.3064e+01],\n",
              "          [ 1.5099e+01,  1.1674e+01,  1.6002e+01,  1.3451e+01],\n",
              "          [-9.2339e-01,  2.4427e+00, -6.6391e-01,  2.6058e+00],\n",
              "          [-8.8662e+00,  6.3563e+00, -8.4167e+00,  6.3719e+00],\n",
              "          [ 1.1888e+01,  5.2528e+00,  1.2482e+01,  5.3372e+00],\n",
              "          [ 2.9834e+00, -2.4611e+01,  2.9892e+00, -2.4606e+01],\n",
              "          [-4.0051e+00, -4.3501e+05,  7.4283e+01,  4.3507e+05],\n",
              "          [ 6.1211e+00, -7.5305e+01,  1.2635e+01, -6.2244e+01],\n",
              "          [-2.4075e+00,  1.2803e+01, -1.8738e+00,  1.3564e+01],\n",
              "          [-3.2435e+01, -1.7651e+01, -3.2022e+01, -1.7646e+01],\n",
              "          [-8.7477e+02,  2.5008e+01,  9.1730e+02,  2.8357e+01],\n",
              "          [ 4.6480e+00, -9.1007e+03,  4.6529e+00,  9.1500e+03],\n",
              "          [ 1.6325e+01, -4.1144e+02,  1.6514e+01,  4.2381e+02],\n",
              "          [-6.5883e+01, -3.4739e+01,  8.6496e+01, -3.4206e+01],\n",
              "          [-3.6826e+01, -4.0435e+01, -3.6313e+01, -3.2418e+01],\n",
              "          [ 1.1465e+01,  1.7515e+01,  1.3738e+01,  2.1789e+01],\n",
              "          [ 3.9121e+00,  3.1228e+00,  2.2325e+01,  3.5548e+00],\n",
              "          [ 9.8252e+00,  2.0451e+01,  3.8704e+01,  2.0964e+01],\n",
              "          [ 4.3140e+00, -9.6546e+00,  4.3141e+00, -9.6064e+00],\n",
              "          [ 7.4078e+00,  1.6642e+01,  7.6381e+00,  1.7103e+01],\n",
              "          [ 1.5635e+01,  2.3466e+01,  1.6877e+01,  2.4082e+01],\n",
              "          [ 1.3338e+01,  1.0100e+01,  1.3367e+01,  1.0591e+01],\n",
              "          [-1.0896e+01,  3.5899e+00,  3.9553e-01,  3.5987e+00],\n",
              "          [-6.0038e+01, -4.8662e+01, -5.9676e+01, -4.8662e+01],\n",
              "          [-6.7372e+01,  1.4598e+01,  6.7681e+01,  1.5043e+01],\n",
              "          [ 5.8043e+00,  9.7045e+00,  6.1056e+00,  1.4151e+01],\n",
              "          [-7.8377e+00, -1.2974e+01, -6.6006e+00, -4.9677e+00],\n",
              "          [-2.9547e+01,  2.5651e+01,  1.1091e+01,  3.4275e+01],\n",
              "          [-4.1009e+00, -1.7731e+11, -4.0953e+00,  1.7731e+11],\n",
              "          [ 2.2094e+00, -1.6050e+00,  2.2137e+00, -1.5941e+00],\n",
              "          [ 1.6906e+00, -6.4793e+00,  2.1003e+00,  1.9046e+00],\n",
              "          [ 4.4783e+00, -4.5246e+00,  5.5643e+00,  2.3631e+01],\n",
              "          [-9.6763e+00, -1.2578e+01, -9.6651e+00,  9.5823e+00],\n",
              "          [ 1.2386e+01,  1.4878e+01,  1.2402e+01,  1.5173e+01],\n",
              "          [ 2.1069e+01, -4.7804e+00,  2.9738e+01, -4.6506e+00],\n",
              "          [-6.4301e+00,  2.4406e+01, -6.3230e+00,  2.4406e+01],\n",
              "          [ 1.3237e+01,  1.7819e+00,  1.4723e+01,  3.2485e+00],\n",
              "          [ 9.0829e+00, -4.7074e+00,  9.0837e+00, -4.2747e+00],\n",
              "          [-8.8765e+01, -3.9072e+01, -8.8765e+01, -3.9072e+01],\n",
              "          [-7.6206e+05, -9.8365e+02,  7.6211e+05,  1.0586e+03],\n",
              "          [-5.3833e+00, -2.9255e+03, -3.2611e+00,  3.0209e+03],\n",
              "          [ 1.3924e+01, -3.1277e+00,  1.3926e+01, -2.3031e+00],\n",
              "          [-6.1520e-01,  4.7703e+00, -5.8669e-01,  5.4966e+00],\n",
              "          [-4.6445e+01, -9.7914e+00, -4.6436e+01, -9.7860e+00],\n",
              "          [ 4.5125e+00, -1.2386e+00,  4.7920e+00, -1.1722e+00],\n",
              "          [-8.0745e+01,  2.2197e+01,  7.2124e+01,  2.2199e+01],\n",
              "          [-2.5162e+02,  3.2550e+01,  3.1327e+02,  3.2635e+01],\n",
              "          [ 4.1523e+01,  2.7465e+01,  4.3016e+01,  4.0246e+01],\n",
              "          [ 3.2229e+00,  2.3003e-01,  5.2593e+00,  1.1620e+00],\n",
              "          [ 1.4596e+01, -8.1767e+00,  1.8097e+01, -5.8439e+00],\n",
              "          [ 8.1248e+00,  2.9296e+01,  1.0059e+01,  3.4948e+01],\n",
              "          [-5.3699e+04,  2.9386e+00,  5.3740e+04,  2.9401e+00],\n",
              "          [ 1.1337e+01,  2.2957e+01,  1.1548e+01,  2.3885e+01],\n",
              "          [-6.0529e+00, -1.5478e+01, -5.7139e+00, -8.6880e+00],\n",
              "          [ 3.3241e+00, -7.5671e+00,  3.7698e+00, -5.9900e+00],\n",
              "          [ 7.6305e+00,  1.4218e+01,  7.8003e+00,  1.4296e+01],\n",
              "          [-1.3104e+01, -1.2450e+01, -1.0644e+01, -5.5157e+00],\n",
              "          [ 6.4853e+00, -2.1120e+01,  6.6359e+00,  1.3809e+01],\n",
              "          [ 1.3340e+01, -1.1432e+02,  1.3589e+01, -2.2184e-01]], device='cuda:0')],\n",
              " [tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996, 0.9996,\n",
              "          0.9996, 0.9995, 0.9995, 0.9995, 0.9995, 0.9995, 0.9994, 0.9994, 0.9994,\n",
              "          0.9994, 0.9994, 0.9994, 0.9994, 0.9993, 0.9993, 0.9993, 0.9993, 0.9993,\n",
              "          0.9992, 0.9992, 0.9992, 0.9992, 0.9992, 0.9991, 0.9991, 0.9991, 0.9991,\n",
              "          0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9991, 0.9990, 0.9990, 0.9989,\n",
              "          0.9989, 0.9989, 0.9989, 0.9988, 0.9987, 0.9987, 0.9987, 0.9986, 0.9986,\n",
              "          0.9986, 0.9985, 0.9985, 0.9985, 0.9984, 0.9984, 0.9984, 0.9984, 0.9983,\n",
              "          0.9983, 0.9983, 0.9983, 0.9983, 0.9982, 0.9981, 0.9981, 0.9981, 0.9981,\n",
              "          0.9980, 0.9979, 0.9978, 0.9977, 0.9977, 0.9975, 0.9975, 0.9974, 0.9973,\n",
              "          0.9973, 0.9973, 0.9972, 0.9971, 0.9970, 0.9970, 0.9969, 0.9969, 0.9969,\n",
              "          0.9969, 0.9968, 0.9968, 0.9968, 0.9967, 0.9967, 0.9966, 0.9965, 0.9964,\n",
              "          0.9961, 0.9960, 0.9959, 0.9958, 0.9958, 0.9957, 0.9954, 0.9953, 0.9952,\n",
              "          0.9950, 0.9950, 0.9950, 0.9949, 0.9949, 0.9948, 0.9948, 0.9947, 0.9946,\n",
              "          0.9942, 0.9941, 0.9940, 0.9937, 0.9934, 0.9933, 0.9933, 0.9932, 0.9932,\n",
              "          0.9931, 0.9928, 0.9927, 0.9923, 0.9922, 0.9921, 0.9920, 0.9918, 0.9918,\n",
              "          0.9916, 0.9914, 0.9914, 0.9912, 0.9911, 0.9910, 0.9907, 0.9905, 0.9904,\n",
              "          0.9904, 0.9903, 0.9902, 0.9901], device='cuda:0'),\n",
              "  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "          1.0000, 1.0000, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n",
              "          0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9998, 0.9998,\n",
              "          0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9997, 0.9997,\n",
              "          0.9997, 0.9997, 0.9997, 0.9997, 0.9996, 0.9996, 0.9996, 0.9994, 0.9993,\n",
              "          0.9992, 0.9991, 0.9991, 0.9990, 0.9990, 0.9989, 0.9989, 0.9989, 0.9987,\n",
              "          0.9987, 0.9985, 0.9984, 0.9984, 0.9984, 0.9983, 0.9983, 0.9981, 0.9981,\n",
              "          0.9980, 0.9980, 0.9980, 0.9979, 0.9978, 0.9978, 0.9978, 0.9976, 0.9975,\n",
              "          0.9975, 0.9975, 0.9971, 0.9970, 0.9970, 0.9967, 0.9967, 0.9966, 0.9965,\n",
              "          0.9964, 0.9964, 0.9960, 0.9948, 0.9947, 0.9946, 0.9944, 0.9941, 0.9939,\n",
              "          0.9935, 0.9933, 0.9932, 0.9927, 0.9927, 0.9925, 0.9923, 0.9920, 0.9911,\n",
              "          0.9910, 0.9910, 0.9909, 0.9909, 0.9907, 0.9906, 0.9903],\n",
              "         device='cuda:0')],\n",
              " [tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         device='cuda:0'),\n",
              "  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "        \n",
        "        # return proposals_final.to(device), conf_scores_final.to(device), feature_map.to(device), [out_c, out_h, out_w]\n",
        "        # return torch.Tensor(proposals_final), torch.Tensor(conf_scores_final), torch.Tensor(feature_map), [out_c, out_h, out_w]\n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n",
        "\n",
        "\n",
        "\n",
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        print('len(proposals_final)', len(proposals_final), proposals_final[0].shape)\n",
        "        print('len(conf_scores_final)', len(conf_scores_final), conf_scores_final[0].shape)\n",
        "        print('len(feature_map)', len(feature_map), feature_map[0].shape)\n",
        "        print(out_size)\n",
        "        # print(conf_scores_final)\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        # print(classes_all)\n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final\n",
        "\n",
        "d = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "\n",
        "d.eval()\n",
        "proposals_final, conf_scores_final, classes_final = d.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)\n",
        "# proposals_final.shape, conf_scores_final.shape, classes_final.shape\n",
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "3370b7e8-0656-415b-de80-0489964f6b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 74.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        counter = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            print(total_loss, loss.item())\n",
        "            if not np.isnan(loss.item()):\n",
        "              total_loss += loss.item()\n",
        "\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # break \n",
        "            counter += 1\n",
        "        \n",
        "        loss_list.append(total_loss/counter)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFnKE3neESZ5"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-1\n",
        "n_epochs = 2\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If2nsbro-Tse",
        "outputId": "158f255f-28bb-4324-d0a8-3a5ef1271116"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7.377430892210402, 6.600075507356275]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "n_dOW6SREUn2",
        "outputId": "fafd01d2-a635-4549-e5ee-c0bb0530f0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fd9e9d062e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(3, 3))\n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxhyFB4szYQV"
      },
      "source": [
        "Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwJsnhFKzZ3F"
      },
      "source": [
        "Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4MFONZWvSY",
        "outputId": "a3c60d6c-a043-443e-a963-15506d67c158"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([], device='cuda:0', size=(0, 4)),\n",
              "  tensor([], device='cuda:0', size=(0, 4))],\n",
              " [tensor([], device='cuda:0'), tensor([], device='cuda:0')],\n",
              " [tensor([], device='cuda:0', dtype=torch.int64),\n",
              "  tensor([], device='cuda:0', dtype=torch.int64)])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JXCX9h1QEnwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "dc6ba8b0-c554-416a-b51a-1ad221e2688e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-8ac16178e34c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_width\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mheight_scale_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_height\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprop_proj_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprop_proj_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_scale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a2p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-954ae02bde6f>\u001b[0m in \u001b[0;36mproject_bboxes\u001b[0;34m(bboxes, width_scale_factor, height_scale_factor, mode)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mproj_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minvalid_bbox_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mproj_bboxes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# indicating padded bboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, -1, 4] because the unspecified dimension size -1 can be any value and is ambiguous"
          ]
        }
      ],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAH99PhbTjI"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all.cpu(), fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0].cpu(), fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1].cpu(), fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivNrzM_PbViM"
      },
      "outputs": [],
      "source": [
        "feature_extractor = FeatureExtractor() \n",
        "out = feature_extractor(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "filters_data =[filters[0].cpu().detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(6, 3))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}