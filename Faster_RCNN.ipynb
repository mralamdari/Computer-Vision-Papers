{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "b47e7255-cc39-4acd-db1f-e82b4aaf2a52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.12.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JqQt700WVFSN"
      },
      "outputs": [],
      "source": [
        "def parse_annotation(data_dir, img_size):\n",
        "\n",
        "  img_paths  = []\n",
        "  gdt_bboxes = []\n",
        "  gdt_classes= []\n",
        "  img_w, img_h = img_size\n",
        "\n",
        "  for img_name in os.listdir(data_dir+'images'):\n",
        "\n",
        "    img_path = os.path.join(data_dir, 'images', img_name)\n",
        "    annotation_path = os.path.join(data_dir, 'annotations', img_name[:-3]+'xml')\n",
        "\n",
        "    with open(annotation_path, 'r') as f:\n",
        "      tree = ET.parse(f)\n",
        "\n",
        "    root = tree.getroot()\n",
        "    \n",
        "    img_paths.append(img_path)\n",
        "    ann_size = root.find('size')\n",
        "    orig_w = int(ann_size.find('width').text)\n",
        "    orig_h = int(ann_size.find('height').text)\n",
        "    ground_truth_bboxes = []\n",
        "    ground_truth_classes = []\n",
        "    \n",
        "    for box in root.findall('object'):\n",
        "      box_root = box.find('bndbox')\n",
        "      xmin = float(box_root.find('xmin').text) * img_w / orig_w\n",
        "      ymin = float(box_root.find('ymin').text) * img_h / orig_h\n",
        "      xmax = float(box_root.find('xmax').text) * img_w / orig_w\n",
        "      ymax = float(box_root.find('ymax').text) * img_h / orig_h\n",
        "      bbox = torch.Tensor([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
        "\n",
        "      ground_truth_bboxes.append(bbox.tolist())\n",
        "      ground_truth_classes.append(int(root.find('segmented').text))\n",
        "\n",
        "    gdt_bboxes.append(torch.Tensor(ground_truth_bboxes))\n",
        "    gdt_classes.append(torch.Tensor(ground_truth_classes))\n",
        "    \n",
        "  return gdt_bboxes, gdt_classes, img_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZwF0UHuz_iHY"
      },
      "outputs": [],
      "source": [
        "class ObjectDetectionDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    A Pytorch Dataset class to load the images and their corresponding annotations.\n",
        "    \n",
        "    Returns\n",
        "    ------------\n",
        "    images: torch.Tensor of size (B, C, H, W)\n",
        "    gt bboxes: torch.Tensor of size (B, max_objects, 4)\n",
        "    gt classes: torch.Tensor of size (B, max_objects)\n",
        "    '''\n",
        "    def __init__(self, data_dir, img_size, device='cpu'):\n",
        "        self.data_dir = data_dir\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.img_data_all, self.gdt_bboxes, self.gdt_classes = self.get_data()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_data_all.size(dim=0)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.img_data_all[idx], self.gdt_bboxes[idx], self.gdt_classes[idx]\n",
        "        \n",
        "    def get_data(self):\n",
        "        img_data = []\n",
        "        gdt_idxs = []\n",
        "\n",
        "        gdt_boxes, gdt_classes, img_paths = parse_annotation(self.data_dir, self.img_size)\n",
        "\n",
        "        for i, img_path in enumerate(img_paths):\n",
        "            # skip if the image path is not valid\n",
        "            if (not img_path) or (not os.path.exists(img_path)):\n",
        "                continue\n",
        "                \n",
        "            # read and resize image\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.resize(img, self.img_size)\n",
        "            # convert image to torch tensor and reshape it so channels come first\n",
        "            img_tensor = torch.from_numpy(img).permute(2, 0, 1)\n",
        "            \n",
        "            # encode class names as integers\n",
        "            gdt_idx = gdt_classes[i]\n",
        "            \n",
        "            img_data.append(img_tensor)\n",
        "            gdt_idxs.append(gdt_idx)\n",
        "        # pad bounding boxes and classes so they are of the same size\n",
        "        gt_bboxes_pad = torch.nn.utils.rnn.pad_sequence(gdt_boxes, batch_first=True, padding_value=-1)\n",
        "        gt_classes_pad = torch.nn.utils.rnn.pad_sequence(gdt_idxs, batch_first=True, padding_value=-1)\n",
        "        \n",
        "        # stack all images\n",
        "        img_data_stacked = torch.stack(img_data, dim=0)\n",
        "        img_data_stacked = img_data_stacked.to(dtype=torch.float32)\n",
        "        return img_data_stacked.to(device), gt_bboxes_pad.to(device), gt_classes_pad.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVyQczF6VjNJ"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUqUKpmpc-dv"
      },
      "source": [
        "Generate Anchor Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lvSumg_yVKDp"
      },
      "outputs": [],
      "source": [
        "def gen_anc_centers(out_size):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    out_h, out_w = out_size\n",
        "    anc_pts_x = torch.arange(0, out_w, device=device) + 0.5\n",
        "    anc_pts_y = torch.arange(0, out_h, device=device) + 0.5\n",
        "    \n",
        "    return anc_pts_x, anc_pts_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_i0BAhJdCol"
      },
      "source": [
        "Generate Anchor Boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "frOp2gl2VQkz"
      },
      "outputs": [],
      "source": [
        "def gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, out_size):\n",
        "    n_anc_boxes = len(anc_scales) * len(anc_ratios)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    anc_base = torch.zeros(1, anc_pts_x.size(dim=0) , anc_pts_y.size(dim=0), n_anc_boxes, 4, device=device) # shape - [1, Hmap, Wmap, n_anchor_boxes, 4]\n",
        "    \n",
        "    for ix, x_center in enumerate(anc_pts_x):\n",
        "        for jx, y_center in enumerate(anc_pts_y):\n",
        "            anc_boxes = torch.zeros((n_anc_boxes, 4))\n",
        "            c = 0\n",
        "            for i, scale in enumerate(anc_scales):\n",
        "                for j, ratio in enumerate(anc_ratios):\n",
        "                    w = scale * ratio\n",
        "                    h = scale\n",
        "                    \n",
        "                    xmin = x_center - w / 2\n",
        "                    ymin = y_center - h / 2\n",
        "                    xmax = x_center + w / 2\n",
        "                    ymax = y_center + h / 2\n",
        "\n",
        "                    anc_boxes[c, :] = torch.Tensor([xmin, ymin, xmax, ymax])\n",
        "                    c += 1\n",
        "\n",
        "            anc_base[:, ix, jx, :] = torchvision.ops.clip_boxes_to_image(anc_boxes, size=out_size)\n",
        "            \n",
        "    return anc_base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODdCaZo3I7u"
      },
      "source": [
        "Positive and Negative Anchor Boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fgb9zAl9VTq0"
      },
      "outputs": [],
      "source": [
        "def get_iou_mat(batch_size, anc_boxes_all, gdt_bboxes_all):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    \n",
        "    # flatten anchor boxes\n",
        "    anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "    # create a placeholder to compute IoUs amongst the boxes\n",
        "    ious_mat = torch.zeros((batch_size, anc_boxes_flat.size(dim=1), gdt_bboxes_all.size(dim=1)), device=device)\n",
        "\n",
        "    # compute IoU of the anc boxes with the gt boxes for all the images\n",
        "    for i in range(batch_size):\n",
        "        gt_bboxes = gdt_bboxes_all[i]\n",
        "        anc_boxes = anc_boxes_flat[i]\n",
        "        ious_mat[i, :] = torchvision.ops.box_iou(anc_boxes, gt_bboxes)\n",
        "        \n",
        "    return ious_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XWNccM5NO2W"
      },
      "source": [
        "Projecting Boxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MI7rVZ-JVMNb"
      },
      "outputs": [],
      "source": [
        "def project_bboxes(bboxes, width_scale_factor, height_scale_factor, mode='a2p'):\n",
        "    assert mode in ['a2p', 'p2a']\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    batch_size = bboxes.size(dim=0)\n",
        "    proj_bboxes = bboxes.clone().reshape(batch_size, -1, 4)\n",
        "    invalid_bbox_mask = (proj_bboxes == -1) # indicating padded bboxes\n",
        "    \n",
        "    if mode == 'a2p':\n",
        "        # activation map to pixel image\n",
        "        proj_bboxes[:, :, [0, 2]] *= width_scale_factor  #xmin, xmax\n",
        "        proj_bboxes[:, :, [1, 3]] *= height_scale_factor #ymin, ymax\n",
        "    else:\n",
        "        # pixel image to activation map\n",
        "        proj_bboxes[:, :, [0, 2]] /= width_scale_factor\n",
        "        proj_bboxes[:, :, [1, 3]] /= height_scale_factor\n",
        "        \n",
        "    proj_bboxes.masked_fill_(invalid_bbox_mask, -1) # fill padded bboxes back with -1\n",
        "    proj_bboxes.resize_as_(bboxes)\n",
        "    \n",
        "    return proj_bboxes.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnvPOK7KO0cA"
      },
      "source": [
        "Computing Offsets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-tTfTVBxVIlw"
      },
      "outputs": [],
      "source": [
        "def calc_gt_offsets(pos_anc_coords, gtd_bbox_mapping):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    pos_anc_coords = torchvision.ops.box_convert(pos_anc_coords, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "    gtd_bbox_mapping = torchvision.ops.box_convert(gtd_bbox_mapping, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    gt_cx, gt_cy, gt_w, gt_h = gtd_bbox_mapping[:, 0], gtd_bbox_mapping[:, 1], gtd_bbox_mapping[:, 2], gtd_bbox_mapping[:, 3]\n",
        "    anc_cx, anc_cy, anc_w, anc_h = pos_anc_coords[:, 0], pos_anc_coords[:, 1], pos_anc_coords[:, 2], pos_anc_coords[:, 3]\n",
        "\n",
        "    tx_ = (gt_cx - anc_cx)/anc_w\n",
        "    ty_ = (gt_cy - anc_cy)/anc_h\n",
        "    tw_ = torch.log(gt_w / anc_w)\n",
        "    th_ = torch.log(gt_h / anc_h)\n",
        "\n",
        "    return torch.stack([tx_, ty_, tw_, th_], dim=-1).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB3wJC9P4Y7x"
      },
      "source": [
        "Create Pos/Neg Anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RX-p3OcGtvqR"
      },
      "outputs": [],
      "source": [
        "def get_req_anchors(anc_boxes_all, gt_bboxes_all, gt_classes_all, pos_thresh=0.7, neg_thresh=0.2):\n",
        "    '''\n",
        "    Prepare necessary data required for training\n",
        "    \n",
        "    Input\n",
        "    ------\n",
        "    anc_boxes_all - torch.Tensor of shape (B, w_amap, h_amap, n_anchor_boxes, 4)\n",
        "        all anchor boxes for a batch of images\n",
        "    gt_bboxes_all - torch.Tensor of shape (B, max_objects, 4)\n",
        "        padded ground truth boxes for a batch of images\n",
        "    gt_classes_all - torch.Tensor of shape (B, max_objects)\n",
        "        padded ground truth classes for a batch of images\n",
        "        \n",
        "    Returns\n",
        "    ---------\n",
        "    positive_anc_ind -  torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    negative_anc_ind - torch.Tensor of shape (n_pos,)\n",
        "        flattened positive indices for all the images in the batch\n",
        "    GT_conf_scores - torch.Tensor of shape (n_pos,), IoU scores of +ve anchors\n",
        "    GT_offsets -  torch.Tensor of shape (n_pos, 4),\n",
        "        offsets between +ve anchors and their corresponding ground truth boxes\n",
        "    GT_class_pos - torch.Tensor of shape (n_pos,)\n",
        "        mapped classes of +ve anchors\n",
        "    positive_anc_coords - (n_pos, 4) coords of +ve anchors (for visualization)\n",
        "    negative_anc_coords - (n_pos, 4) coords of -ve anchors (for visualization)\n",
        "    positive_anc_ind_sep - list of indices to keep track of +ve anchors\n",
        "    '''\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # get the size and shape parameters\n",
        "    B, w_amap, h_amap, A, _ = anc_boxes_all.shape\n",
        "    N = gt_bboxes_all.shape[1] # max number of groundtruth bboxes in a batch\n",
        "    \n",
        "    # get total number of anchor boxes in a single image\n",
        "    tot_anc_boxes = A * w_amap * h_amap\n",
        "    \n",
        "    # get the iou matrix which contains iou of every anchor box\n",
        "    # against all the groundtruth bboxes in an image\n",
        "    iou_mat = get_iou_mat(B, anc_boxes_all, gt_bboxes_all)\n",
        "    \n",
        "    # for every groundtruth bbox in an image, find the iou \n",
        "    # with the anchor box which it overlaps the most\n",
        "    max_iou_per_gt_box, _ = iou_mat.max(dim=1, keepdim=True)\n",
        "    \n",
        "    # get positive anchor boxes\n",
        "    \n",
        "    # condition 1: the anchor box with the max iou for every gt bbox\n",
        "    positive_anc_mask = torch.logical_and(iou_mat == max_iou_per_gt_box, max_iou_per_gt_box > 0) \n",
        "    # condition 2: anchor boxes with iou above a threshold with any of the gt bboxes\n",
        "    positive_anc_mask = torch.logical_or(positive_anc_mask, iou_mat > pos_thresh)\n",
        "    \n",
        "    positive_anc_ind_sep = torch.where(positive_anc_mask)[0] # get separate indices in the batch\n",
        "    # combine all the batches and get the idxs of the +ve anchor boxes\n",
        "    positive_anc_mask = positive_anc_mask.flatten(start_dim=0, end_dim=1)\n",
        "    positive_anc_ind = torch.where(positive_anc_mask)[0]\n",
        "    \n",
        "    # for every anchor box, get the iou and the idx of the\n",
        "    # gt bbox it overlaps with the most\n",
        "    max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "    max_iou_per_anc = max_iou_per_anc.flatten(start_dim=0, end_dim=1)\n",
        "    \n",
        "    # get iou scores of the +ve anchor boxes\n",
        "    GT_conf_scores = max_iou_per_anc[positive_anc_ind]\n",
        "    \n",
        "    # get gt classes of the +ve anchor boxes\n",
        "    \n",
        "    # expand gt classes to map against every anchor box\n",
        "    gt_classes_expand = gt_classes_all.view(B, 1, N).expand(B, tot_anc_boxes, N)\n",
        "    # for every anchor box, consider only the class of the gt bbox it overlaps with the most\n",
        "    GT_class = torch.gather(gt_classes_expand, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1)\n",
        "    # combine all the batches and get the mapped classes of the +ve anchor boxes\n",
        "    GT_class = GT_class.flatten(start_dim=0, end_dim=1)\n",
        "    GT_class_pos = GT_class[positive_anc_ind]\n",
        "    \n",
        "    # get gt bbox coordinates of the +ve anchor boxes\n",
        "    \n",
        "    # expand all the gt bboxes to map against every anchor box\n",
        "    gt_bboxes_expand = gt_bboxes_all.view(B, 1, N, 4).expand(B, tot_anc_boxes, N, 4)\n",
        "    # for every anchor box, consider only the coordinates of the gt bbox it overlaps with the most\n",
        "    GT_bboxes = torch.gather(gt_bboxes_expand, -2, max_iou_per_anc_ind.reshape(B, tot_anc_boxes, 1, 1).repeat(1, 1, 1, 4))\n",
        "    # combine all the batches and get the mapped gt bbox coordinates of the +ve anchor boxes\n",
        "    GT_bboxes = GT_bboxes.flatten(start_dim=0, end_dim=2)\n",
        "    GT_bboxes_pos = GT_bboxes[positive_anc_ind]\n",
        "    \n",
        "    # get coordinates of +ve anc boxes\n",
        "    anc_boxes_flat = anc_boxes_all.flatten(start_dim=0, end_dim=-2) # flatten all the anchor boxes\n",
        "    positive_anc_coords = anc_boxes_flat[positive_anc_ind]\n",
        "    \n",
        "    # calculate gt offsets\n",
        "    GT_offsets = calc_gt_offsets(positive_anc_coords, GT_bboxes_pos)\n",
        "    \n",
        "    # get -ve anchors\n",
        "    \n",
        "    # condition: select the anchor boxes with max iou less than the threshold\n",
        "    negative_anc_mask = (max_iou_per_anc < neg_thresh)\n",
        "    negative_anc_ind = torch.where(negative_anc_mask)[0]\n",
        "    # sample -ve samples to match the +ve samples\n",
        "    negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (positive_anc_ind.shape[0],))]\n",
        "    negative_anc_coords = anc_boxes_flat[negative_anc_ind]\n",
        "    \n",
        "    return positive_anc_ind.to(device), negative_anc_ind.to(device), GT_conf_scores.to(device), GT_offsets.to(device), GT_class_pos.to(device), \\\n",
        "         positive_anc_coords.to(device), negative_anc_coords.to(device), positive_anc_ind_sep.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RSW3LBz1VODx"
      },
      "outputs": [],
      "source": [
        "def generate_proposals(anchors, offsets):\n",
        "   \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    # change format of the anchor boxes from 'xyxy' to 'cxcywh'\n",
        "    anchors = torchvision.ops.box_convert(anchors, in_fmt='xyxy', out_fmt='cxcywh')\n",
        "\n",
        "    # apply offsets to anchors to create proposals\n",
        "    proposals_ = torch.zeros_like(anchors)\n",
        "    proposals_[:,0] = anchors[:,0] + offsets[:,0]*anchors[:,2]\n",
        "    proposals_[:,1] = anchors[:,1] + offsets[:,1]*anchors[:,3]\n",
        "    proposals_[:,2] = anchors[:,2] * torch.exp(offsets[:,2])\n",
        "    proposals_[:,3] = anchors[:,3] * torch.exp(offsets[:,3])\n",
        "\n",
        "    # change format of proposals back from 'cxcywh' to 'xyxy'\n",
        "    proposals = torchvision.ops.box_convert(proposals_, in_fmt='cxcywh', out_fmt='xyxy')\n",
        "\n",
        "    return proposals.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAMvN1nEVZhZ"
      },
      "source": [
        "Visualization Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Bn6rgUFS29CJ"
      },
      "outputs": [],
      "source": [
        "def display_img(img_data, fig, axes):\n",
        "    for i, img in enumerate(img_data):\n",
        "        if type(img) == torch.Tensor:\n",
        "            img = img.permute(1, 2, 0).cpu().numpy()\n",
        "        axes[i].imshow(np.int64(img))\n",
        "    \n",
        "    return fig, axes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "trILCPQb2_n1"
      },
      "outputs": [],
      "source": [
        "def display_bbox(bboxes, fig, ax, classes=None, in_format='xyxy', color='y', line_width=3):\n",
        "    if type(bboxes) == np.ndarray:\n",
        "        bboxes = torch.from_numpy(bboxes)\n",
        "    if classes:\n",
        "        assert len(bboxes) == len(classes)\n",
        "    # convert boxes to xywh format\n",
        "    bboxes = torchvision.ops.box_convert(bboxes, in_fmt=in_format, out_fmt='xywh')\n",
        "    c = 0\n",
        "    for box in bboxes:\n",
        "        x, y, w, h = box.numpy()\n",
        "        # display bounding box\n",
        "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=line_width, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        # display category\n",
        "        if classes:\n",
        "            if classes[c] == 'pad':\n",
        "                continue\n",
        "            ax.text(x + 5, y + 20, classes[c], bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "        c += 1\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NkMgN6MsVZGk"
      },
      "outputs": [],
      "source": [
        "def display_grid(x_points, y_points, fig, ax, special_point=None):\n",
        "    # plot grid\n",
        "    for x in x_points:\n",
        "        for y in y_points:\n",
        "            ax.scatter(x, y, color=\"w\", marker='+')\n",
        "            \n",
        "    # plot a special point we want to emphasize on the grid\n",
        "    if special_point:\n",
        "        x, y = special_point\n",
        "        ax.scatter(x, y, color=\"red\", marker='+')\n",
        "        \n",
        "    return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzny0WVztwB3"
      },
      "source": [
        "Backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8IwgsR3DcVYC"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    req_layers = list(model.children())[:8] #Ignore AdaptiveAvgPool, Linear classifier Layer\n",
        "    self.backbone = torch.nn.Sequential(*req_layers)\n",
        "    for param in self.backbone.named_parameters():\n",
        "      param[1].requres_grad = True\n",
        "\n",
        "  def forward(self, img_data):\n",
        "    return self.backbone(img_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uv_voMXotBCh"
      },
      "outputs": [],
      "source": [
        "class ProposalModule(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, in_features, hidden_dim=512, n_anchors=9, p_dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.n_anchors = n_anchors\n",
        "    self.conv1 = torch.nn.Conv2d(in_features, hidden_dim, kernel_size=3, padding=1).to(device)\n",
        "    self.droput= torch.nn.Dropout(p_dropout).to(device)\n",
        "    self.conf_head = torch.nn.Conv2d(hidden_dim, n_anchors, kernel_size=1).to(device)\n",
        "    self.reg_head = torch.nn.Conv2d(hidden_dim, n_anchors*4, kernel_size=1).to(device)\n",
        "  \n",
        "  def forward(self, feature_map, pos_anc_ind=None, neg_anc_ind=None, pos_anc_coords=None):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if pos_anc_ind is None or neg_anc_ind is None or pos_anc_coords is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "\n",
        "    out = self.conv1(feature_map)\n",
        "    out = self.droput(out)\n",
        "    out = torch.nn.functional.relu(out)\n",
        "\n",
        "    reg_offsets_pred = self.reg_head(out)  # (B, A*4, hmap, wmap)\n",
        "    conf_scores_pred = self.conf_head(out) # (B, A, hmap, wmap)\n",
        "\n",
        "    if mode=='train':\n",
        "      #get confidence scores\n",
        "      conf_scrors_pos = conf_scores_pred.flatten()[pos_anc_ind]\n",
        "      conf_scrors_neg = conf_scores_pred.flatten()[neg_anc_ind]\n",
        "      \n",
        "      #get offsets for positive anchors\n",
        "      offsets_pos = reg_offsets_pred.contiguous().view(-1, 4)[pos_anc_ind]\n",
        "      #generate proposals using offsets\n",
        "      proposals   = generate_proposals(pos_anc_coords, offsets_pos)\n",
        "\n",
        "      return conf_scrors_pos.to(device), conf_scrors_neg.to(device), offsets_pos.to(device), proposals.to(device)\n",
        "    else:\n",
        "      return conf_scores_pred.to(device), reg_offsets_pred.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcSUf2pvWh4m"
      },
      "source": [
        "##Model\n",
        "stage 1 of the detector\n",
        "##RPN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-JjLYGLCAT38"
      },
      "outputs": [],
      "source": [
        "class RegionProposalNetwork(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.img_height, self.img_width = img_size\n",
        "\n",
        "        # scales and ratios for anchor boxes\n",
        "        self.anc_scales = [2, 4, 6]\n",
        "        self.anc_ratios = [0.5, 1, 1.5]\n",
        "        self.n_anc_boxes = len(self.anc_scales) * len(self.anc_ratios)\n",
        "        \n",
        "        # IoU thresholds for +ve and -ve anchors\n",
        "        self.pos_thresh = 0.7\n",
        "        self.neg_thresh = 0.3\n",
        "        \n",
        "        # weights for loss\n",
        "        self.w_conf = 1\n",
        "        self.w_reg = 5\n",
        "        \n",
        "        self.feature_extractor = FeatureExtractor() #feature_map \n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "      \n",
        "        batch_size = images.size(dim=0)\n",
        "        feature_map = self.feature_extractor(images)\n",
        "        out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "        # downsampling scale factor \n",
        "        width_scale_factor = self.img_width // out_w\n",
        "        height_scale_factor = self.img_height // out_h \n",
        "        \n",
        "        # generate anchors\n",
        "        anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "        anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "        anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "        \n",
        "        # get positive and negative anchors amongst other things\n",
        "        gt_bboxes_proj = project_bboxes(gt_bboxes, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "        \n",
        "        positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "        GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "        negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes)\n",
        "        \n",
        "        # pass through the proposal module\n",
        "        proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "        conf_scores_pos, conf_scores_neg, offsets_pos, proposals = proposal_module(feature_map, positive_anc_ind, negative_anc_ind, positive_anc_coords)\n",
        "        \n",
        "        cls_loss = calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size)\n",
        "        reg_loss = calc_bbox_reg_loss(GT_offsets, offsets_pos, batch_size)\n",
        "        \n",
        "        total_rpn_loss = self.w_conf * cls_loss + self.w_reg * reg_loss\n",
        "        \n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        return total_rpn_loss.to(device), feature_map.to(device), proposals.to(device), positive_anc_ind_sep.to(device), GT_class_pos.to(device), [out_c, out_h, out_w]\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            batch_size = images.size(dim=0)\n",
        "            feature_map = self.feature_extractor(images)\n",
        "            out_c, out_h, out_w = feature_map.size(dim=1), feature_map.size(dim=2), feature_map.size(dim=3)\n",
        "\n",
        "            # downsampling scale factor \n",
        "            width_scale_factor = self.img_width // out_w\n",
        "            height_scale_factor = self.img_height // out_h \n",
        "            \n",
        "            # generate anchors\n",
        "            anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))\n",
        "            anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, self.anc_scales, self.anc_ratios, (out_h, out_w))\n",
        "            anc_boxes_all = anc_base.repeat(batch_size, 1, 1, 1, 1)\n",
        "            anc_boxes_flat = anc_boxes_all.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # get conf scores and offsets\n",
        "            proposal_module = ProposalModule(out_c, n_anchors=self.n_anc_boxes)\n",
        "            conf_scores_pred, offsets_pred = proposal_module(feature_map)\n",
        "            conf_scores_pred = conf_scores_pred.reshape(batch_size, -1)\n",
        "            offsets_pred = offsets_pred.reshape(batch_size, -1, 4)\n",
        "\n",
        "            # filter out proposals based on conf threshold and nms threshold for each image\n",
        "            proposals_final = []\n",
        "            conf_scores_final = []\n",
        "            for i in range(batch_size):\n",
        "                conf_scores = torch.sigmoid(conf_scores_pred[i])\n",
        "                offsets = offsets_pred[i]\n",
        "                anc_boxes = anc_boxes_flat[i]\n",
        "                proposals = generate_proposals(anc_boxes, offsets)\n",
        "                # filter based on confidence threshold\n",
        "                conf_idx = torch.where(conf_scores >= conf_thresh)[0]\n",
        "                conf_scores_pos = conf_scores[conf_idx]\n",
        "                proposals_pos = proposals[conf_idx]\n",
        "                # filter based on nms threshold\n",
        "                nms_idx = torchvision.ops.nms(proposals_pos, conf_scores_pos, nms_thresh)\n",
        "                conf_scores_pos = conf_scores_pos[nms_idx]\n",
        "                proposals_pos = proposals_pos[nms_idx]\n",
        "\n",
        "                proposals_final.append(proposals_pos)\n",
        "                conf_scores_final.append(conf_scores_pos)\n",
        "            \n",
        "        return proposals_final, conf_scores_final, feature_map, [out_c, out_h, out_w]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgHib0mggfhI"
      },
      "source": [
        "Loss Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mlBmZEHOg-KA"
      },
      "outputs": [],
      "source": [
        "def calc_cls_loss(conf_scores_pos, conf_scores_neg, batch_size):\n",
        "    target_pos = torch.ones_like(conf_scores_pos)\n",
        "    target_neg = torch.zeros_like(conf_scores_neg)\n",
        "    \n",
        "    target = torch.cat((target_pos, target_neg))\n",
        "    inputs = torch.cat((conf_scores_pos, conf_scores_neg))\n",
        "     \n",
        "    loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, target, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Bd0R3ujvgf30"
      },
      "outputs": [],
      "source": [
        "def calc_bbox_reg_loss(gt_offsets, reg_offsets_pos, batch_size):\n",
        "    assert gt_offsets.size() == reg_offsets_pos.size()\n",
        "    loss = torch.nn.functional.smooth_l1_loss(reg_offsets_pos, gt_offsets, reduction='sum') * 1. / batch_size\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT7c81VcgeZM"
      },
      "source": [
        "Stage 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EGRmD_-YtswD"
      },
      "outputs": [],
      "source": [
        "class ClassificationModule(torch.nn.Module):\n",
        "    def __init__(self, out_channels, n_classes, roi_size, hidden_dim=512, p_dropout=0.3):\n",
        "        super().__init__()      \n",
        "        self.roi_size = roi_size\n",
        "        # hidden network\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.avg_pool = torch.nn.AvgPool2d(self.roi_size).to(device)\n",
        "        self.fc = torch.nn.Linear(out_channels, hidden_dim).to(device)\n",
        "        self.dropout = torch.nn.Dropout(p_dropout).to(device)\n",
        "        \n",
        "        # define classification head\n",
        "        self.cls_head = torch.nn.Linear(hidden_dim, n_classes).to(device)\n",
        "        \n",
        "    def forward(self, feature_map, proposals_list, gt_classes=None):\n",
        "        \n",
        "        if gt_classes is None:\n",
        "            mode = 'eval'\n",
        "        else:\n",
        "            mode = 'train'\n",
        "        \n",
        "        # apply roi pooling on proposals followed by avg pooling\n",
        "        roi_out = torchvision.ops.roi_pool(feature_map, proposals_list, self.roi_size)\n",
        "        roi_out = self.avg_pool(roi_out)\n",
        "        \n",
        "        # flatten the output\n",
        "        roi_out = roi_out.squeeze(-1).squeeze(-1)\n",
        "        # pass the output through the hidden network\n",
        "        out = self.fc(roi_out)\n",
        "        out = torch.nn.functional.relu(self.dropout(out))\n",
        "        \n",
        "        # get the classification scores\n",
        "        cls_scores = self.cls_head(out)\n",
        "        \n",
        "        if mode == 'eval':\n",
        "            return cls_scores\n",
        "        \n",
        "        # compute cross entropy loss\n",
        "        cls_loss = torch.nn.functional.cross_entropy(cls_scores, gt_classes.long())\n",
        "        \n",
        "        return cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3pKtpIm82K"
      },
      "source": [
        "wrap it up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FaFEtLdRgYED"
      },
      "outputs": [],
      "source": [
        "class TwoStageDetector(torch.nn.Module):\n",
        "    def __init__(self, img_size, n_classes, roi_size):\n",
        "        super().__init__() \n",
        "        self.rpn = RegionProposalNetwork(img_size)\n",
        "        \n",
        "    def forward(self, images, gt_bboxes, gt_classes):\n",
        "        total_rpn_loss, feature_map, proposals, \\\n",
        "        positive_anc_ind_sep, GT_class_pos, out_size = self.rpn(images, gt_bboxes, gt_classes)\n",
        "        out_c, out_h, out_w = out_size\n",
        "        # get separate proposals for each sample\n",
        "        pos_proposals_list = []\n",
        "        batch_size = images.size(dim=0)\n",
        "        for idx in range(batch_size):\n",
        "            proposal_idxs = torch.where(positive_anc_ind_sep == idx)[0]\n",
        "            proposals_sep = proposals[proposal_idxs].detach().clone()\n",
        "            pos_proposals_list.append(proposals_sep)\n",
        "        \n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_loss = classifier(feature_map, pos_proposals_list, GT_class_pos)\n",
        "        total_loss = cls_loss + total_rpn_loss\n",
        "        \n",
        "        return total_loss\n",
        "    \n",
        "    def inference(self, images, conf_thresh=0.5, nms_thresh=0.7):\n",
        "        batch_size = images.size(dim=0)\n",
        "        proposals_final, conf_scores_final, feature_map, out_size = self.rpn.inference(images, conf_thresh, nms_thresh)\n",
        "        out_c, out_h, out_w = out_size\n",
        "\n",
        "        classifier = ClassificationModule(out_c, n_classes, roi_size)\n",
        "        cls_scores = classifier(feature_map, proposals_final)\n",
        "        \n",
        "        # convert scores into probability\n",
        "        cls_probs = torch.nn.functional.softmax(cls_scores, dim=-1)\n",
        "        # get classes with highest probability\n",
        "        classes_all = torch.argmax(cls_probs, dim=-1)\n",
        "        \n",
        "        classes_final = []\n",
        "        # slice classes to map to their corresponding image\n",
        "        c = 0\n",
        "        for i in range(batch_size):\n",
        "            n_proposals = len(proposals_final[i]) # get the number of proposals for each image\n",
        "            classes_final.append(classes_all[c: c+n_proposals])\n",
        "            c += n_proposals\n",
        "            \n",
        "        return proposals_final, conf_scores_final, classes_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VF-O12KLyIk"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tGBVjt0i_upr"
      },
      "outputs": [],
      "source": [
        "img_width = 640\n",
        "img_height = 480\n",
        "data_dir = '/content/data/'\n",
        "# name2idx = {'pad': -1, 'camel': 0, 'bird': 1}\n",
        "name2idx = {'pad': -1, 'license': 0}\n",
        "idx2name = {v:k for k, v in name2idx.items()}\n",
        "\n",
        "# run the image through the backbone\n",
        "img_size = (img_height, img_width)\n",
        "n_classes = len(name2idx) - 1 # exclude pad idx\n",
        "roi_size = (2, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASeYFJo4s8kC",
        "outputId": "42f6f53e-d63e-4bec-e0d6-f57dcfd8c11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 301MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "od_dataset = ObjectDetectionDataset(data_dir, img_size, device)\n",
        "od_dataloader = torch.utils.data.DataLoader(od_dataset, batch_size=2)\n",
        "\n",
        "detector = TwoStageDetector(img_size, n_classes, roi_size)\n",
        "detector.to(device)\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TFwPGbtFD3vs"
      },
      "outputs": [],
      "source": [
        "# for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "#     img_data_all = img_batch\n",
        "#     gt_bboxes_all = gt_bboxes_batch\n",
        "#     gt_classes_all = gt_classes_batch\n",
        "#     break\n",
        "    \n",
        "# img_data_all = img_data_all[:2]\n",
        "# gt_bboxes_all = gt_bboxes_all[:2]\n",
        "# gt_classes_all = gt_classes_all[:2]\n",
        "\n",
        "# total_loss = detector(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "# proposals_final, conf_scores_final, classes_final = detector.inference(img_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AWW646M8D5HP"
      },
      "outputs": [],
      "source": [
        "def training_loop(model, learning_rate, train_dataloader, n_epochs):\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model.train()\n",
        "    loss_list = []\n",
        "    \n",
        "    for i in tqdm.tqdm(range(n_epochs)):\n",
        "        total_loss = 0\n",
        "        for img_batch, gt_bboxes_batch, gt_classes_batch in train_dataloader:\n",
        "            # forward pass\n",
        "            loss = model(img_batch, gt_bboxes_batch, gt_classes_batch)\n",
        "            # print(loss.item())\n",
        "            # backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            break \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        loss_list.append(total_loss)\n",
        "        \n",
        "    return loss_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MFnKE3neESZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee31dc0-6a2c-4e69-d3ff-6db9bcf3981f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:14<00:00,  1.44s/it]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-3\n",
        "n_epochs = 10\n",
        "\n",
        "loss_list = training_loop(detector, learning_rate, od_dataloader, n_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "n_dOW6SREUn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "e812f3ed-c463-4c58-e1ad-6db40ef3297f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fccf1d73730>]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ttsY5-kEEccw"
      },
      "outputs": [],
      "source": [
        "torch.save(detector.state_dict(), \"model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ],
      "metadata": {
        "id": "6PzwsZT6WoWC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "1wJXbPO5Eny_"
      },
      "outputs": [],
      "source": [
        "detector.eval()\n",
        "proposals_final, conf_scores_final, classes_final = detector.inference(img_data_all, conf_thresh=0.99, nms_thresh=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "proposals_final, conf_scores_final, classes_final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ4MFONZWvSY",
        "outputId": "81c7e9c3-6fb1-42c5-a08a-31e6b97897ae"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([tensor([[ 9.6488,  1.2736, 15.9819,  5.2117]], device='cuda:0'),\n",
              "  tensor([], device='cuda:0', size=(0, 4))],\n",
              " [tensor([0.9904], device='cuda:0'), tensor([], device='cuda:0')],\n",
              " [tensor([0], device='cuda:0'),\n",
              "  tensor([], device='cuda:0', dtype=torch.int64)])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "JXCX9h1QEnwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "92c4e585-dcd9-4962-9627-76de1bc86424"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c1ed5db4311a>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# run the image through the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "# project proposals to the image space\n",
        "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "req_layers = list(model.children())[:8]\n",
        "backbone = torch.nn.Sequential(*req_layers)\n",
        "# unfreeze all the parameters\n",
        "for param in backbone.named_parameters():\n",
        "    param[1].requires_grad = True\n",
        "# run the image through the backbone\n",
        "out = backbone(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w\n",
        "\n",
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h \n",
        "prop_proj_1 = project_bboxes(proposals_final[0], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "prop_proj_2 = project_bboxes(proposals_final[1], width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# get classes\n",
        "classes_pred_1 = [idx2name[cls] for cls in classes_final[0].tolist()]\n",
        "classes_pred_2 = [idx2name[cls] for cls in classes_final[1].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_f0DiBDEq1n"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
        "\n",
        "fig, axes = display_img(img_batch, fig, axes)\n",
        "fig, _ = display_bbox(prop_proj_1, fig, axes[0], classes=classes_pred_1)\n",
        "fig, _ = display_bbox(prop_proj_2, fig, axes[1], classes=classes_pred_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdLNbWebE8HN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abSHP_PbNPZF"
      },
      "source": [
        "##Test part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "I_AThM_TyKL8"
      },
      "outputs": [],
      "source": [
        "for img_batch, gt_bboxes_batch, gt_classes_batch in od_dataloader:\n",
        "    img_data_all = img_batch\n",
        "    gt_bboxes_all = gt_bboxes_batch\n",
        "    gt_classes_all = gt_classes_batch\n",
        "    break\n",
        "    \n",
        "img_data_all = img_data_all[:2]\n",
        "gt_bboxes_all = gt_bboxes_all[:2]\n",
        "gt_classes_all = gt_classes_all[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DblvBtdQZoZL"
      },
      "outputs": [],
      "source": [
        "# get class names\n",
        "gt_class_1 = gt_classes_all[0].long()\n",
        "gt_class_1 = [idx2name[idx.item()] for idx in gt_class_1]\n",
        "\n",
        "gt_class_2 = gt_classes_all[1].long()\n",
        "gt_class_2 = [idx2name[idx.item()] for idx in gt_class_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "pIAH99PhbTjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "b08c6ec9-304b-413f-8068-35b815c85a35"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-7f432e276da2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_bboxes_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_class_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_bboxes_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgt_class_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-44c2337387d1>\u001b[0m in \u001b[0;36mdisplay_img\u001b[0;34m(img_data, fig, axes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 4))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0], classes=gt_class_1)\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1], classes=gt_class_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ivNrzM_PbViM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "6ca85a6f-3652-4631-f662-cb5c8ee097b7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f6ae5e80acda>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# run the image through the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mout_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "req_layers = list(model.children())[:8]\n",
        "backbone = torch.nn.Sequential(*req_layers)\n",
        "# unfreeze all the parameters\n",
        "for param in backbone.named_parameters():\n",
        "    param[1].requires_grad = True\n",
        "# run the image through the backbone\n",
        "out = backbone(img_data_all)\n",
        "out_c, out_h, out_w = out.size(dim=1), out.size(dim=2), out.size(dim=3)\n",
        "out_c, out_h, out_w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9bkzVXqd1li"
      },
      "outputs": [],
      "source": [
        "width_scale_factor = img_width // out_w\n",
        "height_scale_factor = img_height // out_h\n",
        "height_scale_factor, width_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K5ahtlpigmd"
      },
      "outputs": [],
      "source": [
        "\n",
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 4))\n",
        "\n",
        "filters_data =[filters[0].detach().numpy() for filters in out[:2]]\n",
        "\n",
        "fig, axes = display_img(filters_data, fig, axes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7cuUZO_iu-K"
      },
      "outputs": [],
      "source": [
        "anc_pts_x, anc_pts_y = gen_anc_centers(out_size=(out_h, out_w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9ZeF-HiiyTd"
      },
      "outputs": [],
      "source": [
        "# project anchor centers onto the original image\n",
        "anc_pts_x_proj = anc_pts_x.clone() * width_scale_factor \n",
        "anc_pts_y_proj = anc_pts_y.clone() * height_scale_factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbFH8Ieiiq_"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(8, 4))\n",
        " \n",
        "# project anchor centers onto the original image\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6nmibPBiyqI"
      },
      "outputs": [],
      "source": [
        "anc_scales = [2, 4, 6]\n",
        "anc_ratios = [0.5, 1, 1.5]\n",
        "n_anc_boxes = len(anc_scales) * len(anc_ratios) # number of anchor boxes for each anchor point\n",
        "\n",
        "anc_base = gen_anc_boxes(anc_pts_x, anc_pts_y, anc_scales, anc_ratios, (out_h, out_w))\n",
        "# since all the images are scaled to the same size\n",
        "# we can repeat the anchor base for all the images\n",
        "anc_boxes_all = anc_base.repeat(img_data_all.size(dim=0), 1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23_z2G6EjQ6E"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# project anchor boxes to the image\n",
        "anc_boxes_proj = project_bboxes(anc_boxes_all, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# plot anchor boxes around selected anchor points\n",
        "sp_1 = [5, 8]\n",
        "sp_2 = [12, 9]\n",
        "bboxes_1 = anc_boxes_proj[0][sp_1[0], sp_1[1]]\n",
        "bboxes_2 = anc_boxes_proj[1][sp_2[0], sp_2[1]]\n",
        "\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0], (anc_pts_x_proj[sp_1[0]], anc_pts_y_proj[sp_1[1]]))\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1], (anc_pts_x_proj[sp_2[0]], anc_pts_y_proj[sp_2[1]]))\n",
        "fig, _ = display_bbox(bboxes_1, fig, axes[0])\n",
        "fig, _ = display_bbox(bboxes_2, fig, axes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1By-2dIAciy"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot feature grid\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[0])\n",
        "fig, _ = display_grid(anc_pts_x_proj, anc_pts_y_proj, fig, axes[1])\n",
        "\n",
        "# plot all anchor boxes\n",
        "for x in range(anc_pts_x_proj.size(dim=0)):\n",
        "    for y in range(anc_pts_y_proj.size(dim=0)):\n",
        "        bboxes = anc_boxes_proj[0][x, y]\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[0], line_width=1)\n",
        "        fig, _ = display_bbox(bboxes, fig, axes[1], line_width=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHRtre0LChGO"
      },
      "outputs": [],
      "source": [
        "#Get Positive and Negative Anchors\n",
        "\n",
        "pos_thresh = 0.7\n",
        "neg_thresh = 0.3\n",
        "\n",
        "# project gt bboxes onto the feature map\n",
        "gt_bboxes_proj = project_bboxes(gt_bboxes_all, width_scale_factor, height_scale_factor, mode='p2a')\n",
        "positive_anc_ind, negative_anc_ind, GT_conf_scores, \\\n",
        "GT_offsets, GT_class_pos, positive_anc_coords, \\\n",
        "negative_anc_coords, positive_anc_ind_sep = get_req_anchors(anc_boxes_all, gt_bboxes_proj, gt_classes_all, pos_thresh, neg_thresh)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euzen8haC4hh"
      },
      "outputs": [],
      "source": [
        "# project anchor coords to the image space\n",
        "pos_anc_proj = project_bboxes(positive_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "neg_anc_proj = project_bboxes(negative_anc_coords, width_scale_factor, height_scale_factor, mode='a2p')\n",
        "\n",
        "# grab +ve and -ve anchors for each image separately\n",
        "\n",
        "anc_idx_1 = torch.where(positive_anc_ind_sep == 0)[0]\n",
        "anc_idx_2 = torch.where(positive_anc_ind_sep == 1)[0]\n",
        "\n",
        "pos_anc_1 = pos_anc_proj[anc_idx_1]\n",
        "pos_anc_2 = pos_anc_proj[anc_idx_2]\n",
        "\n",
        "neg_anc_1 = neg_anc_proj[anc_idx_1]\n",
        "neg_anc_2 = neg_anc_proj[anc_idx_2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izSor4SeC59c"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = (1, 2)\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(16, 8))\n",
        "\n",
        "fig, axes = display_img(img_data_all, fig, axes)\n",
        "\n",
        "# plot groundtruth bboxes\n",
        "fig, _ = display_bbox(gt_bboxes_all[0], fig, axes[0])\n",
        "fig, _ = display_bbox(gt_bboxes_all[1], fig, axes[1])\n",
        "\n",
        "# plot positive anchor boxes\n",
        "fig, _ = display_bbox(pos_anc_1, fig, axes[0], color='g')\n",
        "fig, _ = display_bbox(pos_anc_2, fig, axes[1], color='g')\n",
        "\n",
        "# plot negative anchor boxes\n",
        "fig, _ = display_bbox(neg_anc_1, fig, axes[0], color='r')\n",
        "fig, _ = display_bbox(neg_anc_2, fig, axes[1], color='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qak0xCjZRXVb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FF2pFCXWgHC4",
        "vcSUf2pvWh4m"
      ],
      "mount_file_id": "11_HGJnQcno3E-o1OioGQBwA6f_YItBbW",
      "authorship_tag": "ABX9TyOVvJrGtKFbcwNSWfL/hbHI",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}