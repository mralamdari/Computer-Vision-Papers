{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mralamdari/Computer-Vision-Papers/blob/main/Faster_RCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NZ2XqKRSf4Ca",
        "outputId": "5b531476-db65-4b4d-d109-2aab745d0f6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.11.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "from sklearn import model_selection\n",
        "\n",
        "# from tensorflow.python.ops.numpy_ops import np_config\n",
        "# np_config.enable_numpy_behavior()\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2pFCXWgHC4"
      },
      "source": [
        "##Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOqkUHTgf540"
      },
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive'\n",
        "!kaggle datasets download -d andrewmvd/car-plate-detection\n",
        "!unzip \\*.zip && rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NxSpKUU8f52c"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/data/images/'\n",
        "ANNOTATION_PATH =  '/content/data/annotations/'\n",
        "\n",
        "os.makedirs('/content/data/', exist_ok=True)\n",
        "os.replace('/content/images', '/content/data/images')\n",
        "os.replace('/content/annotations', '/content/data/annotations')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YO9lXPAuf7XO"
      },
      "outputs": [],
      "source": [
        "def xml_to_XY(path, img_path):\n",
        "  Y = []\n",
        "  X = []\n",
        "  for xml_file in glob.glob(path+'*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    img_name = img_path + xml_file.replace(path, '')[:-4]+'.png'\n",
        "\n",
        "    h, w, d = plt.imread(img_name).shape\n",
        "\n",
        "    img = tf.keras.preprocessing.image.load_img(img_name, target_size=(320, 320))\n",
        "    img_arr = tf.keras.preprocessing.image.img_to_array(img) / 255.0\n",
        "\n",
        "    for children in root.findall('object'):\n",
        "      bndbox = children[5]\n",
        "      xmin = float(bndbox[0].text) / w\n",
        "      ymin = float(bndbox[1].text) / h\n",
        "      xmax = float(bndbox[2].text) / w\n",
        "      ymax = float(bndbox[3].text) / h\n",
        "      X.append(img_arr)\n",
        "      Y.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BxEWR14Pf9Y2"
      },
      "outputs": [],
      "source": [
        "x, y = xml_to_XY(ANNOTATION_PATH, IMAGE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiLftcsgf9WL",
        "outputId": "87a6b546-0122-4006-9ce6-08a6c50c856f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((471, 320, 320, 3), (471, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "x.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooZKiJNLi59V",
        "outputId": "41130812-9e82-4ad0-f7ee-7b30906d63b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((376, 320, 320, 3), (95, 320, 320, 3), (376, 4), (95, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(x, y, test_size=0.2, random_state=26)\n",
        "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ],
      "metadata": {
        "id": "0TQlgZtkmDr9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAxvvKwygIzV"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nme9tDNRfZg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4da7300-43aa-4224-8c4a-ba63b61c780c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "num_classes = 1\n",
        "base_model = tf.keras.applications.VGG16(input_shape=[320, 320, 3], include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Define the RPN\n",
        "rpn_conv = tf.keras.layers.Conv2D(512, 3, padding='same', activation='relu', name='rpn_conv')\n",
        "rpn_cls  = tf.keras.layers.Conv2D(9 * 2, 1, name='rpn_cls')\n",
        "rpn_reg  = tf.keras.layers.Conv2D(9 * 4, 1, name='rpn_reg')\n",
        "\n",
        "# Define the RoI pooling layer\n",
        "roi_pooling = tf.keras.layers.Lambda(lambda x: tf.image.crop_and_resize(*x), name='roi_pooling')\n",
        "\n",
        "# Define the classifier\n",
        "classifier = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(4096, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(4096, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(num_classes + 1)\n",
        "])\n",
        "regressor = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(num_classes * 4)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CTditA8S-lK3"
      },
      "outputs": [],
      "source": [
        "# Define the complete Faster R-CNN model\n",
        "inputs = tf.keras.Input(shape=[None, None, 3])\n",
        "x = base_model(inputs)\n",
        "x_rpn_conv = rpn_conv(x)\n",
        "x_rpn_cls = rpn_cls(x_rpn_conv)\n",
        "x_rpn_reg = rpn_reg(x_rpn_conv)\n",
        "\n",
        "rpn = tf.keras.Model(inputs, [x_rpn_cls, x_rpn_reg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awF1bfqugN_d",
        "outputId": "e927ee46-5d63-466c-a523-3b36522f44bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 10, 10, 512) dtype=float32 (created by layer 'rpn_conv')>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "x_rpn_cls, x_rpn_reg\n",
        "x_rpn_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mhANZRWmQ_GJ"
      },
      "outputs": [],
      "source": [
        "def generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=[8, 16, 32]):\n",
        "    # Compute the base anchor\n",
        "    base_anchor = tf.constant([1, 1, base_size, base_size], dtype=tf.float32) - 1\n",
        "\n",
        "    # Compute the ratio anchors\n",
        "    ratio_anchors = _ratio_enum(base_anchor, ratios)\n",
        "\n",
        "    # Compute the scale anchors\n",
        "    anchors = tf.concat([_scale_enum(ratio_anchors[i, :], scales) for i in range(ratio_anchors.shape[0])], axis=0)\n",
        "    return anchors\n",
        "\n",
        "def _xy_to_whCenter(anchor):\n",
        "    #Xmin, Xmax, Ymin, Ymax ==> width, height, Xcenter, Ycenter\n",
        "    x1, y1, x2, y2 = anchor\n",
        "    w = x2 - x1 + 1\n",
        "    h = y2 - y1 + 1\n",
        "    x_ctr = x1 + (w-1)/2\n",
        "    y_ctr = y1 + (h-1)/2\n",
        "    return w, h, x_ctr, y_ctr\n",
        "\n",
        "\n",
        "def _mkanchors(ws , hs , x_ctr , y_ctr):\n",
        "# Given a vector of widths (ws) and heights (hs) around a center (x_ctr,y_ctr), output a set of anchors (windows).\n",
        "  ws=tf.reshape(ws , [-1 , 1])\n",
        "  hs=tf.reshape(hs , [-1 , 1])\n",
        "\n",
        "  anchors=tf.concat([x_ctr - 0.5 * (ws-1), y_ctr - 0.5 * (hs-1), x_ctr + 0.5 * (ws-1), y_ctr + 0.5 * (hs-1)],axis=-1)\n",
        "  return anchors\n",
        "\n",
        "def _ratio_enum(anchor, ratios):\n",
        "# Enumerate a set of anchors for each aspect ratio wrt an anchor.\n",
        "  w, h, x_ctr, y_ctr = _xy_to_whCenter(anchor)\n",
        "  size = w * h\n",
        "  size_ratios = size / ratios\n",
        "  ws = tf.round(tf.sqrt(size_ratios))\n",
        "  hs = ws * ratios\n",
        "  anchors=_mkanchors(ws , hs , x_ctr , y_ctr)\n",
        "  return anchors\n",
        "\n",
        "def _scale_enum(anchor,scales):\n",
        "# Enumerate a set of anchors for each scale wrt an anchor.\n",
        "  w, h, x_ctr, y_ctr=_xy_to_whCenter(anchor)\n",
        "  ws=w*scales \n",
        "  hs=h*scales \n",
        "  anchors=_mkanchors(ws , hs , x_ctr , y_ctr)\n",
        "  return anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRzbO_740KzM",
        "outputId": "611b4f68-3a28-405c-edef-ce2226d1812f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(9, 4), dtype=float32, numpy=\n",
              "array([[ -84.,  -38.,   99.,   53.],\n",
              "       [-176.,  -84.,  191.,   99.],\n",
              "       [-360., -176.,  375.,  191.],\n",
              "       [ -56.,  -56.,   71.,   71.],\n",
              "       [-120., -120.,  135.,  135.],\n",
              "       [-248., -248.,  263.,  263.],\n",
              "       [ -36.,  -80.,   51.,   95.],\n",
              "       [ -80., -168.,   95.,  183.],\n",
              "       [-168., -344.,  183.,  359.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "anchors = generate_anchors(base_size=16, ratios=[0.5, 1, 2], scales=[8, 16, 32])\n",
        "anchors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v1TktM3lBIoQ"
      },
      "outputs": [],
      "source": [
        "def rpn_to_roi(rpn_cls, rpn_reg, anchors):\n",
        "    # Convert logits to probabilities\n",
        "    rpn_cls_prob = tf.nn.softmax(rpn_cls)\n",
        "\n",
        "    # Get the foreground probabilities for each anchor\n",
        "    fg_prob = rpn_cls_prob[..., 1]\n",
        "\n",
        "    # Decode the predicted bounding boxes\n",
        "    pred_boxes = decode_boxes(rpn_reg, anchors)\n",
        "\n",
        "    # Apply non-maximum suppression to remove overlapping boxes\n",
        "    indices = tf.image.non_max_suppression(pred_boxes, fg_prob,\n",
        "                                           max_output_size=2000,\n",
        "                                           iou_threshold=0.7)\n",
        "\n",
        "    rois = tf.gather(pred_boxes, indices)\n",
        "    \n",
        "    return rois\n",
        "    \n",
        "\n",
        "def decode_boxes(deltas, anchors):\n",
        "    # Compute the width and height of each anchor box\n",
        "    wa = anchors[..., 2] - anchors[..., 0]\n",
        "    ha = anchors[..., 3] - anchors[..., 1]\n",
        "\n",
        "    # Compute the center coordinates of each anchor box\n",
        "    xa = (anchors[..., 2] + anchors[..., 0]) / 2\n",
        "    ya = (anchors[..., 3] + anchors[..., 1]) / 2\n",
        "\n",
        "    # Decode the predicted bounding box deltas\n",
        "    ty = deltas[..., 0]\n",
        "    tx = deltas[..., 1]\n",
        "    th = deltas[..., 2]\n",
        "    tw = deltas[..., 3]\n",
        "    print(deltas[..., 3].shape)\n",
        "    w = tf.math.multiply(tf.math.exp(tw), wa)\n",
        "    h = tf.math.multiply(tf.math.exp(th), ha)\n",
        "\n",
        "    y = ty * ha + ya\n",
        "    x = tx * wa + xa\n",
        "\n",
        "    # Compute the coordinates of the decoded bounding boxes\n",
        "    y1=x-w/2 \n",
        "    x1=y-h/2 \n",
        "    y2=x+w/2 \n",
        "    x2=y+h/2 \n",
        "\n",
        "    return tf.stack([y1,x1,y2,x2],axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gXxIdkwJ-hnx"
      },
      "outputs": [],
      "source": [
        "rois = rpn_to_roi(x_rpn_cls, x_rpn_reg, anchors) # Generate RoIs using the RPN outputs\n",
        "\n",
        "pooled_rois = roi_pooling([x] + rois)\n",
        "\n",
        "cls_output = classifier(pooled_rois)\n",
        "reg_output = regressor(pooled_rois)\n",
        "\n",
        "faster_rcnn = tf.kers.Model(inputs, [cls_output, reg_output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kBk5d1Zsfewr"
      },
      "outputs": [],
      "source": [
        "rpn_reg[..., 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kgD_p18Ufeud"
      },
      "outputs": [],
      "source": [
        "rpn_reg[..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MjsopT_fYLn"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hTUCjgxkw9E"
      },
      "outputs": [],
      "source": [
        "import torch as tt\n",
        "import numpy as np\n",
        "from utils import array_tool as at\n",
        "from model.utils.bbox_tools import loc2bbox\n",
        "from torchvision.ops import nms\n",
        "\n",
        "from torch import nn\n",
        "from data.dataset import preprocess\n",
        "from torch.nn import functional as F\n",
        "from utils.config import opt\n",
        "\n",
        "\n",
        "def nograd(f):\n",
        "    def new_f(*args,**kwargs):\n",
        "        with t.no_grad():\n",
        "           return f(*args,**kwargs)\n",
        "    return new_f\n",
        "\n",
        "class FasterRCNN(nn.Module):HW image\n",
        "    def __init__(self, extractor, rpn, head,\n",
        "                loc_normalize_mean = (0., 0., 0., 0.),\n",
        "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)):\n",
        "      \n",
        "        super(FasterRCNN, self).__init__()\n",
        "        self.extractor = extractor\n",
        "        self.rpn = rpn\n",
        "        self.head = head\n",
        "\n",
        "        # mean and std\n",
        "        self.loc_normalize_mean = loc_normalize_mean\n",
        "        self.loc_normalize_std = loc_normalize_std\n",
        "        self.use_preset('evaluate')\n",
        "\n",
        "    @property\n",
        "    def n_class(self):\n",
        "        # Total number of classes including the background.\n",
        "        return self.head.n_class\n",
        "\n",
        "    def forward(self, x, scale=1.):\n",
        "        img_size = x.shape[2:]\n",
        "\n",
        "        h = self.extractor(x)\n",
        "        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n",
        "            self.rpn(h, img_size, scale)\n",
        "        roi_cls_locs, roi_scores = self.head(\n",
        "            h, rois, roi_indices)\n",
        "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
        "\n",
        "    def use_preset(self, preset):\n",
        "        if preset == 'visualize':\n",
        "            self.nms_thresh = 0.3\n",
        "            self.score_thresh = 0.7\n",
        "        elif preset == 'evaluate':\n",
        "            self.nms_thresh = 0.3\n",
        "            self.score_thresh = 0.05\n",
        "        else:\n",
        "            raise ValueError('preset must be visualize or evaluate')\n",
        "\n",
        "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
        "        bbox = list()\n",
        "        label = list()\n",
        "        score = list()\n",
        "        # skip cls_id = 0 because it is the background class\n",
        "        for l in range(1, self.n_class):\n",
        "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
        "            prob_l = raw_prob[:, l]\n",
        "            mask = prob_l > self.score_thresh\n",
        "            cls_bbox_l = cls_bbox_l[mask]\n",
        "            prob_l = prob_l[mask]\n",
        "            keep = nms(cls_bbox_l, prob_l,self.nms_thresh)\n",
        "            # import ipdb;ipdb.set_trace()\n",
        "            # keep = cp.asnumpy(keep)\n",
        "            bbox.append(cls_bbox_l[keep].cpu().numpy())\n",
        "            # The labels are in [0, self.n_class - 2].\n",
        "            label.append((l - 1) * np.ones((len(keep),)))\n",
        "            score.append(prob_l[keep].cpu().numpy())\n",
        "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
        "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
        "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
        "        return bbox, label, score\n",
        "\n",
        "    @nograd\n",
        "    def predict(self, imgs,sizes=None,visualize=False):\n",
        "        self.eval()\n",
        "        if visualize:\n",
        "            self.use_preset('visualize')\n",
        "            prepared_imgs = list()\n",
        "            sizes = list()\n",
        "            for img in imgs:\n",
        "                size = img.shape[1:]\n",
        "                img = preprocess(at.tonumpy(img))\n",
        "                prepared_imgs.append(img)\n",
        "                sizes.append(size)\n",
        "        else:\n",
        "             prepared_imgs = imgs \n",
        "        bboxes = list()\n",
        "        labels = list()\n",
        "        scores = list()\n",
        "        for img, size in zip(prepared_imgs, sizes):\n",
        "            img = at.totensor(img[None]).float()\n",
        "            scale = img.shape[3] / size[1]\n",
        "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n",
        "            # We are assuming that batch size is 1.\n",
        "            roi_score = roi_scores.data\n",
        "            roi_cls_loc = roi_cls_loc.data\n",
        "            roi = at.totensor(rois) / scale\n",
        "\n",
        "            # Convert predictions to bounding boxes in image coordinates.\n",
        "            # Bounding boxes are scaled to the scale of the input images.\n",
        "            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n",
        "                repeat(self.n_class)[None]\n",
        "            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n",
        "                repeat(self.n_class)[None]\n",
        "\n",
        "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
        "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
        "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
        "            cls_bbox = loc2bbox(at.tonumpy(roi).reshape((-1, 4)),\n",
        "                                at.tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
        "            cls_bbox = at.totensor(cls_bbox)\n",
        "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
        "            # clip bounding box\n",
        "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
        "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
        "\n",
        "            prob = (F.softmax(at.totensor(roi_score), dim=1))\n",
        "\n",
        "            bbox, label, score = self._suppress(cls_bbox, prob)\n",
        "            bboxes.append(bbox)\n",
        "            labels.append(label)\n",
        "            scores.append(score)\n",
        "\n",
        "        self.use_preset('evaluate')\n",
        "        self.train()\n",
        "        return bboxes, labels, scores\n",
        "\n",
        "    def get_optimizer(self):\n",
        "        lr = opt.lr\n",
        "        params = []\n",
        "        for key, value in dict(self.named_parameters()).items():\n",
        "            if value.requires_grad:\n",
        "                if 'bias' in key:\n",
        "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
        "                else:\n",
        "                    params += [{'params': [value], 'lr': lr, 'weight_decay': opt.weight_decay}]\n",
        "        if opt.use_adam:\n",
        "            self.optimizer = t.optim.Adam(params)\n",
        "        else:\n",
        "            self.optimizer = t.optim.SGD(params, momentum=0.9)\n",
        "        return self.optimizer\n",
        "\n",
        "    def scale_lr(self, decay=0.1):\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] *= decay\n",
        "        return self.optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8APJat3pcCq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from layers import *\n",
        "from data import voc, coco\n",
        "import os\n",
        "\n",
        "\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, phase, size, base, extras, head, num_classes):\n",
        "        super(SSD, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.num_classes = num_classes\n",
        "        self.cfg = (coco, voc)[num_classes == 21]\n",
        "        self.priorbox = PriorBox(self.cfg)\n",
        "        self.priors = Variable(self.priorbox.forward(), volatile=True)\n",
        "        self.size = size\n",
        "\n",
        "        # SSD network\n",
        "        self.vgg = nn.ModuleList(base)\n",
        "        # Layer learns to scale the l2 normalized features from conv4_3\n",
        "        self.L2Norm = L2Norm(512, 20)\n",
        "        self.extras = nn.ModuleList(extras)\n",
        "\n",
        "        self.loc = nn.ModuleList(head[0])\n",
        "        self.conf = nn.ModuleList(head[1])\n",
        "\n",
        "        if phase == 'test':\n",
        "            self.softmax = nn.Softmax(dim=-1)\n",
        "            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()\n",
        "        loc = list()\n",
        "        conf = list()\n",
        "\n",
        "        # apply vgg up to conv4_3 relu\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        s = self.L2Norm(x)\n",
        "        sources.append(s)\n",
        "\n",
        "        # apply vgg up to fc7\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "        sources.append(x)\n",
        "\n",
        "        # apply extra layers and cache source layer outputs\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k % 2 == 1:\n",
        "                sources.append(x)\n",
        "\n",
        "        # apply multibox head to source layers\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "        if self.phase == \"test\":\n",
        "            output = self.detect(\n",
        "                loc.view(loc.size(0), -1, 4),                   # loc preds\n",
        "                self.softmax(conf.view(conf.size(0), -1,\n",
        "                             self.num_classes)),                # conf preds\n",
        "                self.priors.type(type(x.data))                  # default boxes\n",
        "            )\n",
        "        else:\n",
        "            output = (\n",
        "                loc.view(loc.size(0), -1, 4),\n",
        "                conf.view(conf.size(0), -1, self.num_classes),\n",
        "                self.priors\n",
        "            )\n",
        "        return output\n",
        "\n",
        "    def load_weights(self, base_file):\n",
        "        other, ext = os.path.splitext(base_file)\n",
        "        if ext == '.pkl' or '.pth':\n",
        "            print('Loading weights into state dict...')\n",
        "            self.load_state_dict(torch.load(base_file,\n",
        "                                 map_location=lambda storage, loc: storage))\n",
        "            print('Finished!')\n",
        "        else:\n",
        "            print('Sorry only .pth and .pkl files supported.')\n",
        "\n",
        "\n",
        "# This function is derived from torchvision VGG make_layers()\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "def vgg(cfg, i, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = i\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'C':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6,\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "    return layers\n",
        "\n",
        "\n",
        "def add_extras(cfg, i, batch_norm=False):\n",
        "    # Extra layers added to VGG for feature scaling\n",
        "    layers = []\n",
        "    in_channels = i\n",
        "    flag = False\n",
        "    for k, v in enumerate(cfg):\n",
        "        if in_channels != 'S':\n",
        "            if v == 'S':\n",
        "                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n",
        "                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n",
        "            flag = not flag\n",
        "        in_channels = v\n",
        "    return layers\n",
        "\n",
        "\n",
        "def multibox(vgg, extra_layers, cfg, num_classes):\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "    vgg_source = [21, -2]\n",
        "    for k, v in enumerate(vgg_source):\n",
        "        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
        "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
        "        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
        "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
        "    for k, v in enumerate(extra_layers[1::2], 2):\n",
        "        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
        "                                 * 4, kernel_size=3, padding=1)]\n",
        "        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n",
        "                                  * num_classes, kernel_size=3, padding=1)]\n",
        "    return vgg, extra_layers, (loc_layers, conf_layers)\n",
        "\n",
        "\n",
        "base = {\n",
        "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
        "            512, 512, 512],\n",
        "    '512': [],\n",
        "}\n",
        "extras = {\n",
        "    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n",
        "    '512': [],\n",
        "}\n",
        "mbox = {\n",
        "    '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
        "    '512': [],\n",
        "}\n",
        "\n",
        "\n",
        "def build_ssd(phase, size=300, num_classes=21):\n",
        "    if phase != \"test\" and phase != \"train\":\n",
        "        print(\"ERROR: Phase: \" + phase + \" not recognized\")\n",
        "        return\n",
        "    if size != 300:\n",
        "        print(\"ERROR: You specified size \" + repr(size) + \". However, \" +\n",
        "              \"currently only SSD300 (size=300) is supported!\")\n",
        "        return\n",
        "    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),\n",
        "                                     add_extras(extras[str(size)], 1024),\n",
        "                                     mbox[str(size)], num_classes)\n",
        "    return SSD(phase, size, base_, extras_, head_, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#rrrr"
      ],
      "metadata": {
        "id": "sdt-pm18l1T3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZsmAaPjwl2Fy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FF2pFCXWgHC4",
        "0TQlgZtkmDr9",
        "nAxvvKwygIzV",
        "2MjsopT_fYLn"
      ],
      "provenance": [],
      "mount_file_id": "11_HGJnQcno3E-o1OioGQBwA6f_YItBbW",
      "authorship_tag": "ABX9TyPnvrdRwW6pEHErqgAfgyoM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}